{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import lightgbm as lgb\n",
    "from bayes_opt import BayesianOptimization\n",
    "import sys\n",
    "sys.path.append(\"../fraud_detection/src/\")\n",
    "from util import lgb_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train application df shape: (1521787, 29)\n",
      "Test application df shape: (421665, 28)\n",
      "Process train/test application - done in 67s\n",
      "Train application df shape: (1521787, 43)\n",
      "Test application df shape: (421665, 42)\n",
      "Add bacno/cano feature - done in 17s\n",
      "Train application df shape: (1521787, 115)\n",
      "Test application df shape: (421665, 114)\n",
      "Add iterm-related feature - done in 108s\n",
      "Train application df shape: (1521787, 185)\n",
      "Test application df shape: (421665, 184)\n",
      "Add conam-related feature - done in 130s\n",
      "Train application df shape: (1521787, 209)\n",
      "Test application df shape: (421665, 208)\n",
      "Add hour-related feature - done in 545s\n",
      "Train application df shape: (1521787, 210)\n",
      "Test application df shape: (421665, 209)\n",
      "Add cano/conam feature - done in 42s\n",
      "Train application df shape: (1521787, 230)\n",
      "Test application df shape: (421665, 229)\n",
      "Add cano/bacno latent feature - done in 4s\n",
      "Train application df shape: (1521787, 265)\n",
      "Test application df shape: (421665, 264)\n",
      "Add locdt-related feature - done in 188s\n",
      "Train application df shape: (1521787, 272)\n",
      "Test application df shape: (421665, 271)\n",
      "Add mchno-related feature - done in 121s\n",
      "Train application df shape: (1521787, 279)\n",
      "Test application df shape: (421665, 278)\n",
      "Add scity-related feature - done in 120s\n",
      "Train application df shape: (1521787, 286)\n",
      "Test application df shape: (421665, 285)\n",
      "Add stocn-related feature - done in 122s\n",
      "Train application df shape: (1521787, 306)\n",
      "Test application df shape: (421665, 305)\n",
      "Add mchno/bacno latent feature - done in 6s\n",
      "Train application df shape: (1521787, 320)\n",
      "Test application df shape: (421665, 319)\n",
      "Add time second-level feature on bacno - done in 249s\n",
      "Train application df shape: (1521787, 334)\n",
      "Test application df shape: (421665, 333)\n",
      "Add time second-level feature on cano - done in 260s\n",
      "Train application df shape: (1521787, 348)\n",
      "Test application df shape: (421665, 347)\n",
      "Add time second-level feature on mchno - done in 271s\n",
      "Train application df shape: (1521787, 390)\n",
      "Test application df shape: (421665, 389)\n",
      "Add time second-level feature on csmcu/stocn/scity - done in 424s\n",
      "Train application df shape: (1521787, 446)\n",
      "Test application df shape: (421665, 445)\n",
      "Add time second-level feature on acqic/csmcu/stocn/scity - done in 661s\n",
      "Train application df shape: (1521787, 488)\n",
      "Test application df shape: (421665, 487)\n",
      "Add conam-related feature v3 - done in 449s\n",
      "Train application df shape: (1521787, 572)\n",
      "Test application df shape: (421665, 571)\n",
      "Add locdt-related feature v2 - done in 707s\n",
      "Train application df shape: (1521787, 641)\n",
      "Test application df shape: (421665, 640)\n",
      "Add conam-related feature v4 - done in 761s\n",
      "Train application df shape: (1521787, 661)\n",
      "Test application df shape: (421665, 660)\n",
      "Add cano/mchno latent feature - done in 11s\n",
      "Train application df shape: (1521787, 681)\n",
      "Test application df shape: (421665, 680)\n",
      "Add cano/locdt latent feature - done in 11s\n",
      "Train application df shape: (1521787, 701)\n",
      "Test application df shape: (421665, 700)\n",
      "Add mchno/locdt latent feature - done in 10s\n",
      "Run LightGBM with kfold - done in 20s\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../fraud_detection/src/\")\n",
    "import time\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import numpy as np\n",
    "from contextlib import contextmanager\n",
    "import gc \n",
    "from util import s_to_time_format, string_to_datetime, hour_to_range, kfold_lightgbm, kfold_xgb\n",
    "from util import rolling_stats_target_by_cols\n",
    "#from util import _time_elapsed_between_last_transactions,time_elapsed_between_last_transactions\n",
    "#from util import num_transaction_in_past_n_days\n",
    "#from util import add_auto_encoder_feature\n",
    "#from util import group_target_by_cols_split_by_users\n",
    "from time import strftime, localtime\n",
    "import logging\n",
    "import sys\n",
    "from config import Configs\n",
    "\n",
    "# logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "#log_file = '{}-{}-{}.log'.format(opt.model_name, opt.dataset, strftime(\"%y%m%d-%H%M\", localtime()))\n",
    "log_file = '../fraud_detection/result/fs_{}.log'.format(strftime(\"%y%m%d-%H%M\", localtime()))\n",
    "logger.addHandler(logging.FileHandler(log_file))\n",
    "\n",
    "def group_target_by_cols(df_train, df_test, recipe):\n",
    "    df = pd.concat([df_train, df_test], axis = 0)\n",
    "    for m in range(len(recipe)):\n",
    "        cols = recipe[m][0]\n",
    "        for n in range(len(recipe[m][1])):\n",
    "            target = recipe[m][1][n][0]\n",
    "            method = recipe[m][1][n][1]\n",
    "            name_grouped_target = method+\"_\"+target+'_BY_'+'_'.join(cols)\n",
    "            tmp = df[cols + [target]].groupby(cols).agg(method)\n",
    "            tmp = tmp.reset_index().rename(index=str, columns={target: name_grouped_target})\n",
    "            df_train = df_train.merge(tmp, how='left', on=cols)\n",
    "            df_test = df_test.merge(tmp, how='left', on=cols)\n",
    "\n",
    "        # reduced memory    \n",
    "        del tmp\n",
    "        gc.collect()\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    logger.info(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
    "    \n",
    "def main(args):\n",
    "    with timer(\"Process train/test application\"):\n",
    "        #-------------------------\n",
    "        # load dataset\n",
    "        #-------------------------\n",
    "        df_train = pd.read_csv(args.train_file)\n",
    "        df_test = pd.read_csv(args.test_file)\n",
    "        #-------------------------\n",
    "        # pre-processing\n",
    "        #-------------------------\n",
    "\n",
    "        for cat in Configs.CATEGORY:\n",
    "            df_train[cat] = df_train[cat].astype('category') #.cat.codes\n",
    "            df_test[cat] = df_test[cat].astype('category')\n",
    "            \n",
    "        for df in [df_train, df_test]:\n",
    "            # pre-processing\n",
    "            df[\"loctm_\"] = df.loctm.astype(int).astype(str)\n",
    "            df.loctm_ = df.loctm_.apply(s_to_time_format).apply(string_to_datetime)\n",
    "            # # time-related feature\n",
    "            df[\"loctm_hour_of_day\"] = df.loctm_.apply(lambda x: x.hour).astype('category')\n",
    "            df[\"loctm_minute_of_hour\"] = df.loctm_.apply(lambda x: x.minute)\n",
    "            df[\"loctm_second_of_min\"] = df.loctm_.apply(lambda x: x.second)\n",
    "            # df[\"loctm_absolute_time\"] = [h*60+m for h,m in zip(df.loctm_hour_of_day,df.loctm_minute_of_hour)]\n",
    "            df[\"hour_range\"] = df.loctm_.apply(lambda x: hour_to_range(x.hour)).astype(\"category\")\n",
    "            # removed the columns no need\n",
    "            df.drop(columns = [\"loctm_\"], axis = 1, inplace = True)\n",
    "            # auxiliary fields\n",
    "            df[\"day_hr_min\"] = [\"{}:{}:{}\".format(i,j,k) for i,j,k in zip(df.locdt,df.loctm_hour_of_day,df.loctm_minute_of_hour)]\n",
    "            df[\"day_hr_min_sec\"] = [\"{}:{}:{}:{}\".format(i,j,k,z) for i,j,k,z in zip(df.locdt,df.loctm_hour_of_day,df.loctm_minute_of_hour,df.loctm_second_of_min)]\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add bacno/cano feature\"):\n",
    "        df_train, df_test = group_target_by_cols(df_train, df_test, Configs.CONAM_AGG_RECIPE_1)\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add iterm-related feature\"):\n",
    "        df_train, df_test = group_target_by_cols(df_train, df_test, Configs.ITERM_AGG_RECIPE)\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add conam-related feature\"):\n",
    "        df_train, df_test = group_target_by_cols(df_train, df_test, Configs.CONAM_AGG_RECIPE_2)\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add hour-related feature\"):\n",
    "        df_train, df_test = group_target_by_cols(df_train, df_test, Configs.HOUR_AGG_RECIPE)\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add cano/conam feature\"):\n",
    "        df_train, df_test = group_target_by_cols(df_train, df_test, Configs.CANO_CONAM_COUNT_RECIPE)\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add cano/bacno latent feature\"):\n",
    "        df = pd.read_csv(\"../fraud_detection/features/bacno_latent_features_w_cano.csv\")\n",
    "        df_train = df_train.merge(df, on = \"bacno\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"bacno\", how = \"left\")\n",
    "        df = pd.read_csv(\"../fraud_detection/features/bacno_cano_latent_features.csv\")\n",
    "        df_train = df_train.merge(df, on = \"cano\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"cano\", how = \"left\")\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add locdt-related feature\"):\n",
    "        df_train, df_test = group_target_by_cols(df_train, df_test, Configs.LOCDT_CONAM_RECIPE)\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add mchno-related feature\"):\n",
    "        df_train, df_test = group_target_by_cols(df_train, df_test, Configs.MCHNO_CONAM_RECIPE)\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add scity-related feature\"):\n",
    "        df_train, df_test = group_target_by_cols(df_train, df_test, Configs.SCITY_CONAM_RECIPE)\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add stocn-related feature\"):\n",
    "        df_train, df_test = group_target_by_cols(df_train, df_test, Configs.STOCN_CONAM_RECIPE)\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add mchno/bacno latent feature\"):\n",
    "        df = pd.read_csv(\"../fraud_detection/features/bacno_latent_features_w_mchno.csv\")\n",
    "        df_train = df_train.merge(df, on = \"bacno\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"bacno\", how = \"left\")\n",
    "        df = pd.read_csv(\"../fraud_detection/features/bacno_mchno_latent_features.csv\")\n",
    "        df_train = df_train.merge(df, on = \"mchno\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"mchno\", how = \"left\")\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add time second-level feature on bacno\"):\n",
    "        df_train, df_test = group_target_by_cols(\n",
    "            df_train, \n",
    "            df_test, \n",
    "            Configs.HOUR_AGG_SEC_LEVEL_RECIPE_BACNO,\n",
    "            )\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add time second-level feature on cano\"):\n",
    "        df_train, df_test = group_target_by_cols(\n",
    "            df_train, \n",
    "            df_test, \n",
    "            Configs.HOUR_AGG_SEC_LEVEL_RECIPE_CANO,\n",
    "            )\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add time second-level feature on mchno\"):\n",
    "        df_train, df_test = group_target_by_cols(\n",
    "            df_train, \n",
    "            df_test, \n",
    "            Configs.HOUR_AGG_SEC_LEVEL_RECIPE_MCHNO,\n",
    "            )\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add time second-level feature on csmcu/stocn/scity\"):\n",
    "        df_train, df_test = group_target_by_cols(\n",
    "            df_train, \n",
    "            df_test, \n",
    "            Configs.HOUR_AGG_SEC_LEVEL_RECIPE,\n",
    "            )\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add time second-level feature on acqic/csmcu/stocn/scity\"):\n",
    "        df_train, df_test = group_target_by_cols(\n",
    "            df_train, \n",
    "            df_test, \n",
    "            Configs.HOUR_AGG_SEC_LEVEL_RECIPE_2,\n",
    "            )\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add conam-related feature v3\"):\n",
    "        df_train, df_test = group_target_by_cols(\n",
    "            df_train, \n",
    "            df_test, \n",
    "            Configs.CONAM_AGG_RECIPE_3,\n",
    "            )\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add locdt-related feature v2\"):\n",
    "        df_train, df_test = group_target_by_cols(df_train, df_test, Configs.LOCDT_CONAM_RECIPE_2)\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add conam-related feature v4\"):\n",
    "        df_train, df_test = group_target_by_cols(\n",
    "            df_train, \n",
    "            df_test, \n",
    "            Configs.CONAM_AGG_RECIPE_4,\n",
    "            )\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add cano/mchno latent feature\"):\n",
    "        df = pd.read_csv(\"../fraud_detection/features/cano_latent_features_w_mchno.csv\")\n",
    "        df_train = df_train.merge(df, on = \"cano\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"cano\", how = \"left\")\n",
    "        df = pd.read_csv(\"../fraud_detection/features/cano_mchno_latent_features.csv\")\n",
    "        df_train = df_train.merge(df, on = \"mchno\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"mchno\", how = \"left\")\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))   \n",
    "\n",
    "    with timer(\"Add cano/locdt latent feature\"):\n",
    "        df = pd.read_csv(\"../fraud_detection/features/cano_latent_features_w_locdt.csv\")\n",
    "        df_train = df_train.merge(df, on = \"cano\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"cano\", how = \"left\")\n",
    "        df = pd.read_csv(\"../fraud_detection/features/cano_locdt_latent_features.csv\")\n",
    "        df_train = df_train.merge(df, on = \"locdt\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"locdt\", how = \"left\")\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add mchno/locdt latent feature\"):\n",
    "        df = pd.read_csv(\"../fraud_detection/features/mchno_latent_features_w_locdt.csv\")\n",
    "        df_train = df_train.merge(df, on = \"mchno\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"mchno\", how = \"left\")\n",
    "        df = pd.read_csv(\"../fraud_detection/features/mchno_locdt_latent_features.csv\")\n",
    "        df_train = df_train.merge(df, on = \"locdt\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"locdt\", how = \"left\")\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    #return df_train, df_test\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    with timer(\"Run LightGBM with kfold\"):\n",
    "        if args.feature_selection:\n",
    "            logger.info(\"==============Feature Selection==============\")\n",
    "            for df in [df_train, df_test]:\n",
    "                # drop random features (by null hypothesis)\n",
    "                df.drop(Configs.FEATURE_GRAVEYARD, axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "                # drop unused features features_with_no_imp_at_least_twice\n",
    "                df.drop(Configs.FEATURE_USELESSNESS, axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "                gc.collect()   \n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        for df in [df_train, df_test]:\n",
    "            df.drop(columns = [\"loctm_hour_of_day\",\n",
    "                               \"loctm_minute_of_hour\", \n",
    "                               \"loctm_second_of_min\",\n",
    "                               \"day_hr_min\",\n",
    "                               \"day_hr_min_sec\",\n",
    "                               ], axis = 1, inplace = True)\n",
    "   \n",
    "    return df_train, df_test\n",
    "\n",
    "args = {\n",
    " \"train_file\":\"/data/yunrui_li/fraud/dataset/train.csv\",\n",
    " \"test_file\":\"/data/yunrui_li/fraud/dataset/test.csv\",\n",
    " \"result_path\":\"/data/yunrui_li/fraud/fraud_detection/result/submission.csv\",\n",
    " \"feature_selection\":True,\n",
    " \"feature_importance_plot\": True,\n",
    " \"SEED\": 1030,\n",
    " \"NUM_FOLDS\": 2, # 5\n",
    " \"CPU_USE_RATE\":1.0,\n",
    " \"STRATIFIED\": True,\n",
    " \"TEST_NULL_HYPO\":False,\n",
    " \"NUM_LEAVES\":31,\n",
    " \"COLSAMPLE_BYTREE\":1.0,\n",
    " \"SUBSAMPLE\": 1.0,\n",
    " \"SUBSAMPLE_FREQ\": 0,\n",
    " \"MAX_DEPTH\": -1,\n",
    " \"REG_ALPHA\": 0.0,\n",
    " \"REG_LAMBDA\": 0.0,\n",
    " \"MIN_SPLIT_GAIN\": 0.0,\n",
    " \"MIN_CHILD_WEIGHT\": 0.001,\n",
    " \"MAX_BIN\": 255,\n",
    " \"SCALE_POS_WEIGHT\": 3\n",
    "    \n",
    "}\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "args = AttrDict(args)\n",
    "df_train, df_test = main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [df_train, df_test]:\n",
    "    # drop random features (by null hypothesis)\n",
    "    df.drop(Configs.FEATURE_GRAVEYARD, axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    # drop unused features features_with_no_imp_at_least_twice\n",
    "    df.drop(Configs.FEATURE_USELESSNESS, axis=1, inplace=True, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = [f for f in df_train.columns if f not in [\"fraud_ind\"]]\n",
    "X,y = df_train[feats], df_train.fraud_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1521787, 475), (1521787,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-788ee363e0a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert 1==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | lambda_l1 | lambda_l2 |  max_bin  | min_ch... | min_sp... | num_le... | scale_... |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "{'binary_logloss-mean': [0.0583174069529084, 0.05423044856947863, 0.05153480347681252, 0.04948929460627523, 0.047879614924879, 0.04656555881124902, 0.04545107699846366, 0.04449683524758583, 0.0436814159059497, 0.04297973002654774, 0.04235993153504963, 0.041812268770672054, 0.0413200751069355, 0.04089262681885676, 0.040503258619917146, 0.04015809730063233, 0.03985122385447294, 0.039576971099261074, 0.03932361242564557, 0.03910199840507274, 0.03891993831303456, 0.038752206225506194, 0.03860355557959091, 0.038464473630774085, 0.03835321651678525, 0.0382559360634386, 0.038159629130730864, 0.038056501422044066, 0.03797385022078252, 0.03791456327458946, 0.037869464394165706, 0.03783047196380297, 0.037795722637268325, 0.03777549018359709, 0.03775652217250993, 0.037745651180265564, 0.0377359086137123, 0.037734705594832774], 'binary_logloss-stdv': [7.099889376888788e-05, 9.365570922980991e-05, 9.92999354171909e-05, 0.00010745859793035922, 0.0001195517124706594, 0.0001165304468313492, 0.00012738731007190163, 0.00013983423620854277, 0.00015015213960496956, 0.0001625821011514972, 0.00016173815855930113, 0.0001653329431528159, 0.0001670710032483583, 0.00016427149839791876, 0.00015479754145379302, 0.00015231313359734296, 0.00014940994245902096, 0.00015554610746737956, 0.0001568209990919217, 0.00015418566626307467, 0.00015732207101038827, 0.0001582615495193668, 0.00015314265811755155, 0.00015304639424944293, 0.00015249170216079868, 0.00015338125459326836, 0.0001534965398062116, 0.00015730752159988696, 0.00015866252445800885, 0.00015974638718554006, 0.00016530592015687976, 0.0001653193186962773, 0.00016620899696363683, 0.00016921373797042233, 0.00016738570830600097, 0.0001679884997884989, 0.00016435886487015726, 0.00016947150516541317], 'f1-mean': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00019646365422396856, 0.00039278263712671184, 0.0012753758702959115, 0.006648652549311555, 0.014778069420821244, 0.03499444956883932, 0.13778602164159243, 0.24522421547798984, 0.4183011363877017, 0.4886485028151772, 0.5291198964041752, 0.5635999705400282, 0.5878985245086191], 'f1-stdv': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.00024061785292565598, 0.0005725121463010547, 0.0015065640904688912, 0.00452608589462903, 0.008723163836278346, 0.01056422930382488, 0.01736011141927637, 0.030551728869217955, 0.018619922534319853, 0.013587732621501564, 0.016957223736503317, 0.016192405400128042, 0.012117563158671478]}\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.5879  \u001b[0m | \u001b[0m 0.7744  \u001b[0m | \u001b[0m 0.8576  \u001b[0m | \u001b[0m 3.014   \u001b[0m | \u001b[0m 1.635   \u001b[0m | \u001b[0m 297.4   \u001b[0m | \u001b[0m 32.65   \u001b[0m | \u001b[0m 0.04376 \u001b[0m | \u001b[0m 42.73   \u001b[0m | \u001b[0m 9.673   \u001b[0m |\n",
      "{'binary_logloss-mean': [0.05886693543313054, 0.054826460322183815, 0.052156201216173884, 0.050118045033722206, 0.048521668342480846, 0.04724350163151317, 0.04616351903181269, 0.0452563706719427, 0.04446660010656175, 0.043781098586124875, 0.04317499638786883, 0.04263612438635143, 0.042176651471808775, 0.04174404056009952, 0.04138010712878398, 0.04105801685708223, 0.04077096395283376, 0.04052388217322376, 0.0402946841745612, 0.04008568145111755, 0.03991780690070573, 0.03976293445064214, 0.039626565917712596, 0.03950554910464469, 0.039405549486264096, 0.039316161384930395, 0.03922989813905125, 0.03915448944301697, 0.03908727750344811, 0.039047897587908234, 0.03901441313523819, 0.038994249897641074, 0.03896284144479446, 0.038956708857992045, 0.03894092921447992, 0.038938971790803294, 0.038934252530233394], 'binary_logloss-stdv': [0.0001199259674524828, 0.0001241731194243446, 0.00012120321206682436, 0.00012504891176637093, 0.000122550521779368, 0.00012471773314302634, 0.00012658173648953994, 0.00012708343543749607, 0.00013670130734627482, 0.00013800171442249757, 0.00014397237515654192, 0.00015124849956027733, 0.00014891991719599064, 0.00014963831250248513, 0.00014954537131877963, 0.000152112740147229, 0.00014366986195044515, 0.00013736410996890403, 0.00013452801923801962, 0.00012628739511573813, 0.0001231200254550122, 0.00011929412090344683, 0.0001247651762501933, 0.00011825642627702892, 0.00011994843432051207, 0.00010936021116508353, 0.00010793394914640039, 0.00010559248897204999, 0.00010985274650877936, 0.00010893823533380349, 0.00010385019800635696, 0.0001048392529996819, 0.00010596502874353471, 0.00010933808921892966, 0.00010288568560781624, 9.949719060366584e-05, 0.00010229782023209921], 'f1-mean': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0004906771344455349, 0.002351166614690207, 0.009857663790687289, 0.017330369330110153, 0.026812451048192278, 0.10177365434044902, 0.1500982058924357, 0.27752264140524985, 0.38458912464471645, 0.44514644249025564, 0.4817880089947198], 'f1-stdv': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0009813542688910698, 0.0028946375950683763, 0.00584914212267904, 0.003587215837662702, 0.005089844169404791, 0.026983832789737417, 0.0074767550863662725, 0.03563349095006539, 0.012695361855158734, 0.0071167367208444425, 0.00812877133558289]}\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.4818  \u001b[0m | \u001b[0m 0.6917  \u001b[0m | \u001b[0m 0.8959  \u001b[0m | \u001b[0m 2.644   \u001b[0m | \u001b[0m 1.704   \u001b[0m | \u001b[0m 347.6   \u001b[0m | \u001b[0m 4.481   \u001b[0m | \u001b[0m 0.008713\u001b[0m | \u001b[0m 24.42   \u001b[0m | \u001b[0m 8.494   \u001b[0m |\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def lgb_f1_score(y_pred, y_true):\n",
    "    \"\"\"evaluation metric\"\"\"\n",
    "    #print (\"y_pred\",y_pred)\n",
    "    #print (\"y_true\",y_true)\n",
    "    y_hat = np.round(y_pred)\n",
    "    return 'f1', f1_score(y_true.get_label(), y_hat), True\n",
    "\n",
    "def bayes_parameter_opt_lgb(X, y, \n",
    "                            init_round=15, \n",
    "                            opt_round=25, \n",
    "                            n_folds=5, \n",
    "                            random_seed=1030,\n",
    "                            n_estimators=10000,\n",
    "                            learning_rate=0.05, \n",
    "                            output_process=True):\n",
    "    # prepare data\n",
    "    train_data = lgb.Dataset(data=X, label=y, categorical_feature='auto', free_raw_data = False)\n",
    "    # parameters\n",
    "    def lgb_eval(num_leaves, feature_fraction, bagging_fraction,\n",
    "                 #max_depth, \n",
    "                 lambda_l1, lambda_l2, min_split_gain, \n",
    "                 min_child_weight, max_bin, scale_pos_weight):\n",
    "        params = {'application':'binary',\n",
    "                  'num_iterations': n_estimators, \n",
    "                  'learning_rate':learning_rate, \n",
    "                  'early_stopping_round':100, \n",
    "                  'n_jobs':5,\n",
    "                  }\n",
    "        params[\"num_leaves\"] = int(round(num_leaves))\n",
    "        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n",
    "        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n",
    "        #params['max_depth'] = int(round(max_depth))\n",
    "        params['lambda_l1'] = max(lambda_l1, 0)\n",
    "        params['lambda_l2'] = max(lambda_l2, 0)\n",
    "        params['min_split_gain'] = min_split_gain\n",
    "        params['min_child_weight'] = min_child_weight\n",
    "        params['max_bin'] = int(round(max_bin))\n",
    "        params['scale_pos_weight'] = scale_pos_weight\n",
    "\n",
    "        cv_result = lgb.cv(params, \n",
    "                           train_data, \n",
    "                           nfold=n_folds,\n",
    "                           seed=random_seed, \n",
    "                           stratified=True, \n",
    "                           categorical_feature = \"auto\",\n",
    "                           feval=lgb_f1_score)\n",
    "        print (cv_result)\n",
    "        return max(cv_result['f1-mean'])\n",
    "    # range \n",
    "    lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (24, 45),\n",
    "                                            'feature_fraction': (0.5, 1.0),\n",
    "                                            'bagging_fraction': (0.5, 1.0),\n",
    "                                            'lambda_l1': (0, 5),\n",
    "                                            'lambda_l2': (0, 3),\n",
    "                                            'min_split_gain': (0.0, 0.1),\n",
    "                                            'min_child_weight': (1, 50),\n",
    "                                            'scale_pos_weight': (1, 10),\n",
    "                                            'max_bin': (255,355),\n",
    "                                           }, \n",
    "                                 random_state=0)\n",
    "    # optimize\n",
    "    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n",
    "    \n",
    "    # output optimization process\n",
    "    if output_process==True: \n",
    "        pd.DataFrame(lgbBO.res).sort_values(by = \"target\", ascending=False).to_csv(\"../fraud_detection/result/bayes_opt_result.csv\")\n",
    "    return lgbBO.max[\"target\"], lgbBO.max[\"params\"] # best score and best parameter\n",
    "#     return lgbBO\n",
    "#     # return best parameters\n",
    "#     return lgbBO.res['max']['max_params']\n",
    "\n",
    "opt_score, opt_params = bayes_parameter_opt_lgb(X, y, \n",
    "                                     init_round=5, \n",
    "                                     opt_round=10, \n",
    "                                     n_folds=5, \n",
    "                                     random_seed=1030, \n",
    "                                     n_estimators=10000, \n",
    "                                     learning_rate=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.8890783754749252,\n",
       " 'feature_fraction': 0.9350060741234096,\n",
       " 'lambda_l1': 4.89309171116382,\n",
       " 'lambda_l2': 2.397475692650171,\n",
       " 'max_bin': 301.14793622529317,\n",
       " 'min_child_weight': 39.245929638036316,\n",
       " 'min_split_gain': 0.011827442586893323,\n",
       " 'num_leaves': 37.438341447878,\n",
       " 'scale_pos_weight': 2.2901795866814174}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8602295952767033"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_params.max[\"target\"], opt_params.max[\"params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 1000\n",
    "pd.DataFrame(opt_params.res).sort_values(by = \"target\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"A\":[1,2,3,4],\"B\":[1,1,1,1]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"A\"] = df[\"A\"].copy().sample(frac = 1.0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " df.A.copy().sample(frac = 1.0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier(\n",
    "    n_jobs = 3,\n",
    "    boosting_type = \"rf\",\n",
    "    # nthread=int(multiprocessing.cpu_count()*args.CPU_USE_RATE),\n",
    "    n_estimators=10000,\n",
    "    )\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(lgb.cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
