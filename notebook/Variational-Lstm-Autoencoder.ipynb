{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ldap_home/yunrui.li/.pyenv/versions/3.6.5/envs/deepts/lib/python3.6/site-packages/ipykernel_launcher.py:134: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training data (1390382, 30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/18 [00:00<?, ?it/s]/ldap_home/yunrui.li/.pyenv/versions/3.6.5/envs/deepts/lib/python3.6/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 18/18 [00:42<00:00,  2.37s/it]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]/ldap_home/yunrui.li/.pyenv/versions/3.6.5/envs/deepts/lib/python3.6/site-packages/ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 23/23 [01:10<00:00,  3.08s/it]\n",
      "/ldap_home/yunrui.li/.pyenv/versions/3.6.5/envs/deepts/lib/python3.6/site-packages/pandas/core/frame.py:4102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"../fraud_detection/src/\")\n",
    "from util import s_to_time_format, string_to_datetime,hour_to_range\n",
    "from tqdm import tqdm\n",
    "\n",
    "def value_to_count(df_train, df_test, df_train_normal_cano_id, df_):\n",
    "    # separate continuous feature and categorial features\n",
    "    feats = ['acqic', 'bacno', 'cano', 'conam', 'contp', 'csmcu', 'ecfg', 'etymd',\n",
    "       'flbmk', 'flg_3dsmk', 'hcefg', 'insfg', 'iterm', 'locdt',\n",
    "       'mcc', 'mchno', 'ovrlt', 'scity', 'stocn', 'stscd', 'loctm_hour_of_day',\n",
    "       'loctm_minute_of_hour', 'loctm_second_of_min'] \n",
    "    cont_feats = ['iterm', \n",
    "                  'locdt',\n",
    "                  'loctm_hour_of_day',\n",
    "                  'loctm_minute_of_hour', \n",
    "                  'loctm_second_of_min']\n",
    "    feats = [f for f in feats if f not in cont_feats]\n",
    "    # we only coner categorial features\n",
    "    \n",
    "    df = pd.concat([df_train[feats], df_test[feats]], axis = 0)\n",
    "    for f in tqdm(feats):\n",
    "        count_dict = df[f].value_counts(dropna = False).to_dict() \n",
    "        df_train_normal_cano_id[f] = df_train_normal_cano_id[f].apply(lambda v: count_dict[v])\n",
    "        df_train[f] = df_train[f].apply(lambda v: count_dict[v])\n",
    "        df_test[f] = df_test[f].apply(lambda v: count_dict[v])\n",
    "        df_[f] = df_[f].apply(lambda v: count_dict[v])\n",
    "    return df_train,df_test,df_train_normal_cano_id, df_\n",
    "\n",
    "def feature_normalization_auto(df_train, df_test, df_train_normal_cano_id,df_):\n",
    "    \"\"\"\n",
    "    return two inputs of autoencoder, one is for train and another one is for test\n",
    "    \"\"\"\n",
    "    #from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n",
    "    feats = ['acqic', 'bacno', 'cano', 'conam', 'contp', 'csmcu', 'ecfg', 'etymd',\n",
    "       'flbmk', 'flg_3dsmk', 'hcefg', 'insfg', 'iterm', 'locdt',\n",
    "       'mcc', 'mchno', 'ovrlt', 'scity', 'stocn', 'stscd', 'loctm_hour_of_day',\n",
    "       'loctm_minute_of_hour', 'loctm_second_of_min']\n",
    "    df = pd.concat([df_train[feats], df_test[feats]], axis = 0)\n",
    "\n",
    "\n",
    "    for f in tqdm(feats):\n",
    "        try:\n",
    "            #scaler = MinMaxScaler()\n",
    "            max_ = df[f].max()\n",
    "            min_ = df[f].min()\n",
    "            df_train_normal_cano_id[f] = df_train_normal_cano_id[f].apply(lambda x: (x-min_)/(max_-min_))\n",
    "            df_[f] = df_[f].apply(lambda x: (x-min_)/(max_-min_))\n",
    "            #df_test[f] = df_test[f].apply(lambda x: (x-min_)/(max_-min_))\n",
    "        except:\n",
    "            print(f)\n",
    "    return df_train_normal_cano_id,df_\n",
    "\n",
    "def partition_(df, num_features):\n",
    "    data = []\n",
    "    for i in range(len(df)):\n",
    "        out = None\n",
    "        if i == 0:\n",
    "            out = np.concatenate(((np.zeros((2,num_features))),df.iloc[:1].values))\n",
    "        elif i== 1:\n",
    "            out = np.concatenate(((np.zeros((1,num_features))),df.iloc[:i+1].values))\n",
    "        else:\n",
    "            out = df.iloc[i+1-3:i+1].values\n",
    "        data.append(out)\n",
    "    return data\n",
    "\n",
    "def partition(df_, sequence_length = 3):\n",
    "    feats = [f for f in df_.columns if f not in {\"fraud_ind\",\"cano_help\",\"locdt_help\"}]\n",
    "    sequences = []\n",
    "    for _, df in df_.groupby(by = \"cano_help\"):\n",
    "        data = partition_(df[feats], num_features = len(feats))\n",
    "        for d in data:\n",
    "            sequences.append(d)\n",
    "    return sequences\n",
    "\n",
    "def get_sequence_dataframe(df):\n",
    "    df_train_sequences = partition(df)\n",
    "    df_train_sequences = np.concatenate(df_train_sequences)\n",
    "    df_train_sequences = pd.DataFrame(df_train_sequences)\n",
    "    return df_train_sequences\n",
    "\n",
    "\n",
    "#-----------------------------\n",
    "# load data\n",
    "#-----------------------------\n",
    "df_train = pd.read_csv(\"/data/yunrui_li/fraud/dataset/train.csv\")\n",
    "df_test = pd.read_csv(\"/data/yunrui_li/fraud/dataset/test.csv\")\n",
    "\n",
    "\n",
    "for df in [df_train, df_test]:\n",
    "    # pre-processing\n",
    "    df[\"loctm_\"] = df.loctm.astype(int).astype(str)\n",
    "    df.loctm_ = df.loctm_.apply(s_to_time_format).apply(string_to_datetime)\n",
    "    # time-related feature\n",
    "    df[\"loctm_hour_of_day\"] = df.loctm_.apply(lambda x: x.hour)\n",
    "    df[\"loctm_minute_of_hour\"] = df.loctm_.apply(lambda x: x.minute)\n",
    "    df[\"loctm_second_of_min\"] = df.loctm_.apply(lambda x: x.second)\n",
    "\n",
    "    # removed the columns no need\n",
    "    df.drop(columns = [\"loctm_\", \"loctm\",\"txkey\"], axis = 1, inplace = True)\n",
    "\n",
    "df_train[\"cano_locdt_index\"] = [\"{}_{}_{}_{}_{}\".format(str(i),str(j),str(k),str(l),str(m)) for i,j,k,l,m in zip(df_train.cano,\n",
    "                                                                                   df_train.locdt,\n",
    "                                                                                   df_train.loctm_hour_of_day,\n",
    "                                                                                   df_train.loctm_minute_of_hour,\n",
    "                                                                                   df_train.loctm_second_of_min,\n",
    "                                                                                  )]\n",
    "df_test[\"cano_locdt_index\"] = [\"{}_{}_{}_{}_{}\".format(str(i),str(j),str(k),str(l),str(m)) for i,j,k,l,m in zip(df_test.cano,\n",
    "                                                                                  df_test.locdt,\n",
    "                                                                                  df_train.loctm_hour_of_day,\n",
    "                                                                                  df_train.loctm_minute_of_hour,\n",
    "                                                                                  df_train.loctm_second_of_min,\n",
    "                                                                                 )]\n",
    "\n",
    "df_train[\"cano_help\"] = df_train.cano\n",
    "df_test[\"cano_help\"] = df_test.cano\n",
    "\n",
    "df_train[\"locdt_help\"] = df_train.locdt\n",
    "df_test[\"locdt_help\"] = df_test.locdt\n",
    "\n",
    "df_train[\"loctm_hour_of_day_help\"] = df_train.loctm_hour_of_day\n",
    "df_test[\"loctm_hour_of_day_help\"] = df_test.loctm_hour_of_day\n",
    "\n",
    "df_train[\"loctm_minute_of_hour_help\"] = df_train.loctm_minute_of_hour\n",
    "df_test[\"loctm_minute_of_hour_help\"] = df_test.loctm_minute_of_hour\n",
    "\n",
    "df_train[\"loctm_second_of_min_help\"] = df_train.loctm_second_of_min\n",
    "df_test[\"loctm_second_of_min_help\"] = df_test.loctm_second_of_min\n",
    "\n",
    "#-----------------------------\n",
    "# feature extraction\n",
    "#-----------------------------\n",
    "df = pd.concat([df_train, df_test], axis = 0)\n",
    "df.sort_values(by = [\"cano\", \"locdt\",\"loctm_hour_of_day\",\"loctm_minute_of_hour\",\"loctm_second_of_min\"], inplace = True)\n",
    "\n",
    "#-----------------------------\n",
    "# prepare training data\n",
    "#-----------------------------\n",
    "df_train.sort_values(by = [\"cano\", \"locdt\",\"loctm_hour_of_day\",\"loctm_minute_of_hour\",\"loctm_second_of_min\"], inplace = True)\n",
    "\n",
    "# df_train, df_test = value_to_count(df_train, df_test)\n",
    "# df_train, df_test = feature_normalization_auto(df_train, df_test)\n",
    "\n",
    "fraud_cano_id = df_train[df_train.fraud_ind == 1].cano.unique().tolist()\n",
    "\n",
    "df_train_normal_cano_id = df_train[~df_train.cano.isin(fraud_cano_id)]\n",
    "print (\"number of training data\",df_train_normal_cano_id.shape)\n",
    "\n",
    "df_train, df_test, df_train_normal_cano_id, df = value_to_count(df_train, df_test,df_train_normal_cano_id, df)\n",
    "df_train_normal_cano_id, df = feature_normalization_auto(df_train, df_test,df_train_normal_cano_id, df)\n",
    "\n",
    "#-----------------------------\n",
    "# post-processing\n",
    "#-----------------------------\n",
    "df.drop(columns = [\"fraud_ind\"], axis = 1, inplace = True)\n",
    "df_train_normal_cano_id.drop(columns = [\"fraud_ind\"], axis = 1, inplace = True)\n",
    "feats = ['acqic', 'bacno', 'cano', 'conam', 'contp', 'csmcu', 'ecfg', 'etymd',\n",
    "   'flbmk', 'flg_3dsmk', 'hcefg', 'insfg', 'iterm', 'locdt',\n",
    "   'mcc', 'mchno', 'ovrlt', 'scity', 'stocn', 'stscd', 'loctm_hour_of_day',\n",
    "   'loctm_minute_of_hour', 'loctm_second_of_min'] + [\"cano_locdt_index\",\"cano_help\",\"locdt_help\",\n",
    "#                                                      \"loctm_hour_of_day_help\",\n",
    "#                                                      \"loctm_minute_of_hour_help\",\n",
    "#                                                      \"loctm_second_of_min_help\",\n",
    "                                                    ]\n",
    "\n",
    "df = df[feats]\n",
    "df_train_normal_cano_id = df_train_normal_cano_id[feats]\n",
    "\n",
    "#-----------------------------\n",
    "# get train/test data\n",
    "#-----------------------------\n",
    "\n",
    "X_train = get_sequence_dataframe(df_train_normal_cano_id)\n",
    "Feature = get_sequence_dataframe(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------\n",
    "# modeling (unsupervised learning)\n",
    "#-----------------------------\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import six\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "class LSTM_Var_Autoencoder(object):\n",
    "\n",
    "    def __init__(self, intermediate_dim=None, z_dim=None, n_dim=None,\n",
    "                 stateful=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        intermediate_dim : LSTM cells dimension.\n",
    "        z_dim : dimension of latent space.\n",
    "        n_dim : dimension of input data.\n",
    "        statefull : if true, keep cell state through batches.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not intermediate_dim or not z_dim or not n_dim:\n",
    "            raise ValueError(\"You should set intermediate_dim, z_dim\"\n",
    "                             \"(latent space) dimension and your input\"\n",
    "                             \"third dimension, n_dim.\"\n",
    "                             \" \\n            \")\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        self.z_dim = z_dim\n",
    "        self.n_dim = n_dim\n",
    "        self.intermediate_dim = intermediate_dim\n",
    "        self.stateful = stateful\n",
    "        self.input = tf.placeholder(tf.float32, shape=[None, None, self.n_dim])\n",
    "        self.batch_size = tf.placeholder(tf.int64)\n",
    "\n",
    "        # tf.data api\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(self.input).repeat() \\\n",
    "            .batch(self.batch_size)\n",
    "        self.batch_ = tf.placeholder(tf.int32, shape=[])\n",
    "        self.ite = dataset.make_initializable_iterator()\n",
    "        self.x = self.ite.get_next()\n",
    "        self.repeat = tf.placeholder(tf.int32)\n",
    "\n",
    "        def gauss_sampling(mean, sigma):\n",
    "            with tf.name_scope(\"sample_gaussian\"):\n",
    "                eps = tf.random_normal(tf.shape(sigma), 0, 1, dtype=tf.float32)\n",
    "            # It should be log(sigma / 2), but this empirically converges\"\n",
    "            # much better for an unknown reason\"\n",
    "                z = tf.add(mean, sigma * eps)\n",
    "                return z\n",
    "\n",
    "    # (with few modifications) from https://stackoverflow.com/questions\n",
    "\n",
    "        def get_state_variables(batch_size, cell):\n",
    "            # For each layer, get the initial state and make a variable out of it\n",
    "            # to enable updating its value.\n",
    "            state_variables = []\n",
    "            for state_c, state_h in cell.zero_state(batch_size, tf.float32):\n",
    "                state_variables.append(tf.nn.rnn_cell.LSTMStateTuple(\n",
    "                    (state_c), (state_h)))\n",
    "    # Return as a tuple, so that it can be fed to dynamic_rnn as an initial\n",
    "    # state\n",
    "            return tuple(state_variables)\n",
    "\n",
    "        # Add an operation to update the train states with the last state\n",
    "        # tensors\n",
    "        def get_state_update_op(state_variables, new_states):\n",
    "            update_ops = []\n",
    "            for state_variable, new_state in zip(state_variables, new_states):\n",
    "                update_ops.extend([state_variable[0] == new_state[0],\n",
    "                                   state_variable[1] == new_state[1]])\n",
    "            return tf.tuple(update_ops)\n",
    "\n",
    "        # Return an operation to set each variable in a list of LSTMStateTuples\n",
    "        # to zero\n",
    "        def get_state_reset_op(state_variables, cell, batch_size):\n",
    "            zero_states = cell.zero_state(batch_size, tf.float32)\n",
    "            return get_state_update_op(state_variables, zero_states)\n",
    "\n",
    "        weights = {\n",
    "            'z_mean': tf.get_variable(\n",
    "                \"z_mean\",\n",
    "                shape=[\n",
    "                    self.intermediate_dim,\n",
    "                    self.z_dim],\n",
    "                initializer=tf.contrib.layers.xavier_initializer()),\n",
    "            'log_sigma': tf.get_variable(\n",
    "                \"log_sigma\",\n",
    "                shape=[\n",
    "                    self.intermediate_dim,\n",
    "                    self.z_dim],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())}\n",
    "        biases = {\n",
    "            'z_mean_b': tf.get_variable(\"b_mean\", shape=[self.z_dim],\n",
    "                                        initializer=tf.zeros_initializer()),\n",
    "            'z_std_b': tf.get_variable(\"b_log_sigma\", shape=[self.z_dim],\n",
    "                                       initializer=tf.zeros_initializer())\n",
    "        }\n",
    "\n",
    "        with tf.variable_scope(\"encoder\"):\n",
    "            with tf.variable_scope(\"LSTM_encoder\"):\n",
    "                lstm_layer = tf.nn.rnn_cell.LSTMCell(\n",
    "                    self.intermediate_dim,\n",
    "                    forget_bias=1,\n",
    "                    initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                    activation=tf.nn.relu)\n",
    "\n",
    "        if self.stateful:\n",
    "            self.batch_ = tf.placeholder(tf.int32, shape=[])\n",
    "            # throws an error without MultiRNNCell\n",
    "            layer = tf.nn.rnn_cell.MultiRNNCell([lstm_layer])\n",
    "            states = get_state_variables(self.batch_, layer)\n",
    "            outputs, new_states = tf.nn.dynamic_rnn(\n",
    "                layer, self.x, initial_state=states, dtype=tf.float32)\n",
    "            self.update_op = get_state_update_op(states, new_states)\n",
    "            self.reset_state_op = get_state_reset_op(\n",
    "                states, lstm_layer, self.batch_)\n",
    "        else:\n",
    "            outputs, _ = tf.nn.dynamic_rnn(lstm_layer, self.x, dtype=\"float32\")\n",
    "\n",
    "        # For each layer, get the initial state. states will be a tuple of\n",
    "        # LSTMStateTuples.\n",
    "        self.z_mean = tf.add(tf.matmul(\n",
    "            outputs[:, -1, :], weights['z_mean']), biases['z_mean_b'])\n",
    "        self.z_sigma = tf.nn.softplus(tf.add(tf.matmul(\n",
    "            outputs[:, -1, :], weights['log_sigma']), biases['z_std_b']))\n",
    "        self.z = gauss_sampling(self.z_mean, self.z_sigma)\n",
    "\n",
    "        # from [batch_size,z_dim] to [batch_size, TIMESTEPS, z_dim]\n",
    "        repeated_z = tf.keras.layers.RepeatVector(\n",
    "            self.repeat, dtype=\"float32\")(self.z)\n",
    "\n",
    "        with tf.variable_scope(\"decoder\"):\n",
    "            if self.stateful:\n",
    "                with tf.variable_scope('lstm_decoder_stateful'):\n",
    "                    rnn_layers_ = [\n",
    "                        tf.nn.rnn_cell.LSTMCell(\n",
    "                            size,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                            forget_bias=1) for size in [\n",
    "                            self.intermediate_dim,\n",
    "                            n_dim]]\n",
    "                    multi_rnn_cell_ = tf.nn.rnn_cell.MultiRNNCell(rnn_layers_)\n",
    "                    states_ = get_state_variables(self.batch_, multi_rnn_cell_)\n",
    "                self.x_reconstr_mean, new_states_ = tf.nn.dynamic_rnn(\n",
    "                    cell=multi_rnn_cell_, inputs=repeated_z, initial_state=states_, dtype=tf.float32)\n",
    "                self.update_op_ = get_state_update_op(states_, new_states_)\n",
    "                self.reset_state_op_ = get_state_reset_op(\n",
    "                    states_, multi_rnn_cell_, self.batch_)\n",
    "            else:\n",
    "                with tf.variable_scope('lstm_decoder_stateless'):\n",
    "                    rnn_layers = [\n",
    "                        tf.nn.rnn_cell.LSTMCell(\n",
    "                            size,\n",
    "                            initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                            forget_bias=1) for size in [\n",
    "                            self.intermediate_dim,\n",
    "                            n_dim]]\n",
    "                    multi_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(rnn_layers)\n",
    "                self.x_reconstr_mean, _ = tf.nn.dynamic_rnn(\n",
    "                    cell=multi_rnn_cell, inputs=repeated_z, dtype=tf.float32)\n",
    "\n",
    "    def _create_loss_optimizer(self, opt, **param):\n",
    "        with tf.name_scope(\"MSE\"):\n",
    "            reconstr_loss = tf.reduce_sum(\n",
    "                tf.losses.mean_squared_error(\n",
    "                    self.x, self.x_reconstr_mean))\n",
    "        with tf.name_scope(\"KL_divergence\"):\n",
    "            latent_loss = - 0.5 * tf.reduce_sum(1 + self.z_sigma**2\n",
    "                                               - self.z_mean**2\n",
    "                                               + tf.log(1.e-8 + self.z_sigma**2), 1)\n",
    "            self._cost = tf.reduce_mean(reconstr_loss + latent_loss)\n",
    "        # apply gradient clipping\n",
    "        tvars = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self._cost, tvars), 10)\n",
    "        self.train_op = opt(**param).apply_gradients(zip(grads, tvars))\n",
    "\n",
    "    def fit(\n",
    "            self,\n",
    "            X,\n",
    "            learning_rate=0.001,\n",
    "            batch_size=100,\n",
    "            num_epochs=200,\n",
    "            opt=tf.train.AdamOptimizer,\n",
    "            REG_LAMBDA=0,\n",
    "            grad_clip_norm=10,\n",
    "            optimizer_params=None,\n",
    "            verbose=True):      \n",
    "        \n",
    "        if len(np.shape(X)) != 3:\n",
    "            raise ValueError(\n",
    "                'Input must be a 3-D array. I could reshape it for you, but I am too lazy.'\n",
    "                ' \\n            Use input.reshape(-1,timesteps,1).')\n",
    "        if optimizer_params is None:\n",
    "            optimizer_params = {}\n",
    "            optimizer_params['learning_rate'] = learning_rate\n",
    "        else:\n",
    "            optimizer_params = dict(six.iteritems(optimizer_params))\n",
    "\n",
    "        self._create_loss_optimizer(opt, **optimizer_params)\n",
    "        lstm_var = tf.get_collection(\n",
    "            tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "            scope='LSTM_encoder')\n",
    "        self._cost += REG_LAMBDA * tf.reduce_mean(tf.nn.l2_loss(lstm_var))\n",
    "\n",
    "        config = tf.ConfigProto(allow_soft_placement=True)\n",
    "        config.gpu_options.allow_growth = True\n",
    "\n",
    "        self.sess = tf.Session(config=config)\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "        print (\"batch_size\",batch_size)\n",
    "        print (\"self.input\", self.input)\n",
    "        self.sess.run(\n",
    "            self.ite.initializer,\n",
    "            feed_dict={\n",
    "                self.input: X,\n",
    "                self.batch_size: batch_size})\n",
    "        batches_per_epoch = int(np.ceil(len(X) / batch_size))\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"Training...\")\n",
    "        print(\"\\n\")\n",
    "        start = timer()\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            train_error = 0\n",
    "            for step in range(batches_per_epoch):\n",
    "                if self.stateful:\n",
    "                    loss, _, s, _ = self.sess.run([self._cost, self.train_op, self.update_op, self.update_op_],\n",
    "                                                  feed_dict={self.repeat: np.shape(X)[1], self.batch_: batch_size})\n",
    "                else:\n",
    "                    loss, _ = self.sess.run([self._cost, self.train_op], feed_dict={\n",
    "                                            self.repeat: np.shape(X)[1]})\n",
    "                #print (\"loss\", loss)\n",
    "                train_error += loss\n",
    "            if step == (batches_per_epoch - 1):\n",
    "                mean_loss = train_error / batches_per_epoch\n",
    "\n",
    "                if self.stateful:  # reset cell & hidden states between epochs\n",
    "                    self.sess.run([self.reset_state_op],\n",
    "                                  feed_dict={self.batch_: batch_size})\n",
    "                    self.sess.run([self.reset_state_op_],\n",
    "                                  feed_dict={self.batch_: batch_size})\n",
    "            if epoch % 10 == 0 & verbose:\n",
    "                print(\n",
    "                    \"Epoch {:^6} Loss {:0.5f}\"  .format(\n",
    "                        epoch + 1, mean_loss))\n",
    "        end = timer()\n",
    "        print(\"\\n\")\n",
    "        print(\"Training time {:0.2f} minutes\".format((end - start) / (60)))\n",
    "\n",
    "    def reconstruct(self, X, get_error=False):\n",
    "        self.sess.run(\n",
    "            self.ite.initializer,\n",
    "            feed_dict={\n",
    "                self.input: X,\n",
    "                self.batch_size: np.shape(X)[0]})\n",
    "        if self.stateful:\n",
    "            _, _ = self.sess.run([self.reset_state_op, self.reset_state_op_], feed_dict={\n",
    "                                 self.batch_: np.shape(X)[0]})\n",
    "            x_rec, _, _ = self.sess.run([self.x_reconstr_mean, self.update_op, self.update_op_], feed_dict={\n",
    "                                        self.batch_: np.shape(X)[0], self.repeat: np.shape(X)[1]})\n",
    "        else:\n",
    "            x_rec = self.sess.run(self.x_reconstr_mean,\n",
    "                                  feed_dict={self.repeat: np.shape(X)[1]})\n",
    "        if get_error:\n",
    "            squared_error = (x_rec - X)**2\n",
    "            return x_rec, squared_error\n",
    "        else:\n",
    "            return x_rec\n",
    "\n",
    "    def reduce(self, X):\n",
    "        self.sess.run(\n",
    "            self.ite.initializer,\n",
    "            feed_dict={\n",
    "                self.input: X,\n",
    "                self.batch_size: np.shape(X)[0]})\n",
    "        if self.stateful:\n",
    "            _ = self.sess.run([self.reset_state_op], feed_dict={\n",
    "                              self.batch_: np.shape(X)[0]})\n",
    "            x, _ = self.sess.run([self.z, self.update_op], feed_dict={\n",
    "                                 self.batch_: np.shape(X)[0], self.repeat: np.shape(X)[1]})\n",
    "        else:\n",
    "            x = self.sess.run(self.z)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = X_train.iloc[:,:-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = X_train_.reshape(-1,3,23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1390382, 3, 23)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.24511543634190078, 0.007168458781362007, 0.007168458781362007,\n",
       "        0.00286028602860286, 1.0, 1.0, 1.0, 0.7834072999927885, 1.0, 1.0,\n",
       "        1.0, 1.0, 0.0, 0.025210084033613446, 1.0, 0.07716814714837252,\n",
       "        1.0, 1.0, 1.0, 1.0, 0.6521739130434783, 0.7457627118644068,\n",
       "        0.11864406779661017],\n",
       "       [0.7945588015540944, 0.007168458781362007, 0.007168458781362007,\n",
       "        0.047804239883447806, 1.0, 1.0, 1.0, 0.7834072999927885, 1.0,\n",
       "        1.0, 1.0, 1.0, 0.0, 0.15966386554621848, 0.06928499002387728,\n",
       "        0.0001098590665118177, 1.0, 0.11813955687056894, 1.0, 1.0,\n",
       "        0.6086956521739131, 0.8983050847457628, 0.711864406779661],\n",
       "       [0.24511543634190078, 0.007168458781362007, 0.007168458781362007,\n",
       "        0.01238069752921238, 1.0, 1.0, 1.0, 0.7834072999927885, 1.0, 1.0,\n",
       "        1.0, 1.0, 0.0, 0.23529411764705882, 1.0, 0.07716814714837252,\n",
       "        1.0, 1.0, 1.0, 1.0, 0.6521739130434783, 0.3728813559322034,\n",
       "        0.7288135593220338]], dtype=object)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fe30c7b9cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fe30c7b9cc0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fe30c7b9cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fe30c7b9cc0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7fe23db767b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7fe23db767b8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7fe23db767b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7fe23db767b8>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fe23db76710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fe23db76710>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fe23db76710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fe23db76710>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fe23db76e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fe23db76e80>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fe23db76e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method LSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7fe23db76e80>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    }
   ],
   "source": [
    "vae = LSTM_Var_Autoencoder(intermediate_dim = 15,z_dim = 10, n_dim=23, stateful = False) #default stateful = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 100\n",
      "self.input Tensor(\"Placeholder:0\", shape=(?, ?, 23), dtype=float32)\n",
      "\n",
      "\n",
      "Training...\n",
      "\n",
      "\n",
      "Epoch   1    Loss -493545247081190592.00000\n",
      "Epoch   11   Loss nan\n"
     ]
    }
   ],
   "source": [
    "vae.fit(X_train_, learning_rate=0.001, batch_size = 100, num_epochs = 100, opt = tf.train.AdamOptimizer, REG_LAMBDA = 0,\n",
    "            grad_clip_norm=10, optimizer_params=None, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature_ = Feature.iloc[:3,:-1].values\n",
    "\n",
    "Feature_ = Feature_.reshape(-1,3,23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_reconstructed, recons_error = vae.reconstruct(Feature_, get_error = True) #returns squared error\n",
    "\n",
    "x_reduced = vae.reduce(Feature_) #latent space representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recons_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "18381/5/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
