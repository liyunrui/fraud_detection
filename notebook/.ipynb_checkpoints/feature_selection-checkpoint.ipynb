{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "python3 main.py ../../dataset/train.csv ../../dataset/test.csv ../result/cv_results.csv ../result/submission.csv > ../result/logs.txt\n",
    "\n",
    "make train\n",
    "\n",
    "\"\"\"\n",
    "import sys\n",
    "sys.path.append(\"../fraud_detection/src/\")\n",
    "import time\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import numpy as np\n",
    "from contextlib import contextmanager\n",
    "import gc \n",
    "from util import s_to_time_format, string_to_datetime, hour_to_range, kfold_lightgbm, kfold_xgb\n",
    "from util import rolling_stats_target_by_cols\n",
    "#from util import _time_elapsed_between_last_transactions,time_elapsed_between_last_transactions\n",
    "#from util import num_transaction_in_past_n_days\n",
    "#from util import add_auto_encoder_feature\n",
    "#from util import group_target_by_cols_split_by_users\n",
    "from time import strftime, localtime\n",
    "import logging\n",
    "import sys\n",
    "from config import Configs\n",
    "\n",
    "# logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "#log_file = '{}-{}-{}.log'.format(opt.model_name, opt.dataset, strftime(\"%y%m%d-%H%M\", localtime()))\n",
    "log_file = '../fraud_detection/result/fs_{}.log'.format(strftime(\"%y%m%d-%H%M\", localtime()))\n",
    "logger.addHandler(logging.FileHandler(log_file))\n",
    "\n",
    "def group_target_by_cols(df_train, df_test, recipe):\n",
    "    df = pd.concat([df_train, df_test], axis = 0)\n",
    "    for m in range(len(recipe)):\n",
    "        cols = recipe[m][0]\n",
    "        for n in range(len(recipe[m][1])):\n",
    "            target = recipe[m][1][n][0]\n",
    "            method = recipe[m][1][n][1]\n",
    "            name_grouped_target = method+\"_\"+target+'_BY_'+'_'.join(cols)\n",
    "            tmp = df[cols + [target]].groupby(cols).agg(method)\n",
    "            tmp = tmp.reset_index().rename(index=str, columns={target: name_grouped_target})\n",
    "            df_train = df_train.merge(tmp, how='left', on=cols)\n",
    "            df_test = df_test.merge(tmp, how='left', on=cols)\n",
    "\n",
    "        # reduced memory    \n",
    "        del tmp\n",
    "        gc.collect()\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    logger.info(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
    "    \n",
    "def main(args):\n",
    "    if args.load_feature == True:\n",
    "        with timer(\"Load train/test features extracted\"):\n",
    "            #-------------------------\n",
    "            # load dataset\n",
    "            #-------------------------\n",
    "            df_train = pd.read_csv(\"../fraud_detection/features/train.csv\")\n",
    "            df_test = pd.read_csv(\"../fraud_detection/features/test.csv\")\n",
    "\n",
    "            #-------------------------\n",
    "            # pre-processing\n",
    "            #-------------------------\n",
    "\n",
    "            for cat in Configs.CATEGORY:\n",
    "                df_train[cat] = df_train[cat].astype('category') #.cat.codes\n",
    "                df_test[cat] = df_test[cat].astype('category')\n",
    "            for df in [df_train, df_test]:\n",
    "                df[\"hour_range\"] = df[\"hour_range\"].astype('category')\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))     \n",
    "\n",
    "    else:\n",
    "        with timer(\"Process train/test application\"):\n",
    "            #-------------------------\n",
    "            # load dataset\n",
    "            #-------------------------\n",
    "            df_train = pd.read_csv(args.train_file)\n",
    "            df_test = pd.read_csv(args.test_file)\n",
    "\n",
    "            #-------------------------\n",
    "            # pre-processing\n",
    "            #-------------------------\n",
    "\n",
    "            for cat in Configs.CATEGORY:\n",
    "                df_train[cat] = df_train[cat].astype('category') #.cat.codes\n",
    "                df_test[cat] = df_test[cat].astype('category')\n",
    "                \n",
    "            for df in [df_train, df_test]:\n",
    "                # pre-processing\n",
    "                df[\"loctm_\"] = df.loctm.astype(int).astype(str)\n",
    "                df.loctm_ = df.loctm_.apply(s_to_time_format).apply(string_to_datetime)\n",
    "                # # time-related feature\n",
    "                df[\"loctm_hour_of_day\"] = df.loctm_.apply(lambda x: x.hour).astype('category')\n",
    "                df[\"loctm_minute_of_hour\"] = df.loctm_.apply(lambda x: x.minute)\n",
    "                df[\"loctm_second_of_min\"] = df.loctm_.apply(lambda x: x.second)\n",
    "                # df[\"loctm_absolute_time\"] = [h*60+m for h,m in zip(df.loctm_hour_of_day,df.loctm_minute_of_hour)]\n",
    "                df[\"hour_range\"] = df.loctm_.apply(lambda x: hour_to_range(x.hour)).astype(\"category\")\n",
    "                # removed the columns no need\n",
    "                df.drop(columns = [\"loctm_\"], axis = 1, inplace = True)\n",
    "                # auxiliary fields\n",
    "                df[\"day_hr_min\"] = [\"{}:{}:{}\".format(i,j,k) for i,j,k in zip(df.locdt,df.loctm_hour_of_day,df.loctm_minute_of_hour)]\n",
    "                df[\"day_hr_min_sec\"] = [\"{}:{}:{}:{}\".format(i,j,k,z) for i,j,k,z in zip(df.locdt,df.loctm_hour_of_day,df.loctm_minute_of_hour,df.loctm_second_of_min)]\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add bacno/cano feature\"):\n",
    "            df_train, df_test = group_target_by_cols(df_train, df_test, Configs.CONAM_AGG_RECIPE_1)\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add iterm-related feature\"):\n",
    "            df_train, df_test = group_target_by_cols(df_train, df_test, Configs.ITERM_AGG_RECIPE)\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add conam-related feature\"):\n",
    "            df_train, df_test = group_target_by_cols(df_train, df_test, Configs.CONAM_AGG_RECIPE_2)\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add hour-related feature\"):\n",
    "            df_train, df_test = group_target_by_cols(df_train, df_test, Configs.HOUR_AGG_RECIPE)\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add cano/conam feature\"):\n",
    "            df_train, df_test = group_target_by_cols(df_train, df_test, Configs.CANO_CONAM_COUNT_RECIPE)\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add cano/bacno latent feature\"):\n",
    "            df = pd.read_csv(\"../fraud_detection/features/bacno_latent_features_w_cano.csv\")\n",
    "            df_train = df_train.merge(df, on = \"bacno\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"bacno\", how = \"left\")\n",
    "            df = pd.read_csv(\"../fraud_detection/features/bacno_cano_latent_features.csv\")\n",
    "            df_train = df_train.merge(df, on = \"cano\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"cano\", how = \"left\")\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add locdt-related feature\"):\n",
    "            df_train, df_test = group_target_by_cols(df_train, df_test, Configs.LOCDT_CONAM_RECIPE)\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add mchno-related feature\"):\n",
    "            df_train, df_test = group_target_by_cols(df_train, df_test, Configs.MCHNO_CONAM_RECIPE)\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add scity-related feature\"):\n",
    "            df_train, df_test = group_target_by_cols(df_train, df_test, Configs.SCITY_CONAM_RECIPE)\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add stocn-related feature\"):\n",
    "            df_train, df_test = group_target_by_cols(df_train, df_test, Configs.STOCN_CONAM_RECIPE)\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add mchno/bacno latent feature\"):\n",
    "            df = pd.read_csv(\"../fraud_detection/features/bacno_latent_features_w_mchno.csv\")\n",
    "            df_train = df_train.merge(df, on = \"bacno\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"bacno\", how = \"left\")\n",
    "            df = pd.read_csv(\"../fraud_detection/features/bacno_mchno_latent_features.csv\")\n",
    "            df_train = df_train.merge(df, on = \"mchno\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"mchno\", how = \"left\")\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add time second-level feature on bacno\"):\n",
    "            df_train, df_test = group_target_by_cols(\n",
    "                df_train, \n",
    "                df_test, \n",
    "                Configs.HOUR_AGG_SEC_LEVEL_RECIPE_BACNO,\n",
    "                )\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add time second-level feature on cano\"):\n",
    "            df_train, df_test = group_target_by_cols(\n",
    "                df_train, \n",
    "                df_test, \n",
    "                Configs.HOUR_AGG_SEC_LEVEL_RECIPE_CANO,\n",
    "                )\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add time second-level feature on mchno\"):\n",
    "            df_train, df_test = group_target_by_cols(\n",
    "                df_train, \n",
    "                df_test, \n",
    "                Configs.HOUR_AGG_SEC_LEVEL_RECIPE_MCHNO,\n",
    "                )\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add time second-level feature on csmcu/stocn/scity\"):\n",
    "            df_train, df_test = group_target_by_cols(\n",
    "                df_train, \n",
    "                df_test, \n",
    "                Configs.HOUR_AGG_SEC_LEVEL_RECIPE,\n",
    "                )\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add time second-level feature on acqic/csmcu/stocn/scity\"):\n",
    "            df_train, df_test = group_target_by_cols(\n",
    "                df_train, \n",
    "                df_test, \n",
    "                Configs.HOUR_AGG_SEC_LEVEL_RECIPE_2,\n",
    "                )\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add conam-related feature v3\"):\n",
    "            df_train, df_test = group_target_by_cols(\n",
    "                df_train, \n",
    "                df_test, \n",
    "                Configs.CONAM_AGG_RECIPE_3,\n",
    "                )\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add locdt-related feature v2\"):\n",
    "            df_train, df_test = group_target_by_cols(df_train, df_test, Configs.LOCDT_CONAM_RECIPE_2)\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add conam-related feature v4\"):\n",
    "            df_train, df_test = group_target_by_cols(\n",
    "                df_train, \n",
    "                df_test, \n",
    "                Configs.CONAM_AGG_RECIPE_4,\n",
    "                )\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add cano/mchno latent feature\"):\n",
    "            df = pd.read_csv(\"../features/cano_latent_features_w_mchno.csv\")\n",
    "            df_train = df_train.merge(df, on = \"cano\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"cano\", how = \"left\")\n",
    "            df = pd.read_csv(\"../features/cano_mchno_latent_features.csv\")\n",
    "            df_train = df_train.merge(df, on = \"mchno\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"mchno\", how = \"left\")\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add cano/locdt latent feature\"):\n",
    "            df = pd.read_csv(\"../features/cano_latent_features_w_locdt.csv\")\n",
    "            df_train = df_train.merge(df, on = \"cano\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"cano\", how = \"left\")\n",
    "            df = pd.read_csv(\"../features/cano_locdt_latent_features.csv\")\n",
    "            df_train = df_train.merge(df, on = \"locdt\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"locdt\", how = \"left\")\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add mchno/locdt latent feature\"):\n",
    "            df = pd.read_csv(\"../features/mchno_latent_features_w_locdt.csv\")\n",
    "            df_train = df_train.merge(df, on = \"mchno\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"mchno\", how = \"left\")\n",
    "            df = pd.read_csv(\"../features/mchno_locdt_latent_features.csv\")\n",
    "            df_train = df_train.merge(df, on = \"locdt\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"locdt\", how = \"left\")\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add mchno time aggregate average feature\"):\n",
    "            # df = pd.read_csv(\"../features/average_mchno_time_agg.csv\")\n",
    "            # df_train = df_train.merge(df, on = \"txkey\", how = \"left\")\n",
    "            # df_test = df_test.merge(df, on = \"txkey\", how = \"left\")\n",
    "            df = pd.read_csv(\"../features/average_mchno_mean_conam_in_past_7_days.csv\")\n",
    "            df_train = df_train.merge(df, on = \"mchno\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"mchno\", how = \"left\")\n",
    "\n",
    "            df = pd.read_csv(\"../features/average_mchno_mean_conam_in_past_14_days.csv\")\n",
    "            df_train = df_train.merge(df, on = \"mchno\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"mchno\", how = \"left\")\n",
    "\n",
    "            df = pd.read_csv(\"../features/average_mchno_std_conam_in_past_7_days.csv\")\n",
    "            df_train = df_train.merge(df, on = \"mchno\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"mchno\", how = \"left\")\n",
    "\n",
    "            df = pd.read_csv(\"../features/average_mchno_std_conam_in_past_14_days.csv\")\n",
    "            df_train = df_train.merge(df, on = \"mchno\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"mchno\", how = \"left\")\n",
    "\n",
    "            df = pd.read_csv(\"../features/average_mchno_min_conam_in_past_7_days.csv\")\n",
    "            df_train = df_train.merge(df, on = \"mchno\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"mchno\", how = \"left\")\n",
    "\n",
    "            df = pd.read_csv(\"../features/average_mchno_min_conam_in_past_14_days.csv\")\n",
    "            df_train = df_train.merge(df, on = \"mchno\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"mchno\", how = \"left\")\n",
    "\n",
    "            df = pd.read_csv(\"../features/average_mchno_max_conam_in_past_7_days.csv\")\n",
    "            df_train = df_train.merge(df, on = \"mchno\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"mchno\", how = \"left\")\n",
    "\n",
    "            df = pd.read_csv(\"../features/average_mchno_max_conam_in_past_14_days.csv\")\n",
    "            df_train = df_train.merge(df, on = \"mchno\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"mchno\", how = \"left\")\n",
    "\n",
    "            df = pd.read_csv(\"../features/average_mchno_median_conam_in_past_7_days.csv\")\n",
    "            df_train = df_train.merge(df, on = \"mchno\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"mchno\", how = \"left\")\n",
    "\n",
    "            df = pd.read_csv(\"../features/average_mchno_median_conam_in_past_14_days.csv\")\n",
    "            df_train = df_train.merge(df, on = \"mchno\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"mchno\", how = \"left\")\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add bacno time aggregate average feature\"):\n",
    "        df = pd.read_csv(\"../features/average_bacno_min_conam_in_past_7_days.csv\").iloc[:,1:]\n",
    "        df_train = df_train.merge(df, on = \"bacno\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"bacno\", how = \"left\")\n",
    "\n",
    "        df = pd.read_csv(\"../features/average_bacno_max_conam_in_past_7_days.csv\").iloc[:,1:]\n",
    "        df_train = df_train.merge(df, on = \"bacno\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"bacno\", how = \"left\")\n",
    "\n",
    "        df = pd.read_csv(\"../features/average_bacno_mean_conam_in_past_7_days.csv\").iloc[:,1:]\n",
    "        df_train = df_train.merge(df, on = \"bacno\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"bacno\", how = \"left\")\n",
    "\n",
    "        df = pd.read_csv(\"../features/average_bacno_median_conam_in_past_7_days.csv\").iloc[:,1:]\n",
    "        df_train = df_train.merge(df, on = \"bacno\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"bacno\", how = \"left\")\n",
    "\n",
    "        df = pd.read_csv(\"../features/average_bacno_std_conam_in_past_7_days.csv\").iloc[:,1:]\n",
    "        df_train = df_train.merge(df, on = \"bacno\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"bacno\", how = \"left\")\n",
    "\n",
    "        df = pd.read_csv(\"../features/average_bacno_min_conam_in_past_14_days.csv\").iloc[:,1:]\n",
    "        df_train = df_train.merge(df, on = \"bacno\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"bacno\", how = \"left\")\n",
    "\n",
    "        df = pd.read_csv(\"../features/average_bacno_max_conam_in_past_14_days.csv\").iloc[:,1:]\n",
    "        df_train = df_train.merge(df, on = \"bacno\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"bacno\", how = \"left\")\n",
    "\n",
    "        df = pd.read_csv(\"../features/average_bacno_mean_conam_in_past_14_days.csv\").iloc[:,1:]\n",
    "        df_train = df_train.merge(df, on = \"bacno\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"bacno\", how = \"left\")\n",
    "\n",
    "        df = pd.read_csv(\"../features/average_bacno_median_conam_in_past_14_days.csv\").iloc[:,1:]\n",
    "        df_train = df_train.merge(df, on = \"bacno\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"bacno\", how = \"left\")\n",
    "\n",
    "        df = pd.read_csv(\"../features/average_bacno_std_conam_in_past_14_days.csv\").iloc[:,1:]\n",
    "        df_train = df_train.merge(df, on = \"bacno\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"bacno\", how = \"left\")\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add mcc time aggregate average feature\"):\n",
    "        df = pd.read_csv(\"../features/average_mcc_median_conam_in_past_7_days.csv\")\n",
    "        df_train = df_train.merge(df, on = \"mcc\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"mcc\", how = \"left\")\n",
    "\n",
    "        df = pd.read_csv(\"../features/average_mcc_max_conam_in_past_7_days.csv\")\n",
    "        df_train = df_train.merge(df, on = \"mcc\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"mcc\", how = \"left\")\n",
    "\n",
    "        df = pd.read_csv(\"../features/average_mcc_min_conam_in_past_7_days.csv\")\n",
    "        df_train = df_train.merge(df, on = \"mcc\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"mcc\", how = \"left\")\n",
    "\n",
    "        df = pd.read_csv(\"../features/average_mcc_mean_conam_in_past_7_days.csv\")\n",
    "        df_train = df_train.merge(df, on = \"mcc\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"mcc\", how = \"left\")\n",
    "\n",
    "        df = pd.read_csv(\"../features/average_mcc_std_conam_in_past_7_days.csv\")\n",
    "        df_train = df_train.merge(df, on = \"mcc\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"mcc\", how = \"left\")\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add scity time aggregate feature\"):\n",
    "        df = pd.read_csv(\"../features/scity_mean_conam_in_past_7_days.csv\")\n",
    "        df_train = df_train.merge(df, on = [\"scity\",\"locdt\"], how = \"left\")\n",
    "        df_test = df_test.merge(df, on = [\"scity\",\"locdt\"], how = \"left\")\n",
    "\n",
    "        df = pd.read_csv(\"../features/scity_mean_conam_in_past_14_days.csv\")\n",
    "        df_train = df_train.merge(df, on = [\"scity\",\"locdt\"], how = \"left\")\n",
    "        df_test = df_test.merge(df, on = [\"scity\",\"locdt\"], how = \"left\")\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add stocn time aggregate feature\"):\n",
    "        df = pd.read_csv(\"../features/stocn_mean_conam_in_past_7_days.csv\")\n",
    "        df_train = df_train.merge(df, on = [\"stocn\",\"locdt\"], how = \"left\")\n",
    "        df_test = df_test.merge(df, on = [\"stocn\",\"locdt\"], how = \"left\")\n",
    "\n",
    "        df = pd.read_csv(\"../features/stocn_mean_conam_in_past_14_days.csv\")\n",
    "        df_train = df_train.merge(df, on = [\"stocn\",\"locdt\"], how = \"left\")\n",
    "        df_test = df_test.merge(df, on = [\"stocn\",\"locdt\"], how = \"left\")\n",
    "        \n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add acqic time aggregate feature\"):\n",
    "        df = pd.read_csv(\"../features/acqic_mean_conam_in_past_7_days.csv\")\n",
    "        df_train = df_train.merge(df, on = [\"acqic\",\"locdt\"], how = \"left\")\n",
    "        df_test = df_test.merge(df, on = [\"acqic\",\"locdt\"], how = \"left\")\n",
    "\n",
    "        df = pd.read_csv(\"../features/acqic_mean_conam_in_past_14_days.csv\")\n",
    "        df_train = df_train.merge(df, on = [\"acqic\",\"locdt\"], how = \"left\")\n",
    "        df_test = df_test.merge(df, on = [\"acqic\",\"locdt\"], how = \"left\")\n",
    "        \n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add mchno time aggregate feature\"):\n",
    "        df = pd.read_csv(\"../features/mchno_mean_conam_in_past_7_days.csv\")\n",
    "        df_train = df_train.merge(df, on = [\"mchno\",\"locdt\"], how = \"left\")\n",
    "        df_test = df_test.merge(df, on = [\"mchno\",\"locdt\"], how = \"left\")\n",
    "\n",
    "        df = pd.read_csv(\"../features/mchno_mean_conam_in_past_14_days.csv\")\n",
    "        df_train = df_train.merge(df, on = [\"mchno\",\"locdt\"], how = \"left\")\n",
    "        df_test = df_test.merge(df, on = [\"mchno\",\"locdt\"], how = \"left\")\n",
    "        \n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))   \n",
    "        \n",
    "    return df_train, df_test\n",
    "\n",
    "#         ITERATION = (5 if args.TEST_NULL_HYPO else 1)\n",
    "#         feature_importance_df = pd.DataFrame()\n",
    "#         over_iterations_val_auc = np.zeros(ITERATION)\n",
    "#         for i in range(ITERATION):\n",
    "#             logger.info('Iteration %i' %i)\n",
    "#             if args.model == \"lgb\":    \n",
    "#                 iter_feat_imp, over_folds_val_auc = kfold_lightgbm(df_train, df_test, num_folds = args.NUM_FOLDS, args = args, stratified = args.STRATIFIED, seed = args.SEED, logger = logger)\n",
    "#             elif args.model == \"xgb\":\n",
    "#                 iter_feat_imp, over_folds_val_auc = kfold_xgb(df_train, df_test, num_folds = args.NUM_FOLDS, args = args, stratified = args.STRATIFIED, seed = args.SEED, logger = logger)\n",
    "#             else:\n",
    "#                 print(\"Now we only support LightGBM or Xgboost model!\")           \n",
    "#             feature_importance_df = pd.concat([feature_importance_df, iter_feat_imp], axis=0)\n",
    "#             over_iterations_val_auc[i] = over_folds_val_auc\n",
    "\n",
    "#         logger.info('============================================\\nOver-iterations val f1 score %.6f' %over_iterations_val_auc.mean())\n",
    "#         logger.info('Standard deviation %.6f\\n============================================' %over_iterations_val_auc.std())\n",
    "    \n",
    "#     if args.feature_importance_plot == True:\n",
    "#         from util import display_importances\n",
    "#         display_importances(feature_importance_df, args.model)\n",
    "        \n",
    "#     feature_importance_df_median = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").median().sort_values(by=\"importance\", ascending=False)\n",
    "#     useless_features_df = feature_importance_df_median.loc[feature_importance_df_median['importance'] == 0]\n",
    "#     feature_importance_df_mean = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "#     if args.TEST_NULL_HYPO:\n",
    "#         feature_importance_df_mean.to_csv(\"../fraud_detection/result/feature_importance-null_hypo.csv\", index = True)\n",
    "#     else:\n",
    "#         feature_importance_df_mean.to_csv(\"../fraud_detection/result/feature_importance.csv\", index = True)\n",
    "#         useless_features_list = useless_features_df.index.tolist()\n",
    "#         logger.info('Useless features: \\'' + '\\', \\''.join(useless_features_list) + '\\'')\n",
    "#     return feature_importance_df_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_file': '/data/yunrui_li/fraud/dataset/train.csv',\n",
       " 'test_file': '/data/yunrui_li/fraud/dataset/test.csv',\n",
       " 'result_path': '/data/yunrui_li/fraud/fraud_detection/result/submission.csv',\n",
       " 'feature_selection': True,\n",
       " 'feature_importance_plot': False,\n",
       " 'SEED': 1030,\n",
       " 'NUM_FOLDS': 5,\n",
       " 'CPU_USE_RATE': 1.0,\n",
       " 'STRATIFIED': True,\n",
       " 'NUM_LEAVES': 31,\n",
       " 'COLSAMPLE_BYTREE': 1.0,\n",
       " 'SUBSAMPLE': 1.0,\n",
       " 'SUBSAMPLE_FREQ': 0,\n",
       " 'MAX_DEPTH': -1,\n",
       " 'REG_ALPHA': 0.0,\n",
       " 'REG_LAMBDA': 0.0,\n",
       " 'MIN_SPLIT_GAIN': 0.0,\n",
       " 'MIN_CHILD_WEIGHT': 0.001,\n",
       " 'MAX_BIN': 255,\n",
       " 'SCALE_POS_WEIGHT': 3,\n",
       " 'TEST_NULL_HYPO': False,\n",
       " 'model': 'lgb',\n",
       " 'ensemble': False,\n",
       " 'seed': 1030,\n",
       " 'load_feature': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = {\n",
    " \"train_file\":\"/data/yunrui_li/fraud/dataset/train.csv\",\n",
    " \"test_file\":\"/data/yunrui_li/fraud/dataset/test.csv\",\n",
    " \"result_path\":\"/data/yunrui_li/fraud/fraud_detection/result/submission.csv\",\n",
    " \"feature_selection\":True,\n",
    " \"feature_importance_plot\": False,\n",
    " \"SEED\": 1030,\n",
    " \"NUM_FOLDS\": 5, # 5\n",
    " \"CPU_USE_RATE\":1.0,\n",
    " \"STRATIFIED\": True,\n",
    " \"NUM_LEAVES\":31,\n",
    " \"COLSAMPLE_BYTREE\":1.0,\n",
    " \"SUBSAMPLE\": 1.0,\n",
    " \"SUBSAMPLE_FREQ\": 0,\n",
    " \"MAX_DEPTH\": -1,\n",
    " \"REG_ALPHA\": 0.0,\n",
    " \"REG_LAMBDA\": 0.0,\n",
    " \"MIN_SPLIT_GAIN\": 0.0,\n",
    " \"MIN_CHILD_WEIGHT\": 0.001,\n",
    " \"MAX_BIN\": 255,\n",
    " \"SCALE_POS_WEIGHT\": 3,\n",
    " \"TEST_NULL_HYPO\":False,\n",
    " \"model\": \"lgb\",\n",
    " \"ensemble\":False,\n",
    " \"seed\":1030,\n",
    " \"load_feature\": True\n",
    "}\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "args = AttrDict(args)\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'../features/train.csv' does not exist: b'../features/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-a79315989ffe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-77956713a831>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;31m# load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m#-------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../features/train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../features/test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/fraud/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/fraud/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/fraud/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/fraud/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/fraud/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'../features/train.csv' does not exist: b'../features/train.csv'"
     ]
    }
   ],
   "source": [
    "df_train, df_test = main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ldap_home/yunrui.li/.pyenv/versions/3.6.5/envs/fraud/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([df_train,df_test], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(\"../fraud_detection/features/features.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x1400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# csmcu & scity positive correlation (幣別與消費城市)\n",
    "plt.figure(figsize = (14,14))\n",
    "plt.title('Credit Card Transactions features correlation plot (Pearson)')\n",
    "sns.heatmap(corr,xticklabels=corr.columns,yticklabels=corr.columns,linewidths=.1,cmap=\"Reds\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a8784ddd4e15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'corr' is not defined"
     ]
    }
   ],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "th = 0.990\n",
    "f = []\n",
    "feature_seen = []\n",
    "for feat1 in corr.columns.tolist():\n",
    "    for feat2, value in corr[feat1].iteritems():\n",
    "        if feat1!=feat2:\n",
    "            if value > th or value < (-1)* th:\n",
    "                if feat2 not in feature_seen:\n",
    "                    f.append((feat1,feat2,value))\n",
    "                    feature_seen.append(feat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_GRAVEYARD = []\n",
    "for fea_pair in f:\n",
    "    if fea_pair[0]!=fea_pair[1]:\n",
    "        if (fea_pair[0]) not in FEATURE_GRAVEYARD and (fea_pair[1]) not in FEATURE_GRAVEYARD:\n",
    "                FEATURE_GRAVEYARD.append(fea_pair[0])\n",
    "            \n",
    "FEATURE_GRAVEYARD = list(set(FEATURE_GRAVEYARD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(FEATURE_GRAVEYARD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sum_conam_BY_bacno_flbmk',\n",
       " 'max_conam_BY_acqic_day_hr_min_sec',\n",
       " 'max_iterm_BY_csmcu',\n",
       " 'var_conam_BY_bacno',\n",
       " 'count_conam_BY_cano_locdt_scity',\n",
       " 'mean_conam_BY_bacno_scity_day_hr_min',\n",
       " 'median_conam_BY_bacno_scity_day_hr_min',\n",
       " 'min_conam_BY_bacno_scity_day_hr_min',\n",
       " 'mean_conam_BY_cano_locdt_scity',\n",
       " 'mean_conam_BY_bacno_stocn_day_hr_min',\n",
       " 'mean_iterm_BY_stocn',\n",
       " 'sum_conam_BY_bacno_locdt_mchno',\n",
       " 'max_conam_BY_bacno_day_hr_min_sec',\n",
       " 'min_conam_BY_bacno_cano',\n",
       " 'max_conam_BY_bacno_day_hr_min',\n",
       " 'max_conam_BY_bacno_locdt_stocn_scity',\n",
       " 'max_conam_BY_bacno_stocn_day_hr_min_sec',\n",
       " 'conam',\n",
       " 'max_conam_BY_bacno',\n",
       " 'median_conam_BY_cano',\n",
       " 'count_conam_BY_csmcu',\n",
       " 'min_conam_BY_bacno_acqic_day_hr_min_sec',\n",
       " 'count_conam_BY_scity_locdt',\n",
       " 'mean_conam_BY_bacno_locdt_stocn',\n",
       " 'min_iterm_BY_bacno_cano',\n",
       " 'sum_conam_BY_stscd',\n",
       " 'mean_conam_BY_bacno',\n",
       " 'count_conam_BY_bacno_cano',\n",
       " 'count_conam_BY_bacno_stocn_day_hr_min_sec',\n",
       " 'var_conam_BY_bacno_locdt_mchno',\n",
       " 'stscd',\n",
       " 'count_conam_BY_bacno_acqic_day_hr_min_sec',\n",
       " 'sum_conam_BY_bacno_stocn_day_hr_min_sec',\n",
       " 'count_conam_BY_bacno_day_hr_min_sec',\n",
       " 'mean_iterm_BY_acqic',\n",
       " 'count_conam_BY_bacno_locdt_mchno',\n",
       " 'mean_iterm_BY_bacno_cano',\n",
       " 'sum_conam_BY_bacno_cano',\n",
       " 'sum_conam_BY_bacno_acqic_day_hr_min_sec',\n",
       " 'sum_conam_BY_bacno_acqic_day_hr_min',\n",
       " 'median_conam_BY_bacno_day_hr_min_sec',\n",
       " 'mean_conam_BY_mchno_day_hr_min',\n",
       " 'sum_conam_BY_bacno',\n",
       " 'sum_conam_BY_bacno_scity_day_hr_min_sec',\n",
       " 'sum_conam_BY_contp',\n",
       " 'mean_conam_BY_bacno_mchno',\n",
       " 'median_conam_BY_cano_day_hr_min_sec',\n",
       " 'median_conam_BY_ovrlt',\n",
       " 'max_conam_BY_cano',\n",
       " 'max_conam_BY_bacno_acqic_day_hr_min',\n",
       " 'min_conam_BY_bacno_acqic_day_hr_min',\n",
       " 'count_conam_BY_bacno_day_hr_min',\n",
       " 'mean_conam_BY_flbmk',\n",
       " 'median_conam_BY_cano_day_hr_min',\n",
       " 'sum_iterm_BY_bacno_cano',\n",
       " 'sum_conam_BY_bacno_locdt_scity',\n",
       " 'sum_conam_BY_loctm_hour_of_day_bacno',\n",
       " 'count_conam_BY_bacno',\n",
       " 'max_conam_BY_flbmk',\n",
       " 'sum_conam_BY_ovrlt',\n",
       " 'var_conam_BY_bacno_locdt_scity',\n",
       " 'count_conam_BY_bacno_locdt_scity',\n",
       " 'median_conam_BY_bacno_acqic_day_hr_min_sec',\n",
       " 'max_conam_BY_bacno_locdt_scity',\n",
       " 'mean_conam_BY_stocn_day_hr_min_sec',\n",
       " 'count_conam_BY_hcefg',\n",
       " 'mean_conam_BY_cano',\n",
       " 'min_conam_BY_bacno_scity_day_hr_min_sec',\n",
       " 'sum_conam_BY_stocn',\n",
       " 'max_conam_BY_bacno_acqic_day_hr_min_sec',\n",
       " 'mean_conam_BY_cano_locdt_stocn_scity',\n",
       " 'median_conam_BY_bacno_acqic_day_hr_min',\n",
       " 'median_conam_BY_bacno_locdt_scity',\n",
       " 'var_conam_BY_bacno_cano',\n",
       " 'max_conam_BY_bacno_cano',\n",
       " 'mean_conam_BY_scity_day_hr_min_sec',\n",
       " 'count_conam_BY_flbmk',\n",
       " 'mean_conam_BY_cano_locdt_stocn',\n",
       " 'min_conam_BY_bacno_day_hr_min_sec',\n",
       " 'max_conam_BY_bacno_locdt_mchno',\n",
       " 'mean_conam_BY_mchno_day_hr_min_sec',\n",
       " 'count_conam_BY_stocn_locdt',\n",
       " 'median_conam_BY_bacno_locdt_stocn_scity',\n",
       " 'count_conam_BY_contp_locdt',\n",
       " 'count_conam_BY_scity',\n",
       " 'mean_conam_BY_bacno_locdt_stocn_scity',\n",
       " 'mean_conam_BY_bacno_acqic_day_hr_min',\n",
       " 'median_conam_BY_mchno_day_hr_min_sec',\n",
       " 'mean_conam_BY_bacno_stocn_day_hr_min_sec',\n",
       " 'mean_conam_BY_acqic_day_hr_min_sec',\n",
       " 'count_conam_BY_stocn',\n",
       " 'max_conam_BY_ovrlt',\n",
       " 'min_conam_BY_cano_day_hr_min_sec',\n",
       " 'max_conam_BY_mchno_day_hr_min_sec',\n",
       " 'min_conam_BY_bacno_stocn_day_hr_min_sec',\n",
       " 'mean_conam_BY_cano_day_hr_min',\n",
       " 'mean_conam_BY_bacno_day_hr_min',\n",
       " 'var_conam_BY_cano_locdt_scity',\n",
       " 'sum_conam_BY_cano_locdt_scity',\n",
       " 'count_conam_BY_bacno_locdt_stocn_scity',\n",
       " 'max_conam_BY_cano_locdt_scity',\n",
       " 'mean_conam_BY_csmcu_day_hr_min_sec',\n",
       " 'min_conam_BY_bacno_stocn_day_hr_min',\n",
       " 'var_conam_BY_bacno_scity_day_hr_min_sec',\n",
       " 'sum_conam_BY_cano',\n",
       " 'var_iterm_BY_bacno_cano',\n",
       " 'min_conam_BY_bacno_locdt_stocn_scity',\n",
       " 'count_conam_BY_bacno_locdt',\n",
       " 'count_conam_BY_contp',\n",
       " 'var_conam_BY_cano',\n",
       " 'median_conam_BY_cano_locdt_scity',\n",
       " 'mean_conam_BY_cano_day_hr_min_sec',\n",
       " 'min_conam_BY_bacno_day_hr_min',\n",
       " 'var_conam_BY_bacno_day_hr_min_sec',\n",
       " 'median_conam_BY_acqic_day_hr_min_sec',\n",
       " 'var_conam_BY_bacno_acqic_day_hr_min_sec',\n",
       " 'median_conam_BY_bacno_cano',\n",
       " 'min_conam_BY_bacno_locdt_mchno',\n",
       " 'count_conam_BY_bacno_locdt_stocn',\n",
       " 'median_conam_BY_bacno_scity_day_hr_min_sec',\n",
       " 'min_conam_BY_bacno_locdt_scity',\n",
       " 'mean_conam_BY_cano_locdt_mchno',\n",
       " 'median_iterm_BY_bacno_cano',\n",
       " 'var_conam_BY_stscd',\n",
       " 'mean_conam_BY_bacno_day_hr_min_sec',\n",
       " 'max_conam_BY_bacno_scity_day_hr_min',\n",
       " 'count_conam_BY_stscd',\n",
       " 'min_conam_BY_acqic_day_hr_min_sec',\n",
       " 'max_conam_BY_bacno_scity_day_hr_min_sec',\n",
       " 'median_conam_BY_flbmk',\n",
       " 'sum_iterm_BY_csmcu',\n",
       " 'sum_iterm_BY_stocn',\n",
       " 'mean_iterm_BY_scity',\n",
       " 'min_conam_BY_cano_locdt_scity',\n",
       " 'max_iterm_BY_bacno_cano',\n",
       " 'mean_conam_BY_ovrlt',\n",
       " 'median_conam_BY_bacno_day_hr_min',\n",
       " 'mean_iterm_BY_stscd',\n",
       " 'var_conam_BY_bacno_day_hr_min',\n",
       " 'sum_conam_BY_bacno_day_hr_min_sec',\n",
       " 'mean_conam_BY_bacno_acqic_day_hr_min_sec',\n",
       " 'median_conam_BY_bacno_stocn_day_hr_min',\n",
       " 'mean_conam_BY_bacno_cano',\n",
       " 'mean_conam_BY_bacno_locdt_mchno',\n",
       " 'count_conam_BY_ovrlt',\n",
       " 'mean_conam_BY_bacno_locdt_scity',\n",
       " 'mean_conam_BY_bacno_scity_day_hr_min_sec',\n",
       " 'median_conam_BY_bacno_locdt_mchno',\n",
       " 'max_conam_BY_stscd',\n",
       " 'sum_conam_BY_bacno_locdt_stocn_scity',\n",
       " 'mean_iterm_BY_csmcu',\n",
       " 'var_conam_BY_bacno_stocn_day_hr_min_sec',\n",
       " 'count_conam_BY_bacno_scity_day_hr_min_sec',\n",
       " 'max_iterm_BY_stocn',\n",
       " 'max_conam_BY_bacno_stocn_day_hr_min',\n",
       " 'max_conam_BY_cano_day_hr_min_sec',\n",
       " 'median_conam_BY_bacno',\n",
       " 'sum_conam_BY_flbmk',\n",
       " 'sum_conam_BY_csmcu',\n",
       " 'sum_conam_BY_bacno_locdt_stocn',\n",
       " 'sum_conam_BY_bacno_day_hr_min',\n",
       " 'median_conam_BY_bacno_stocn_day_hr_min_sec',\n",
       " 'max_conam_BY_stocn']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FEATURE_GRAVEYARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('max_conam_BY_bacno', 'max_conam_BY_bacno_flbmk', 0.9947683007357537)\n"
     ]
    }
   ],
   "source": [
    "for pair in f:\n",
    "    if pair[0]==\"max_conam_BY_bacno\":\n",
    "        print (pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
