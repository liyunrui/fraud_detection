{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-64c79c021cd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;31m# pre-processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loctm_\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloctm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloctm_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloctm_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_to_time_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring_to_datetime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;31m# time-related feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/deepts/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors, **kwargs)\u001b[0m\n\u001b[1;32m   5880\u001b[0m             \u001b[0;31m# else, only a single dtype is given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5881\u001b[0m             new_data = self._data.astype(\n\u001b[0;32m-> 5882\u001b[0;31m                 \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5883\u001b[0m             )\n\u001b[1;32m   5884\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/deepts/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/deepts/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, axes, filter, do_integrity_check, consolidate, **kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malign_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/deepts/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors, values, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"raise\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_astype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_astype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"raise\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/deepts/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36m_astype\u001b[0;34m(self, dtype, copy, errors, values, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m                     \u001b[0;31m# _astype_nansafe works fine with 1-d only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m                     \u001b[0mvals1d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m                     \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mastype_nansafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals1d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0;31m# TODO(extension)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/deepts/lib/python3.6/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mastype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_datetime64_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.astype_str\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"../fraud_detection/src/\")\n",
    "from util import s_to_time_format, string_to_datetime,hour_to_range\n",
    "from tqdm import tqdm\n",
    "\n",
    "def value_to_count(df_train, df_test, df_train_normal_cano_id, df_):\n",
    "    # separate continuous feature and categorial features\n",
    "    feats = ['acqic', 'bacno', 'cano', 'conam', 'contp', 'csmcu', 'ecfg', 'etymd',\n",
    "       'flbmk', 'flg_3dsmk', 'hcefg', 'insfg', 'iterm', 'locdt',\n",
    "       'mcc', 'mchno', 'ovrlt', 'scity', 'stocn', 'stscd', 'loctm_hour_of_day',\n",
    "       'loctm_minute_of_hour', 'loctm_second_of_min'] \n",
    "    cont_feats = ['iterm', \n",
    "                  'locdt',\n",
    "                  'loctm_hour_of_day',\n",
    "                  'loctm_minute_of_hour', \n",
    "                  'loctm_second_of_min']\n",
    "    feats = [f for f in feats if f not in cont_feats]\n",
    "    # we only coner categorial features\n",
    "    \n",
    "    df = pd.concat([df_train[feats], df_test[feats]], axis = 0)\n",
    "    for f in tqdm(feats):\n",
    "        count_dict = df[f].value_counts(dropna = False).to_dict() \n",
    "        df_train_normal_cano_id[f] = df_train_normal_cano_id[f].apply(lambda v: count_dict[v])\n",
    "        df_train[f] = df_train[f].apply(lambda v: count_dict[v])\n",
    "        df_test[f] = df_test[f].apply(lambda v: count_dict[v])\n",
    "        df_[f] = df_[f].apply(lambda v: count_dict[v])\n",
    "    return df_train,df_test,df_train_normal_cano_id, df_\n",
    "\n",
    "def feature_normalization_auto(df_train, df_test, df_train_normal_cano_id,df_):\n",
    "    \"\"\"\n",
    "    return two inputs of autoencoder, one is for train and another one is for test\n",
    "    \"\"\"\n",
    "    #from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n",
    "    feats = ['acqic', 'bacno', 'cano', 'conam', 'contp', 'csmcu', 'ecfg', 'etymd',\n",
    "       'flbmk', 'flg_3dsmk', 'hcefg', 'insfg', 'iterm', 'locdt',\n",
    "       'mcc', 'mchno', 'ovrlt', 'scity', 'stocn', 'stscd', 'loctm_hour_of_day',\n",
    "       'loctm_minute_of_hour', 'loctm_second_of_min']\n",
    "    df = pd.concat([df_train[feats], df_test[feats]], axis = 0)\n",
    "\n",
    "\n",
    "    for f in tqdm(feats):\n",
    "        try:\n",
    "            #scaler = MinMaxScaler()\n",
    "            max_ = df[f].max()\n",
    "            min_ = df[f].min()\n",
    "            df_train_normal_cano_id[f] = df_train_normal_cano_id[f].apply(lambda x: (x-min_)/(max_-min_))\n",
    "            df_[f] = df_[f].apply(lambda x: (x-min_)/(max_-min_))\n",
    "            #df_test[f] = df_test[f].apply(lambda x: (x-min_)/(max_-min_))\n",
    "        except:\n",
    "            print(f)\n",
    "    return df_train_normal_cano_id,df_\n",
    "\n",
    "def partition_(df, num_features):\n",
    "    data = []\n",
    "    for i in range(len(df)):\n",
    "        out = None\n",
    "        if i == 0:\n",
    "            out = np.concatenate(((np.zeros((2,num_features))),df.iloc[:1].values))\n",
    "        elif i== 1:\n",
    "            out = np.concatenate(((np.zeros((1,num_features))),df.iloc[:i+1].values))\n",
    "        else:\n",
    "            out = df.iloc[i+1-3:i+1].values\n",
    "        data.append(out)\n",
    "    return data\n",
    "\n",
    "def partition(df_, sequence_length = 3):\n",
    "    feats = [f for f in df_.columns if f not in {\"fraud_ind\",\"cano_help\",\"locdt_help\"}]\n",
    "    sequences = []\n",
    "    for _, df in df_.groupby(by = \"cano_help\"):\n",
    "        data = partition_(df[feats], num_features = len(feats))\n",
    "        for d in data:\n",
    "            sequences.append(d)\n",
    "    return sequences\n",
    "\n",
    "def get_sequence_dataframe(df):\n",
    "    df_train_sequences = partition(df)\n",
    "    df_train_sequences = np.concatenate(df_train_sequences)\n",
    "    df_train_sequences = pd.DataFrame(df_train_sequences)\n",
    "    return df_train_sequences\n",
    "\n",
    "\n",
    "#-----------------------------\n",
    "# load data\n",
    "#-----------------------------\n",
    "df_train = pd.read_csv(\"/data/yunrui_li/fraud/dataset/train.csv\")\n",
    "df_test = pd.read_csv(\"/data/yunrui_li/fraud/dataset/test.csv\")\n",
    "\n",
    "\n",
    "for df in [df_train, df_test]:\n",
    "    # pre-processing\n",
    "    df[\"loctm_\"] = df.loctm.astype(int).astype(str)\n",
    "    df.loctm_ = df.loctm_.apply(s_to_time_format).apply(string_to_datetime)\n",
    "    # time-related feature\n",
    "    df[\"loctm_hour_of_day\"] = df.loctm_.apply(lambda x: x.hour)\n",
    "    df[\"loctm_minute_of_hour\"] = df.loctm_.apply(lambda x: x.minute)\n",
    "    df[\"loctm_second_of_min\"] = df.loctm_.apply(lambda x: x.second)\n",
    "\n",
    "    # removed the columns no need\n",
    "    df.drop(columns = [\"loctm_\", \"loctm\",\"txkey\"], axis = 1, inplace = True)\n",
    "\n",
    "df_train[\"cano_locdt_index\"] = [\"{}_{}_{}_{}_{}\".format(str(i),str(j),str(k),str(l),str(m)) for i,j,k,l,m in zip(df_train.cano,\n",
    "                                                                                   df_train.locdt,\n",
    "                                                                                   df_train.loctm_hour_of_day,\n",
    "                                                                                   df_train.loctm_minute_of_hour,\n",
    "                                                                                   df_train.loctm_second_of_min,\n",
    "                                                                                  )]\n",
    "df_test[\"cano_locdt_index\"] = [\"{}_{}_{}_{}_{}\".format(str(i),str(j),str(k),str(l),str(m)) for i,j,k,l,m in zip(df_test.cano,\n",
    "                                                                                  df_test.locdt,\n",
    "                                                                                  df_train.loctm_hour_of_day,\n",
    "                                                                                  df_train.loctm_minute_of_hour,\n",
    "                                                                                  df_train.loctm_second_of_min,\n",
    "                                                                                 )]\n",
    "\n",
    "df_train[\"cano_help\"] = df_train.cano\n",
    "df_test[\"cano_help\"] = df_test.cano\n",
    "\n",
    "df_train[\"locdt_help\"] = df_train.locdt\n",
    "df_test[\"locdt_help\"] = df_test.locdt\n",
    "\n",
    "df_train[\"loctm_hour_of_day_help\"] = df_train.loctm_hour_of_day\n",
    "df_test[\"loctm_hour_of_day_help\"] = df_test.loctm_hour_of_day\n",
    "\n",
    "df_train[\"loctm_minute_of_hour_help\"] = df_train.loctm_minute_of_hour\n",
    "df_test[\"loctm_minute_of_hour_help\"] = df_test.loctm_minute_of_hour\n",
    "\n",
    "df_train[\"loctm_second_of_min_help\"] = df_train.loctm_second_of_min\n",
    "df_test[\"loctm_second_of_min_help\"] = df_test.loctm_second_of_min\n",
    "\n",
    "#-----------------------------\n",
    "# feature extraction\n",
    "#-----------------------------\n",
    "df = pd.concat([df_train, df_test], axis = 0)\n",
    "df.sort_values(by = [\"cano\", \"locdt\",\"loctm_hour_of_day\",\"loctm_minute_of_hour\",\"loctm_second_of_min\"], inplace = True)\n",
    "\n",
    "#-----------------------------\n",
    "# prepare training data\n",
    "#-----------------------------\n",
    "df_train.sort_values(by = [\"cano\", \"locdt\",\"loctm_hour_of_day\",\"loctm_minute_of_hour\",\"loctm_second_of_min\"], inplace = True)\n",
    "\n",
    "# df_train, df_test = value_to_count(df_train, df_test)\n",
    "# df_train, df_test = feature_normalization_auto(df_train, df_test)\n",
    "\n",
    "fraud_cano_id = df_train[df_train.fraud_ind == 1].cano.unique().tolist()\n",
    "\n",
    "df_train_normal_cano_id = df_train[~df_train.cano.isin(fraud_cano_id)]\n",
    "print (\"number of training data\",df_train_normal_cano_id.shape)\n",
    "\n",
    "df_train, df_test, df_train_normal_cano_id, df = value_to_count(df_train, df_test,df_train_normal_cano_id, df)\n",
    "df_train_normal_cano_id, df = feature_normalization_auto(df_train, df_test,df_train_normal_cano_id, df)\n",
    "\n",
    "#-----------------------------\n",
    "# post-processing\n",
    "#-----------------------------\n",
    "df.drop(columns = [\"fraud_ind\"], axis = 1, inplace = True)\n",
    "df_train_normal_cano_id.drop(columns = [\"fraud_ind\"], axis = 1, inplace = True)\n",
    "feats = ['acqic', 'bacno', 'cano', 'conam', 'contp', 'csmcu', 'ecfg', 'etymd',\n",
    "   'flbmk', 'flg_3dsmk', 'hcefg', 'insfg', 'iterm', 'locdt',\n",
    "   'mcc', 'mchno', 'ovrlt', 'scity', 'stocn', 'stscd', 'loctm_hour_of_day',\n",
    "   'loctm_minute_of_hour', 'loctm_second_of_min'] + [\"cano_locdt_index\",\"cano_help\",\"locdt_help\",\n",
    "#                                                      \"loctm_hour_of_day_help\",\n",
    "#                                                      \"loctm_minute_of_hour_help\",\n",
    "#                                                      \"loctm_second_of_min_help\",\n",
    "                                                    ]\n",
    "\n",
    "df = df[feats]\n",
    "df_train_normal_cano_id = df_train_normal_cano_id[feats]\n",
    "\n",
    "#-----------------------------\n",
    "# get train/test data\n",
    "#-----------------------------\n",
    "\n",
    "X_train = get_sequence_dataframe(df_train_normal_cano_id)\n",
    "Feature = get_sequence_dataframe(df)\n",
    "#-----------------------------\n",
    "# modeling (unsupervised learning)\n",
    "#-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4171146, 24)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5830356, 24)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.245115</td>\n",
       "      <td>0.00716846</td>\n",
       "      <td>0.00716846</td>\n",
       "      <td>0.0142658</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0771681</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.322034</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0_1_15_19_48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.245115</td>\n",
       "      <td>0.00716846</td>\n",
       "      <td>0.00716846</td>\n",
       "      <td>0.0142658</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0771681</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.322034</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0_1_15_19_48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4171141</td>\n",
       "      <td>0.0418379</td>\n",
       "      <td>0.00268817</td>\n",
       "      <td>0.00268817</td>\n",
       "      <td>0.00118336</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0102579</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00517195</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0573014</td>\n",
       "      <td>0.000470825</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0044661</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.627119</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>213334_90_13_37_22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4171142</td>\n",
       "      <td>0.365315</td>\n",
       "      <td>0.00268817</td>\n",
       "      <td>0.00268817</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0102579</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00517195</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.568704</td>\n",
       "      <td>0.000674849</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>213334_90_15_46_42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4171143</td>\n",
       "      <td>0.0418379</td>\n",
       "      <td>0.00268817</td>\n",
       "      <td>0.00268817</td>\n",
       "      <td>0.00118336</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0102579</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00517195</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0573014</td>\n",
       "      <td>0.000470825</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0044661</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.627119</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>213334_90_13_37_22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4171144</td>\n",
       "      <td>0.365315</td>\n",
       "      <td>0.00268817</td>\n",
       "      <td>0.00268817</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0102579</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00517195</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.568704</td>\n",
       "      <td>0.000674849</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>213334_90_15_46_42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4171145</td>\n",
       "      <td>0.0320937</td>\n",
       "      <td>0.00268817</td>\n",
       "      <td>0.00268817</td>\n",
       "      <td>0.00290191</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0102579</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00517195</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000141247</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000190161</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>0.661017</td>\n",
       "      <td>213334_90_19_46_39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4171146 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0           1           2           3  4          5  6   \\\n",
       "0                0           0           0           0  0          0  0   \n",
       "1                0           0           0           0  0          0  0   \n",
       "2         0.245115  0.00716846  0.00716846   0.0142658  1          1  1   \n",
       "3                0           0           0           0  0          0  0   \n",
       "4         0.245115  0.00716846  0.00716846   0.0142658  1          1  1   \n",
       "...            ...         ...         ...         ... ..        ... ..   \n",
       "4171141  0.0418379  0.00268817  0.00268817  0.00118336  1  0.0102579  1   \n",
       "4171142   0.365315  0.00268817  0.00268817           1  1  0.0102579  1   \n",
       "4171143  0.0418379  0.00268817  0.00268817  0.00118336  1  0.0102579  1   \n",
       "4171144   0.365315  0.00268817  0.00268817           1  1  0.0102579  1   \n",
       "4171145  0.0320937  0.00268817  0.00268817  0.00290191  1  0.0102579  1   \n",
       "\n",
       "               7           8  9   ...         14           15 16           17  \\\n",
       "0               0           0  0  ...          0            0  0            0   \n",
       "1               0           0  0  ...          0            0  0            0   \n",
       "2        0.783407           1  1  ...          1    0.0771681  1            1   \n",
       "3               0           0  0  ...          0            0  0            0   \n",
       "4        0.783407           1  1  ...          1    0.0771681  1            1   \n",
       "...           ...         ... ..  ...        ...          ... ..          ...   \n",
       "4171141         1  0.00517195  0  ...  0.0573014  0.000470825  1    0.0044661   \n",
       "4171142         1  0.00517195  0  ...   0.568704  0.000674849  1            1   \n",
       "4171143         1  0.00517195  0  ...  0.0573014  0.000470825  1    0.0044661   \n",
       "4171144         1  0.00517195  0  ...   0.568704  0.000674849  1            1   \n",
       "4171145         1  0.00517195  0  ...          1  0.000141247  1  0.000190161   \n",
       "\n",
       "        18 19        20        21        22                  23  \n",
       "0        0  0         0         0         0                   0  \n",
       "1        0  0         0         0         0                   0  \n",
       "2        1  1  0.652174  0.322034  0.813559        0_1_15_19_48  \n",
       "3        0  0         0         0         0                   0  \n",
       "4        1  1  0.652174  0.322034  0.813559        0_1_15_19_48  \n",
       "...     .. ..       ...       ...       ...                 ...  \n",
       "4171141  1  1  0.565217  0.627119  0.372881  213334_90_13_37_22  \n",
       "4171142  1  1  0.652174  0.779661  0.711864  213334_90_15_46_42  \n",
       "4171143  1  1  0.565217  0.627119  0.372881  213334_90_13_37_22  \n",
       "4171144  1  1  0.652174  0.779661  0.711864  213334_90_15_46_42  \n",
       "4171145  1  1  0.826087  0.779661  0.661017  213334_90_19_46_39  \n",
       "\n",
       "[4171146 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.245115</td>\n",
       "      <td>0.00716846</td>\n",
       "      <td>0.00716846</td>\n",
       "      <td>0.0142658</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0771681</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.322034</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0_1_15_19_48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.245115</td>\n",
       "      <td>0.00716846</td>\n",
       "      <td>0.00716846</td>\n",
       "      <td>0.0142658</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0771681</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.322034</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0_1_15_19_48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5830351</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5830352</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.94654e-06</td>\n",
       "      <td>0.0533723</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.328259</td>\n",
       "      <td>0.0398945</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>213572_120_18_49_52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5830353</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5830354</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5830355</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0533723</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.328259</td>\n",
       "      <td>0.0398945</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.0508475</td>\n",
       "      <td>0.389831</td>\n",
       "      <td>213575_120_21_45_3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5830356 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2            3          4  5  6   \\\n",
       "0               0           0           0            0          0  0  0   \n",
       "1               0           0           0            0          0  0  0   \n",
       "2        0.245115  0.00716846  0.00716846    0.0142658          1  1  1   \n",
       "3               0           0           0            0          0  0  0   \n",
       "4        0.245115  0.00716846  0.00716846    0.0142658          1  1  1   \n",
       "...           ...         ...         ...          ...        ... .. ..   \n",
       "5830351         0           0           0            0          0  0  0   \n",
       "5830352         1           0           0  5.94654e-06  0.0533723  1  0   \n",
       "5830353         0           0           0            0          0  0  0   \n",
       "5830354         0           0           0            0          0  0  0   \n",
       "5830355         1           0           0            0  0.0533723  1  0   \n",
       "\n",
       "               7  8  9   ...        14         15 16 17 18 19        20  \\\n",
       "0               0  0  0  ...         0          0  0  0  0  0         0   \n",
       "1               0  0  0  ...         0          0  0  0  0  0         0   \n",
       "2        0.783407  1  1  ...         1  0.0771681  1  1  1  1  0.652174   \n",
       "3               0  0  0  ...         0          0  0  0  0  0         0   \n",
       "4        0.783407  1  1  ...         1  0.0771681  1  1  1  1  0.652174   \n",
       "...           ... .. ..  ...       ...        ... .. .. .. ..       ...   \n",
       "5830351         0  0  0  ...         0          0  0  0  0  0         0   \n",
       "5830352  0.955971  1  1  ...  0.328259  0.0398945  1  1  1  1  0.608696   \n",
       "5830353         0  0  0  ...         0          0  0  0  0  0         0   \n",
       "5830354         0  0  0  ...         0          0  0  0  0  0         0   \n",
       "5830355  0.955971  1  1  ...  0.328259  0.0398945  1  1  1  1  0.478261   \n",
       "\n",
       "                21        22                   23  \n",
       "0                0         0                    0  \n",
       "1                0         0                    0  \n",
       "2         0.322034  0.813559         0_1_15_19_48  \n",
       "3                0         0                    0  \n",
       "4         0.322034  0.813559         0_1_15_19_48  \n",
       "...            ...       ...                  ...  \n",
       "5830351          0         0                    0  \n",
       "5830352   0.237288  0.135593  213572_120_18_49_52  \n",
       "5830353          0         0                    0  \n",
       "5830354          0         0                    0  \n",
       "5830355  0.0508475  0.389831   213575_120_21_45_3  \n",
       "\n",
       "[5830356 rows x 24 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ldap_home/yunrui.li/.pyenv/versions/3.6.5/envs/deepts/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/ldap_home/yunrui.li/.pyenv/versions/3.6.5/envs/deepts/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/ldap_home/yunrui.li/.pyenv/versions/3.6.5/envs/deepts/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/ldap_home/yunrui.li/.pyenv/versions/3.6.5/envs/deepts/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/ldap_home/yunrui.li/.pyenv/versions/3.6.5/envs/deepts/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/ldap_home/yunrui.li/.pyenv/versions/3.6.5/envs/deepts/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/ldap_home/yunrui.li/.pyenv/versions/3.6.5/envs/deepts/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/ldap_home/yunrui.li/.pyenv/versions/3.6.5/envs/deepts/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/ldap_home/yunrui.li/.pyenv/versions/3.6.5/envs/deepts/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/ldap_home/yunrui.li/.pyenv/versions/3.6.5/envs/deepts/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/ldap_home/yunrui.li/.pyenv/versions/3.6.5/envs/deepts/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/ldap_home/yunrui.li/.pyenv/versions/3.6.5/envs/deepts/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import trange\n",
    "import sys\n",
    "sys.path.append(\"../DeepADoTS/src/algorithms/\")\n",
    "# from .algorithm_utils import Algorithm, PyTorchUtils\n",
    "# from .autoencoder import AutoEncoderModule\n",
    "#from lstm_enc_dec_axl import LSTMEDModule\n",
    "import abc\n",
    "import logging\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class Algorithm(metaclass=abc.ABCMeta):\n",
    "    def __init__(self, module_name, name, seed, details=False):\n",
    "        self.logger = logging.getLogger(module_name)\n",
    "        self.name = name\n",
    "        self.seed = seed\n",
    "        self.details = details\n",
    "        self.prediction_details = {}\n",
    "\n",
    "        if self.seed is not None:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Train the algorithm on the given dataset\n",
    "        \"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        :return anomaly score\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "class PyTorchUtils(metaclass=abc.ABCMeta):\n",
    "    def __init__(self, seed, gpu):\n",
    "        self.gpu = gpu\n",
    "        self.seed = seed\n",
    "        if self.seed is not None:\n",
    "            torch.manual_seed(self.seed)\n",
    "            torch.cuda.manual_seed(self.seed)\n",
    "        self.framework = 0\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        #return 'cuda:0'\n",
    "        return torch.device(f'cuda:{self.gpu}' if torch.cuda.is_available() and self.gpu is not None else 'cpu')\n",
    "\n",
    "    def to_var(self, t, **kwargs):\n",
    "        # ToDo: check whether cuda Variable.\n",
    "        t = t.to(self.device)\n",
    "        return Variable(t, **kwargs)\n",
    "\n",
    "    def to_device(self, model):\n",
    "        model.to(self.device)\n",
    "\n",
    "\n",
    "class TensorflowUtils(metaclass=abc.ABCMeta):\n",
    "    def __init__(self, seed, gpu):\n",
    "        self.gpu = gpu\n",
    "        self.seed = seed\n",
    "        if self.seed is not None:\n",
    "            tf.set_random_seed(seed)\n",
    "        self.framework = 1\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        local_device_protos = device_lib.list_local_devices()\n",
    "        gpus = [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "        return tf.device(gpus[self.gpu] if gpus and self.gpu is not None else '/cpu:0')\n",
    "    \n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.stats import multivariate_normal\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "class AutoEncoder(Algorithm, PyTorchUtils):\n",
    "    def __init__(self, name: str='AutoEncoder', num_epochs: int=10, batch_size: int=20, lr: float=1e-3,\n",
    "                 hidden_size: int=5, sequence_length: int=30, train_gaussian_percentage: float=0.25,\n",
    "                 seed: int=None, gpu: int=None, details=True):\n",
    "        Algorithm.__init__(self, __name__, name, seed, details=details)\n",
    "        PyTorchUtils.__init__(self, seed, gpu)\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.train_gaussian_percentage = train_gaussian_percentage\n",
    "\n",
    "        self.aed = None\n",
    "        self.mean, self.cov = None, None\n",
    "\n",
    "    def fit(self, X: pd.DataFrame):\n",
    "        X.interpolate(inplace=True)\n",
    "        X.bfill(inplace=True)\n",
    "        data = X.values\n",
    "        sequences = [data[i:i + self.sequence_length] for i in range(data.shape[0] - self.sequence_length + 1)]\n",
    "        indices = np.random.permutation(len(sequences))\n",
    "        split_point = int(self.train_gaussian_percentage * len(sequences))\n",
    "        train_loader = DataLoader(dataset=sequences, batch_size=self.batch_size, drop_last=True,\n",
    "                                  sampler=SubsetRandomSampler(indices[:-split_point]), pin_memory=True)\n",
    "        train_gaussian_loader = DataLoader(dataset=sequences, batch_size=self.batch_size, drop_last=True,\n",
    "                                           sampler=SubsetRandomSampler(indices[-split_point:]), pin_memory=True)\n",
    "\n",
    "        self.aed = AutoEncoderModule(X.shape[1], self.sequence_length, self.hidden_size, seed=self.seed, gpu=self.gpu)\n",
    "        self.to_device(self.aed)  # .double()\n",
    "        optimizer = torch.optim.Adam(self.aed.parameters(), lr=self.lr)\n",
    "\n",
    "        self.aed.train()\n",
    "        for epoch in trange(self.num_epochs):\n",
    "            logging.debug(f'Epoch {epoch+1}/{self.num_epochs}.')\n",
    "            for ts_batch in train_loader:\n",
    "                output = self.aed(self.to_var(ts_batch))\n",
    "                loss = nn.MSELoss(size_average=False)(output, self.to_var(ts_batch.float()))\n",
    "                self.aed.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        self.aed.eval()\n",
    "        error_vectors = []\n",
    "        for ts_batch in train_gaussian_loader:\n",
    "            output = self.aed(self.to_var(ts_batch))\n",
    "            error = nn.L1Loss(reduce=False)(output, self.to_var(ts_batch.float()))\n",
    "            error_vectors += list(error.view(-1, X.shape[1]).data.cpu().numpy())\n",
    "\n",
    "        self.mean = np.mean(error_vectors, axis=0)\n",
    "        self.cov = np.cov(error_vectors, rowvar=False)\n",
    "\n",
    "    def predict(self, X: pd.DataFrame) -> np.array:\n",
    "        X.interpolate(inplace=True)\n",
    "        X.bfill(inplace=True)\n",
    "        data = X.values\n",
    "        sequences = [data[i:i + self.sequence_length] for i in range(data.shape[0] - self.sequence_length + 1)]\n",
    "        data_loader = DataLoader(dataset=sequences, batch_size=self.batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "        self.aed.eval()\n",
    "        mvnormal = multivariate_normal(self.mean, self.cov, allow_singular=True)\n",
    "        scores = []\n",
    "        outputs = []\n",
    "        errors = []\n",
    "        for idx, ts in enumerate(data_loader):\n",
    "            output = self.aed(self.to_var(ts))\n",
    "            error = nn.L1Loss(reduce=False)(output, self.to_var(ts.float()))\n",
    "            score = -mvnormal.logpdf(error.view(-1, X.shape[1]).data.cpu().numpy())\n",
    "            scores.append(score.reshape(ts.size(0), self.sequence_length))\n",
    "            if self.details:\n",
    "                outputs.append(output.data.numpy())\n",
    "                errors.append(error.data.numpy())\n",
    "\n",
    "        # stores seq_len-many scores per timestamp and averages them\n",
    "        scores = np.concatenate(scores)\n",
    "        lattice = np.full((self.sequence_length, X.shape[0]), np.nan)\n",
    "        for i, score in enumerate(scores):\n",
    "            lattice[i % self.sequence_length, i:i + self.sequence_length] = score\n",
    "        scores = np.nanmean(lattice, axis=0)\n",
    "\n",
    "        if self.details:\n",
    "            outputs = np.concatenate(outputs)\n",
    "            lattice = np.full((self.sequence_length, X.shape[0], X.shape[1]), np.nan)\n",
    "            for i, output in enumerate(outputs):\n",
    "                lattice[i % self.sequence_length, i:i + self.sequence_length, :] = output\n",
    "            self.prediction_details.update({'reconstructions_mean': np.nanmean(lattice, axis=0).T})\n",
    "\n",
    "            errors = np.concatenate(errors)\n",
    "            lattice = np.full((self.sequence_length, X.shape[0], X.shape[1]), np.nan)\n",
    "            for i, error in enumerate(errors):\n",
    "                lattice[i % self.sequence_length, i:i + self.sequence_length, :] = error\n",
    "            self.prediction_details.update({'errors_mean': np.nanmean(lattice, axis=0).T})\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "class AutoEncoderModule(nn.Module, PyTorchUtils):\n",
    "    def __init__(self, n_features: int, sequence_length: int, hidden_size: int, seed: int, gpu: int):\n",
    "        # Each point is a flattened window and thus has as many features as sequence_length * features\n",
    "        super().__init__()\n",
    "        PyTorchUtils.__init__(self, seed, gpu)\n",
    "        input_length = n_features * sequence_length\n",
    "\n",
    "        # creates powers of two between eight and the next smaller power from the input_length\n",
    "        dec_steps = 2 ** np.arange(max(np.ceil(np.log2(hidden_size)), 2), np.log2(input_length))[1:]\n",
    "        dec_setup = np.concatenate([[hidden_size], dec_steps.repeat(2), [input_length]])\n",
    "        enc_setup = dec_setup[::-1]\n",
    "\n",
    "        layers = np.array([[nn.Linear(int(a), int(b)), nn.Tanh()] for a, b in enc_setup.reshape(-1, 2)]).flatten()[:-1]\n",
    "        self._encoder = nn.Sequential(*layers)\n",
    "        self.to_device(self._encoder)\n",
    "\n",
    "        layers = np.array([[nn.Linear(int(a), int(b)), nn.Tanh()] for a, b in dec_setup.reshape(-1, 2)]).flatten()[:-1]\n",
    "        self._decoder = nn.Sequential(*layers)\n",
    "        self.to_device(self._decoder)\n",
    "\n",
    "    def forward(self, ts_batch, return_latent: bool=False):\n",
    "        flattened_sequence = ts_batch.view(ts_batch.size(0), -1)\n",
    "        enc = self._encoder(flattened_sequence.float())\n",
    "        dec = self._decoder(enc)\n",
    "        reconstructed_sequence = dec.view(ts_batch.size())\n",
    "        return (reconstructed_sequence, enc) if return_latent else reconstructed_sequence\n",
    "    \n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.stats import multivariate_normal\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from tqdm import trange\n",
    "\n",
    "# from .algorithm_utils import Algorithm, PyTorchUtils\n",
    "\n",
    "\n",
    "class LSTMED(Algorithm, PyTorchUtils):\n",
    "    def __init__(self, name: str='LSTM-ED', num_epochs: int=10, batch_size: int=20, lr: float=1e-3,\n",
    "                 hidden_size: int=5, sequence_length: int=30, train_gaussian_percentage: float=0.25,\n",
    "                 n_layers: tuple=(1, 1), use_bias: tuple=(True, True), dropout: tuple=(0, 0),\n",
    "                 seed: int=None, gpu: int = None, details=True):\n",
    "        Algorithm.__init__(self, __name__, name, seed, details=details)\n",
    "        PyTorchUtils.__init__(self, seed, gpu)\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.train_gaussian_percentage = train_gaussian_percentage\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.use_bias = use_bias\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.lstmed = None\n",
    "        self.mean, self.cov = None, None\n",
    "\n",
    "    def fit(self, X: pd.DataFrame):\n",
    "        X.interpolate(inplace=True)\n",
    "        X.bfill(inplace=True)\n",
    "        data = X.values\n",
    "        sequences = [data[i:i + self.sequence_length] for i in range(data.shape[0] - self.sequence_length + 1)]\n",
    "        indices = np.random.permutation(len(sequences))\n",
    "        split_point = int(self.train_gaussian_percentage * len(sequences))\n",
    "        train_loader = DataLoader(dataset=sequences, batch_size=self.batch_size, drop_last=True,\n",
    "                                  sampler=SubsetRandomSampler(indices[:-split_point]), pin_memory=True)\n",
    "        train_gaussian_loader = DataLoader(dataset=sequences, batch_size=self.batch_size, drop_last=True,\n",
    "                                           sampler=SubsetRandomSampler(indices[-split_point:]), pin_memory=True)\n",
    "\n",
    "        self.lstmed = LSTMEDModule(X.shape[1], self.hidden_size,\n",
    "                                   self.n_layers, self.use_bias, self.dropout,\n",
    "                                   seed=self.seed, gpu=self.gpu)\n",
    "        self.to_device(self.lstmed)\n",
    "        optimizer = torch.optim.Adam(self.lstmed.parameters(), lr=self.lr)\n",
    "\n",
    "        self.lstmed.train()\n",
    "        for epoch in trange(self.num_epochs):\n",
    "            logging.debug(f'Epoch {epoch+1}/{self.num_epochs}.')\n",
    "            for ts_batch in train_loader:\n",
    "                output = self.lstmed(self.to_var(ts_batch))\n",
    "                loss = nn.MSELoss(size_average=False)(output, self.to_var(ts_batch.float()))\n",
    "                self.lstmed.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        self.lstmed.eval()\n",
    "        error_vectors = []\n",
    "        for ts_batch in train_gaussian_loader:\n",
    "            output = self.lstmed(self.to_var(ts_batch))\n",
    "            error = nn.L1Loss(reduce=False)(output, self.to_var(ts_batch.float()))\n",
    "            error_vectors += list(error.view(-1, X.shape[1]).data.cpu().numpy())\n",
    "\n",
    "        self.mean = np.mean(error_vectors, axis=0)\n",
    "        self.cov = np.cov(error_vectors, rowvar=False)\n",
    "\n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        X.interpolate(inplace=True)\n",
    "        X.bfill(inplace=True)\n",
    "        data = X.values\n",
    "        sequences = [data[i:i + self.sequence_length] for i in range(data.shape[0] - self.sequence_length + 1)]\n",
    "        data_loader = DataLoader(dataset=sequences, batch_size=self.batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "        self.lstmed.eval()\n",
    "        mvnormal = multivariate_normal(self.mean, self.cov, allow_singular=True)\n",
    "        scores = []\n",
    "        outputs = []\n",
    "        errors = []\n",
    "        for idx, ts in enumerate(data_loader):\n",
    "            output = self.lstmed(self.to_var(ts))\n",
    "            error = nn.L1Loss(reduce=False)(output, self.to_var(ts.float()))\n",
    "            score = -mvnormal.logpdf(error.view(-1, X.shape[1]).data.cpu().numpy())\n",
    "            scores.append(score.reshape(ts.size(0), self.sequence_length))\n",
    "            if self.details:\n",
    "                outputs.append(output.data.numpy())\n",
    "                errors.append(error.data.numpy())\n",
    "\n",
    "        # stores seq_len-many scores per timestamp and averages them\n",
    "        scores = np.concatenate(scores)\n",
    "        lattice = np.full((self.sequence_length, data.shape[0]), np.nan)\n",
    "        for i, score in enumerate(scores):\n",
    "            lattice[i % self.sequence_length, i:i + self.sequence_length] = score\n",
    "        scores = np.nanmean(lattice, axis=0)\n",
    "\n",
    "        if self.details:\n",
    "            outputs = np.concatenate(outputs)\n",
    "            lattice = np.full((self.sequence_length, X.shape[0], X.shape[1]), np.nan)\n",
    "            for i, output in enumerate(outputs):\n",
    "                lattice[i % self.sequence_length, i:i + self.sequence_length, :] = output\n",
    "            self.prediction_details.update({'reconstructions_mean': np.nanmean(lattice, axis=0).T})\n",
    "\n",
    "            errors = np.concatenate(errors)\n",
    "            lattice = np.full((self.sequence_length, X.shape[0], X.shape[1]), np.nan)\n",
    "            for i, error in enumerate(errors):\n",
    "                lattice[i % self.sequence_length, i:i + self.sequence_length, :] = error\n",
    "            self.prediction_details.update({'errors_mean': np.nanmean(lattice, axis=0).T})\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "class LSTMEDModule(nn.Module, PyTorchUtils):\n",
    "    def __init__(self, n_features: int, hidden_size: int,\n",
    "                 n_layers: tuple, use_bias: tuple, dropout: tuple,\n",
    "                 seed: int, gpu: int):\n",
    "        super().__init__()\n",
    "        PyTorchUtils.__init__(self, seed, gpu)\n",
    "        self.n_features = n_features\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.use_bias = use_bias\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.encoder = nn.LSTM(self.n_features, self.hidden_size, batch_first=True,\n",
    "                               num_layers=self.n_layers[0], bias=self.use_bias[0], dropout=self.dropout[0])\n",
    "        self.to_device(self.encoder)\n",
    "        self.decoder = nn.LSTM(self.n_features, self.hidden_size, batch_first=True,\n",
    "                               num_layers=self.n_layers[1], bias=self.use_bias[1], dropout=self.dropout[1])\n",
    "        self.to_device(self.decoder)\n",
    "        self.hidden2output = nn.Linear(self.hidden_size, self.n_features)\n",
    "        self.to_device(self.hidden2output)\n",
    "\n",
    "    def _init_hidden(self, batch_size):\n",
    "        return (self.to_var(torch.Tensor(self.n_layers[0], batch_size, self.hidden_size).zero_()),\n",
    "                self.to_var(torch.Tensor(self.n_layers[0], batch_size, self.hidden_size).zero_()))\n",
    "\n",
    "    def forward(self, ts_batch, return_latent: bool=False):\n",
    "        batch_size = ts_batch.shape[0]\n",
    "\n",
    "        # 1. Encode the timeseries to make use of the last hidden state.\n",
    "        enc_hidden = self._init_hidden(batch_size)  # initialization with zero\n",
    "        _, enc_hidden = self.encoder(ts_batch.float(), enc_hidden)  # .float() here or .double() for the model\n",
    "\n",
    "        # 2. Use hidden state as initialization for our Decoder-LSTM\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        # 3. Also, use this hidden state to get the first output aka the last point of the reconstructed timeseries\n",
    "        # 4. Reconstruct timeseries backwards\n",
    "        #    * Use true data for training decoder\n",
    "        #    * Use hidden2output for prediction\n",
    "        output = self.to_var(torch.Tensor(ts_batch.size()).zero_())\n",
    "        for i in reversed(range(ts_batch.shape[1])):\n",
    "            output[:, i, :] = self.hidden2output(dec_hidden[0][0, :])\n",
    "\n",
    "            if self.training:\n",
    "                _, dec_hidden = self.decoder(ts_batch[:, i].unsqueeze(1).float(), dec_hidden)\n",
    "            else:\n",
    "                _, dec_hidden = self.decoder(output[:, i].unsqueeze(1), dec_hidden)\n",
    "\n",
    "        return (output, enc_hidden[1][-1]) if return_latent else output\n",
    "\n",
    "\"\"\"Adapted from Daniel Stanley Tan (https://github.com/danieltan07/dagmm)\"\"\"\n",
    "import logging\n",
    "import sys\n",
    "sys.path.append(\"../DeepADoTS/src/algorithms/\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import trange\n",
    "\n",
    "# from .algorithm_utils import Algorithm, PyTorchUtils\n",
    "# from .autoencoder import AutoEncoderModule\n",
    "#from lstm_enc_dec_axl import LSTMEDModule\n",
    "\n",
    "\n",
    "class DAGMM(Algorithm, PyTorchUtils):\n",
    "    class AutoEncoder:\n",
    "        NN = AutoEncoderModule\n",
    "        LSTM = LSTMEDModule\n",
    "\n",
    "    def __init__(self, num_epochs=10, lambda_energy=0.1, lambda_cov_diag=0.005, lr=1e-3, batch_size=50, gmm_k=3,\n",
    "                 normal_percentile=80, sequence_length=30, autoencoder_type=AutoEncoderModule, autoencoder_args=None,\n",
    "                 hidden_size: int=5, seed: int=None, gpu: int=None, details=True):\n",
    "        _name = 'LSTM-DAGMM' if autoencoder_type == LSTMEDModule else 'DAGMM'\n",
    "        Algorithm.__init__(self, __name__, _name, seed, details=details)\n",
    "        PyTorchUtils.__init__(self, seed, gpu)\n",
    "        self.num_epochs = num_epochs\n",
    "        self.lambda_energy = lambda_energy\n",
    "        self.lambda_cov_diag = lambda_cov_diag\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.gmm_k = gmm_k  # Number of Gaussian mixtures\n",
    "        self.normal_percentile = normal_percentile  # Up to which percentile data should be considered normal\n",
    "        self.autoencoder_type = autoencoder_type\n",
    "        if autoencoder_type == AutoEncoderModule:\n",
    "            self.autoencoder_args = ({'sequence_length': self.sequence_length})\n",
    "        elif autoencoder_type == LSTMEDModule:\n",
    "            self.autoencoder_args = ({'n_layers': (1, 1), 'use_bias': (True, True), 'dropout': (0.0, 0.0)})\n",
    "        self.autoencoder_args.update({'seed': seed, 'gpu': gpu})\n",
    "        if autoencoder_args is not None:\n",
    "            self.autoencoder_args.update(autoencoder_args)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.dagmm, self.optimizer, self.train_energy, self._threshold = None, None, None, None\n",
    "\n",
    "    def reset_grad(self):\n",
    "        self.dagmm.zero_grad()\n",
    "\n",
    "    def dagmm_step(self, input_data):\n",
    "        self.dagmm.train()\n",
    "        enc, dec, z, gamma = self.dagmm(input_data)\n",
    "        #print (enc, dec, z, gamma)\n",
    "        total_loss, sample_energy, recon_error, cov_diag = self.dagmm.loss_function(input_data, dec, z, gamma,\n",
    "                                                                                    self.lambda_energy,\n",
    "                                                                                    self.lambda_cov_diag)\n",
    "        self.reset_grad()\n",
    "        total_loss = torch.clamp(total_loss, max=1e7)  # Extremely high loss can cause NaN gradients\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.dagmm.parameters(), 5)\n",
    "        # if np.array([np.isnan(p.grad.detach().numpy()).any() for p in self.dagmm.parameters()]).any():\n",
    "        #     import IPython; IPython.embed()\n",
    "        self.optimizer.step()\n",
    "        return total_loss, sample_energy, recon_error, cov_diag\n",
    "\n",
    "    def fit(self, X: pd.DataFrame):\n",
    "        \"\"\"Learn the mixture probability, mean and covariance for each component k.\n",
    "        Store the computed energy based on the training data and the aforementioned parameters.\"\"\"\n",
    "        #X.interpolate(inplace=True)\n",
    "        X.bfill(inplace=True)\n",
    "        data = X.values\n",
    "        sequences = [data[i:i + self.sequence_length] for i in range(X.shape[0] - self.sequence_length + 1)]\n",
    "        data_loader = DataLoader(dataset=sequences, batch_size=self.batch_size, shuffle=True, drop_last=True)\n",
    "        self.hidden_size = 5 + int(X.shape[1] / 20)\n",
    "        autoencoder = self.autoencoder_type(X.shape[1], hidden_size=self.hidden_size, **self.autoencoder_args)\n",
    "        self.dagmm = DAGMMModule(autoencoder, n_gmm=self.gmm_k, latent_dim=self.hidden_size + 2,\n",
    "                                 seed=self.seed, gpu=self.gpu)\n",
    "        self.to_device(self.dagmm)\n",
    "        self.optimizer = torch.optim.Adam(self.dagmm.parameters(), lr=self.lr)\n",
    "\n",
    "        for _ in trange(self.num_epochs):\n",
    "            for input_data in data_loader:\n",
    "                input_data = self.to_var(input_data)\n",
    "                self.dagmm_step(input_data.float())\n",
    "\n",
    "        self.dagmm.eval()\n",
    "        n = 0\n",
    "        mu_sum = 0\n",
    "        cov_sum = 0\n",
    "        gamma_sum = 0\n",
    "        for input_data in data_loader:\n",
    "            input_data = self.to_var(input_data)\n",
    "            _, _, z, gamma = self.dagmm(input_data.float())\n",
    "            phi, mu, cov = self.dagmm.compute_gmm_params(z, gamma)\n",
    "\n",
    "            batch_gamma_sum = torch.sum(gamma, dim=0)\n",
    "\n",
    "            gamma_sum += batch_gamma_sum\n",
    "            mu_sum += mu * batch_gamma_sum.unsqueeze(-1)  # keep sums of the numerator only\n",
    "            cov_sum += cov * batch_gamma_sum.unsqueeze(-1).unsqueeze(-1)  # keep sums of the numerator only\n",
    "\n",
    "            n += input_data.size(0)\n",
    "\n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        \"\"\"Using the learned mixture probability, mean and covariance for each component k, compute the energy on the\n",
    "        given data.\"\"\"\n",
    "        self.dagmm.eval()\n",
    "        #X.interpolate(inplace=True)\n",
    "        X.bfill(inplace=True)\n",
    "        data = X.values\n",
    "        sequences = [data[i:i + self.sequence_length] for i in range(len(data) - self.sequence_length + 1)]\n",
    "        data_loader = DataLoader(dataset=sequences, batch_size=1, shuffle=False)\n",
    "        test_energy = np.full((self.sequence_length, X.shape[0]), np.nan)\n",
    "\n",
    "        encodings = np.full((self.sequence_length, X.shape[0], self.hidden_size), np.nan)\n",
    "        decodings = np.full((self.sequence_length, X.shape[0], X.shape[1]), np.nan)\n",
    "        euc_errors = np.full((self.sequence_length, X.shape[0]), np.nan)\n",
    "        csn_errors = np.full((self.sequence_length, X.shape[0]), np.nan)\n",
    "\n",
    "        for i, sequence in enumerate(data_loader):\n",
    "            #print (\"shape of sequence\",self.to_var(sequence).float().shape)\n",
    "            enc, dec, z, gamma = self.dagmm(self.to_var(sequence).float())\n",
    "            sample_energy, _ = self.dagmm.compute_energy(z, size_average=False)\n",
    "            idx = (i % self.sequence_length, np.arange(i, i + self.sequence_length))\n",
    "            test_energy[idx] = sample_energy.data.numpy()\n",
    "\n",
    "            if self.details:\n",
    "                encodings[idx] = enc.data.cpu().numpy()\n",
    "                decodings[idx] = dec.data.cpu().numpy()\n",
    "                euc_errors[idx] = z[:, 1].data.cpu().numpy()\n",
    "                csn_errors[idx] = z[:, 2].data.cpu().numpy()\n",
    "\n",
    "        test_energy = np.nanmean(test_energy, axis=0)\n",
    "\n",
    "        if self.details:\n",
    "            self.prediction_details.update({'latent_representations': np.nanmean(encodings, axis=0).T})\n",
    "            self.prediction_details.update({'reconstructions_mean': np.nanmean(decodings, axis=0).T})\n",
    "            self.prediction_details.update({'euclidean_errors_mean': np.nanmean(euc_errors, axis=0)})\n",
    "            self.prediction_details.update({'cosine_errors_mean': np.nanmean(csn_errors, axis=0)})\n",
    "\n",
    "        return test_energy\n",
    "\n",
    "\n",
    "class DAGMMModule(nn.Module, PyTorchUtils):\n",
    "    \"\"\"Residual Block.\"\"\"\n",
    "\n",
    "    def __init__(self, autoencoder, n_gmm, latent_dim, seed: int, gpu: int):\n",
    "        super(DAGMMModule, self).__init__()\n",
    "        PyTorchUtils.__init__(self, seed, gpu)\n",
    "\n",
    "        self.add_module('autoencoder', autoencoder)\n",
    "\n",
    "        layers = [\n",
    "            nn.Linear(latent_dim, 10),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(10, n_gmm),\n",
    "            nn.Softmax(dim=1)\n",
    "        ]\n",
    "        self.estimation = nn.Sequential(*layers)\n",
    "        self.to_device(self.estimation)\n",
    "\n",
    "        self.register_buffer('phi', self.to_var(torch.zeros(n_gmm)))\n",
    "        self.register_buffer('mu', self.to_var(torch.zeros(n_gmm, latent_dim)))\n",
    "        self.register_buffer('cov', self.to_var(torch.zeros(n_gmm, latent_dim, latent_dim)))\n",
    "\n",
    "    def relative_euclidean_distance(self, a, b, dim=1):\n",
    "        return (a - b).norm(2, dim=dim) / torch.clamp(a.norm(2, dim=dim), min=1e-10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        dec, enc = self.autoencoder(x, return_latent=True)\n",
    "\n",
    "        rec_cosine = F.cosine_similarity(x.view(x.shape[0], -1), dec.view(dec.shape[0], -1), dim=1)\n",
    "        rec_euclidean = self.relative_euclidean_distance(x.view(x.shape[0], -1), dec.view(dec.shape[0], -1), dim=1)\n",
    "\n",
    "        # Concatenate latent representation, cosine similarity and relative Euclidean distance between x and dec(enc(x))\n",
    "        z = torch.cat([enc, rec_euclidean.unsqueeze(-1), rec_cosine.unsqueeze(-1)], dim=1)\n",
    "        gamma = self.estimation(z)\n",
    "\n",
    "        return enc, dec, z, gamma\n",
    "\n",
    "    def compute_gmm_params(self, z, gamma):\n",
    "        N = gamma.size(0)\n",
    "        # K\n",
    "        sum_gamma = torch.sum(gamma, dim=0)\n",
    "\n",
    "        # K\n",
    "        phi = (sum_gamma / N)\n",
    "\n",
    "        self.phi = phi.data\n",
    "\n",
    "        # K x D\n",
    "        mu = torch.sum(gamma.unsqueeze(-1) * z.unsqueeze(1), dim=0) / sum_gamma.unsqueeze(-1)\n",
    "        self.mu = mu.data\n",
    "        # z = N x D\n",
    "        # mu = K x D\n",
    "        # gamma N x K\n",
    "\n",
    "        # z_mu = N x K x D\n",
    "        z_mu = (z.unsqueeze(1) - mu.unsqueeze(0))\n",
    "\n",
    "        # z_mu_outer = N x K x D x D\n",
    "        z_mu_outer = z_mu.unsqueeze(-1) * z_mu.unsqueeze(-2)\n",
    "\n",
    "        # K x D x D\n",
    "        cov = torch.sum(gamma.unsqueeze(-1).unsqueeze(-1) * z_mu_outer, dim=0) / sum_gamma.unsqueeze(-1).unsqueeze(-1)\n",
    "        self.cov = cov.data\n",
    "\n",
    "        return phi, mu, cov\n",
    "\n",
    "    def compute_energy(self, z, phi=None, mu=None, cov=None, size_average=True):\n",
    "        if phi is None:\n",
    "            phi = Variable(self.phi)\n",
    "        if mu is None:\n",
    "            mu = Variable(self.mu)\n",
    "        if cov is None:\n",
    "            cov = Variable(self.cov)\n",
    "\n",
    "        k, d, _ = cov.size()\n",
    "\n",
    "        z_mu = (z.unsqueeze(1) - mu.unsqueeze(0))\n",
    "\n",
    "        cov_inverse = []\n",
    "        det_cov = []\n",
    "        cov_diag = 0\n",
    "        eps = 1e-12\n",
    "        for i in range(k):\n",
    "            # K x D x D\n",
    "            cov_k = cov[i] + self.to_var(torch.eye(d) * eps)\n",
    "            pinv = np.linalg.pinv(cov_k.data.cpu().numpy())\n",
    "            cov_inverse.append(Variable(torch.from_numpy(pinv)).unsqueeze(0))\n",
    "\n",
    "            eigvals = np.linalg.eigvals(cov_k.data.cpu().numpy() * (2 * np.pi))\n",
    "            if np.min(eigvals) < 0:\n",
    "                pass\n",
    "                #logging.warning(f'Determinant was negative! Clipping Eigenvalues to 0+epsilon from {np.min(eigvals)}')\n",
    "            determinant = np.prod(np.clip(eigvals, a_min=sys.float_info.epsilon, a_max=None))\n",
    "            det_cov.append(determinant)\n",
    "\n",
    "            cov_diag = cov_diag + torch.sum(1 / cov_k.diag())\n",
    "\n",
    "        # K x D x D\n",
    "        cov_inverse = torch.cat(cov_inverse, dim=0)\n",
    "        # K\n",
    "        det_cov = Variable(torch.from_numpy(np.float32(np.array(det_cov))))\n",
    "#         print (\"sum-0\",cov_inverse.unsqueeze(0))\n",
    "#         print (\"sum-1\",z_mu.ufnsqueeze(-1).cpu())\n",
    "#         print (\"sum\", torch.sum(z_mu.unsqueeze(-1).cpu() * cov_inverse.unsqueeze(0), dim=-2))\n",
    "        # N x K\n",
    "        exp_term_tmp = -0.5 * torch.sum(torch.sum(z_mu.unsqueeze(-1).cpu() * cov_inverse.unsqueeze(0), dim=-2) * z_mu.cpu(), dim=-1)\n",
    "        # for stability (logsumexp)\n",
    "        max_val = torch.max((exp_term_tmp).clamp(min=0), dim=1, keepdim=True)[0]\n",
    "\n",
    "        exp_term = torch.exp(exp_term_tmp - max_val)\n",
    "#         print (\"sample_energy\", self.to_var(phi.unsqueeze(0)).cpu())\n",
    "#         print (\"sample_energy-exp_term\", exp_term)\n",
    "        sample_energy = -max_val.squeeze() - torch.log(\n",
    "            torch.sum(self.to_var(phi.unsqueeze(0)).cpu() * exp_term / (torch.sqrt(self.to_var(det_cov).cpu()) + eps).unsqueeze(0),\n",
    "                      dim=1) + eps)\n",
    "\n",
    "        if size_average:\n",
    "            sample_energy = torch.mean(sample_energy)\n",
    "\n",
    "        return sample_energy, cov_diag\n",
    "\n",
    "    def loss_function(self, x, x_hat, z, gamma, lambda_energy, lambda_cov_diag):\n",
    "        recon_error = torch.mean((x.view(*x_hat.shape) - x_hat) ** 2)\n",
    "        #print (z, gamma)\n",
    "        phi, mu, cov = self.compute_gmm_params(z, gamma)\n",
    "        \n",
    "        #print (z, phi, mu, cov)\n",
    "        sample_energy, cov_diag = self.compute_energy(z, phi, mu, cov)\n",
    "#         print (\"recon_error\",recon_error)\n",
    "#         print (\"lambda_energy\",lambda_energy)\n",
    "#         print (\"lambda_cov_diag\",lambda_cov_diag)\n",
    "#         cov_diag = cov_diag.float()\n",
    "#         print (\"cov_diag\",cov_diag)\n",
    "        loss = recon_error + lambda_energy * sample_energy + lambda_cov_diag * cov_diag\n",
    "        \n",
    "        return loss, sample_energy, recon_error, cov_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [3:23:38<22:44, 1364.61s/it]  "
     ]
    }
   ],
   "source": [
    "detectors = DAGMM(num_epochs=10, sequence_length=3)\n",
    "detectors.fit(X_train.iloc[:,:-1].copy())\n",
    "\n",
    "score = detectors.predict(Feature.iloc[:,:-1].copy())\n",
    "output = pd.DataFrame({\"cano_locdt_index\":Feature.iloc[:,-1]})\n",
    "output[\"score\"] = score\n",
    "\n",
    "print (output.shape)\n",
    "\n",
    "output[\"cosine_errors_mean\"] = detectors.prediction_details[\"cosine_errors_mean\"]\n",
    "output[\"euclidean_errors_mean\"]  = detectors.prediction_details[\"euclidean_errors_mean\"]\n",
    "data = detectors.prediction_details[\"reconstructions_mean\"]\n",
    "reconstructions_mean = pd.DataFrame(data.T,\n",
    "             columns = [\"reconstructions_mean_latent_features_{}\".format(i) for i in range(data.shape[0])]\n",
    "            )\n",
    "print (reconstructions_mean.shape)\n",
    "data = detectors.prediction_details[\"latent_representations\"]\n",
    "latent_representations = pd.DataFrame(data.T,\n",
    "             columns = [\"latent_representations_latent_features_{}\".format(i) for i in range(data.shape[0])]\n",
    "            )\n",
    "print (latent_representations.shape)\n",
    "output = pd.concat([output,reconstructions_mean,latent_representations], axis = 1)\n",
    "print (output.shape)\n",
    "\n",
    "feature = []\n",
    "for i in range(len(output)):\n",
    "    if i%3 == 2:\n",
    "        feature.append(output.iloc[i:i+1])\n",
    "feature = pd.concat(feature,axis = 0)\n",
    "\n",
    "feature.to_csv(\"/data/yunrui_li/fraud/fraud_detection/features/DAGMM_features_modified.csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.iloc[35:135]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[df_train.cano == 2].sort_values(by = [\"cano\",\"locdt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/data/yunrui_li/fraud/fraud_detection/features/DAGMM_features_modified_2.csv\")\n",
    "\n",
    "df_train[\"cano_locdt_index\"] = [\"{}_{}_{}_{}_{}\".format(str(i),str(j),str(k),str(l),str(m)) for i,j,k,l,m in zip(df_train.cano,\n",
    "                                                                                   df_train.locdt,\n",
    "                                                                                   df_train.loctm_hour_of_day,\n",
    "                                                                                   df_train.loctm_minute_of_hour,\n",
    "                                                                                   df_train.loctm_second_of_min,\n",
    "                                                                                  )]\n",
    "df_test[\"cano_locdt_index\"] = [\"{}_{}_{}_{}_{}\".format(str(i),str(j),str(k),str(l),str(m)) for i,j,k,l,m in zip(df_test.cano,\n",
    "                                                                                  df_test.locdt,\n",
    "                                                                                  df_train.loctm_hour_of_day,\n",
    "                                                                                  df_train.loctm_minute_of_hour,\n",
    "                                                                                  df_train.loctm_second_of_min,\n",
    "                                                                                 )]\n",
    "\n",
    "# df_train = pd.read_csv(\"/data/yunrui_li/fraud/dataset/train.csv\")\n",
    "# df_test = pd.read_csv(\"/data/yunrui_li/fraud/dataset/test.csv\")\n",
    "# df_train[\"cano_locdt_index\"] = [\"{}_{}\".format(str(i),str(j)) for i,j in zip(df_train.cano,df_train.locdt)]\n",
    "# df_test[\"cano_locdt_index\"] = [\"{}_{}\".format(str(i),str(j)) for i,j in zip(df_test.cano,df_test.locdt)]\n",
    "# print (\"df_train\", df_train.shape)\n",
    "# print (\"df_test\", df_test.shape)\n",
    "\n",
    "# df_train_ = df_train.merge(df, on = \"cano_locdt_index\", how = \"left\").drop_duplicates(\"txkey\")\n",
    "# df_test_ = df_test.merge(df, on = \"cano_locdt_index\", how = \"left\").drop_duplicates(\"txkey\")\n",
    "# # df_train_ = df_train.merge(df, on = \"cano_locdt_index\", how = \"left\")\\\n",
    "# # .drop_duplicates(subset =[\"cano\",\"locdt\",\"cosine_errors_mean\",\"euclidean_errors_mean\",\"reconstructions_mean_latent_features_0\",\"score\"])\n",
    "# # df_test_ = df_test.merge(df, on = \"cano_locdt_index\", how = \"left\")\\\n",
    "# # .drop_duplicates(subset =[\"cano\",\"locdt\",\"cosine_errors_mean\",\"euclidean_errors_mean\",\"reconstructions_mean_latent_features_0\",\"score\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cano_locdt_index</th>\n",
       "      <th>score</th>\n",
       "      <th>cosine_errors_mean</th>\n",
       "      <th>euclidean_errors_mean</th>\n",
       "      <th>reconstructions_mean_latent_features_0</th>\n",
       "      <th>reconstructions_mean_latent_features_1</th>\n",
       "      <th>reconstructions_mean_latent_features_2</th>\n",
       "      <th>reconstructions_mean_latent_features_3</th>\n",
       "      <th>reconstructions_mean_latent_features_4</th>\n",
       "      <th>reconstructions_mean_latent_features_5</th>\n",
       "      <th>reconstructions_mean_latent_features_6</th>\n",
       "      <th>reconstructions_mean_latent_features_7</th>\n",
       "      <th>reconstructions_mean_latent_features_8</th>\n",
       "      <th>reconstructions_mean_latent_features_9</th>\n",
       "      <th>reconstructions_mean_latent_features_10</th>\n",
       "      <th>reconstructions_mean_latent_features_11</th>\n",
       "      <th>reconstructions_mean_latent_features_12</th>\n",
       "      <th>reconstructions_mean_latent_features_13</th>\n",
       "      <th>reconstructions_mean_latent_features_14</th>\n",
       "      <th>reconstructions_mean_latent_features_15</th>\n",
       "      <th>reconstructions_mean_latent_features_16</th>\n",
       "      <th>reconstructions_mean_latent_features_17</th>\n",
       "      <th>reconstructions_mean_latent_features_18</th>\n",
       "      <th>reconstructions_mean_latent_features_19</th>\n",
       "      <th>reconstructions_mean_latent_features_20</th>\n",
       "      <th>reconstructions_mean_latent_features_21</th>\n",
       "      <th>reconstructions_mean_latent_features_22</th>\n",
       "      <th>latent_representations_latent_features_0</th>\n",
       "      <th>latent_representations_latent_features_1</th>\n",
       "      <th>latent_representations_latent_features_2</th>\n",
       "      <th>latent_representations_latent_features_3</th>\n",
       "      <th>latent_representations_latent_features_4</th>\n",
       "      <th>latent_representations_latent_features_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0_1_15_19_48</td>\n",
       "      <td>10.979028</td>\n",
       "      <td>12.364199</td>\n",
       "      <td>-8.007674</td>\n",
       "      <td>-0.295668</td>\n",
       "      <td>-0.029343</td>\n",
       "      <td>-0.062085</td>\n",
       "      <td>-0.187661</td>\n",
       "      <td>-0.305402</td>\n",
       "      <td>-0.292156</td>\n",
       "      <td>-0.295825</td>\n",
       "      <td>-0.345110</td>\n",
       "      <td>0.083790</td>\n",
       "      <td>-0.119797</td>\n",
       "      <td>-0.139597</td>\n",
       "      <td>-0.221676</td>\n",
       "      <td>-0.176965</td>\n",
       "      <td>-0.176095</td>\n",
       "      <td>-0.293721</td>\n",
       "      <td>-0.130228</td>\n",
       "      <td>-0.126550</td>\n",
       "      <td>-0.290750</td>\n",
       "      <td>-0.199623</td>\n",
       "      <td>0.023329</td>\n",
       "      <td>-0.327014</td>\n",
       "      <td>-0.275622</td>\n",
       "      <td>-0.305517</td>\n",
       "      <td>10.813115</td>\n",
       "      <td>-8.007674</td>\n",
       "      <td>12.364199</td>\n",
       "      <td>-17.007282</td>\n",
       "      <td>-9.952607</td>\n",
       "      <td>10.117329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0_4_15_44_7</td>\n",
       "      <td>-9.142813</td>\n",
       "      <td>6.705316</td>\n",
       "      <td>-5.288274</td>\n",
       "      <td>0.051264</td>\n",
       "      <td>0.099559</td>\n",
       "      <td>0.167285</td>\n",
       "      <td>-0.003343</td>\n",
       "      <td>0.094564</td>\n",
       "      <td>-0.003356</td>\n",
       "      <td>0.060736</td>\n",
       "      <td>0.106433</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.021833</td>\n",
       "      <td>0.102818</td>\n",
       "      <td>0.093015</td>\n",
       "      <td>0.014612</td>\n",
       "      <td>0.077044</td>\n",
       "      <td>0.040543</td>\n",
       "      <td>0.035237</td>\n",
       "      <td>0.326648</td>\n",
       "      <td>0.037958</td>\n",
       "      <td>0.106233</td>\n",
       "      <td>1.376777</td>\n",
       "      <td>0.039747</td>\n",
       "      <td>0.052369</td>\n",
       "      <td>0.040240</td>\n",
       "      <td>6.438653</td>\n",
       "      <td>-5.288274</td>\n",
       "      <td>6.705316</td>\n",
       "      <td>-1.351625</td>\n",
       "      <td>-10.976459</td>\n",
       "      <td>5.906591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0_20_14_53_42</td>\n",
       "      <td>-27.536726</td>\n",
       "      <td>0.693698</td>\n",
       "      <td>-2.400572</td>\n",
       "      <td>0.503422</td>\n",
       "      <td>0.239930</td>\n",
       "      <td>0.367019</td>\n",
       "      <td>0.307875</td>\n",
       "      <td>0.521885</td>\n",
       "      <td>0.452024</td>\n",
       "      <td>0.497343</td>\n",
       "      <td>0.678100</td>\n",
       "      <td>0.502930</td>\n",
       "      <td>0.479732</td>\n",
       "      <td>0.443191</td>\n",
       "      <td>0.534467</td>\n",
       "      <td>0.235679</td>\n",
       "      <td>0.367595</td>\n",
       "      <td>0.459611</td>\n",
       "      <td>0.277773</td>\n",
       "      <td>0.582493</td>\n",
       "      <td>0.455292</td>\n",
       "      <td>0.431844</td>\n",
       "      <td>1.822399</td>\n",
       "      <td>0.524805</td>\n",
       "      <td>0.453604</td>\n",
       "      <td>0.475764</td>\n",
       "      <td>1.799086</td>\n",
       "      <td>-2.400572</td>\n",
       "      <td>0.693698</td>\n",
       "      <td>15.269121</td>\n",
       "      <td>-12.062453</td>\n",
       "      <td>1.437511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0_29_15_22_43</td>\n",
       "      <td>-27.299505</td>\n",
       "      <td>0.698366</td>\n",
       "      <td>-2.402813</td>\n",
       "      <td>0.503466</td>\n",
       "      <td>0.243847</td>\n",
       "      <td>0.376149</td>\n",
       "      <td>0.309773</td>\n",
       "      <td>0.519667</td>\n",
       "      <td>0.452806</td>\n",
       "      <td>0.497322</td>\n",
       "      <td>0.678940</td>\n",
       "      <td>0.502650</td>\n",
       "      <td>0.477642</td>\n",
       "      <td>0.441043</td>\n",
       "      <td>0.531946</td>\n",
       "      <td>0.238838</td>\n",
       "      <td>0.365970</td>\n",
       "      <td>0.463696</td>\n",
       "      <td>0.277994</td>\n",
       "      <td>0.579809</td>\n",
       "      <td>0.455426</td>\n",
       "      <td>0.428629</td>\n",
       "      <td>1.871941</td>\n",
       "      <td>0.525425</td>\n",
       "      <td>0.453050</td>\n",
       "      <td>0.475354</td>\n",
       "      <td>1.802612</td>\n",
       "      <td>-2.402813</td>\n",
       "      <td>0.698366</td>\n",
       "      <td>15.256314</td>\n",
       "      <td>-12.061617</td>\n",
       "      <td>1.440938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0_37_14_37_10</td>\n",
       "      <td>-27.538051</td>\n",
       "      <td>0.701460</td>\n",
       "      <td>-2.404299</td>\n",
       "      <td>0.504996</td>\n",
       "      <td>0.246576</td>\n",
       "      <td>0.381254</td>\n",
       "      <td>0.311620</td>\n",
       "      <td>0.516418</td>\n",
       "      <td>0.453405</td>\n",
       "      <td>0.497762</td>\n",
       "      <td>0.678617</td>\n",
       "      <td>0.502843</td>\n",
       "      <td>0.477464</td>\n",
       "      <td>0.440238</td>\n",
       "      <td>0.527018</td>\n",
       "      <td>0.240219</td>\n",
       "      <td>0.365524</td>\n",
       "      <td>0.466002</td>\n",
       "      <td>0.278495</td>\n",
       "      <td>0.576738</td>\n",
       "      <td>0.456497</td>\n",
       "      <td>0.426791</td>\n",
       "      <td>1.891553</td>\n",
       "      <td>0.525191</td>\n",
       "      <td>0.453053</td>\n",
       "      <td>0.475336</td>\n",
       "      <td>1.804948</td>\n",
       "      <td>-2.404299</td>\n",
       "      <td>0.701460</td>\n",
       "      <td>15.247843</td>\n",
       "      <td>-12.061057</td>\n",
       "      <td>1.443211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1943447</td>\n",
       "      <td>213570_119_19_17_45</td>\n",
       "      <td>26.599059</td>\n",
       "      <td>12.260830</td>\n",
       "      <td>-7.958227</td>\n",
       "      <td>-0.341522</td>\n",
       "      <td>-0.035077</td>\n",
       "      <td>-0.047967</td>\n",
       "      <td>-0.216096</td>\n",
       "      <td>-0.315000</td>\n",
       "      <td>-0.339600</td>\n",
       "      <td>-0.342677</td>\n",
       "      <td>-0.402981</td>\n",
       "      <td>-0.142024</td>\n",
       "      <td>-0.274363</td>\n",
       "      <td>-0.229468</td>\n",
       "      <td>-0.256957</td>\n",
       "      <td>-0.158140</td>\n",
       "      <td>-0.123354</td>\n",
       "      <td>-0.370937</td>\n",
       "      <td>-0.188145</td>\n",
       "      <td>-0.177943</td>\n",
       "      <td>-0.315919</td>\n",
       "      <td>-0.277883</td>\n",
       "      <td>0.161597</td>\n",
       "      <td>-0.350359</td>\n",
       "      <td>-0.294658</td>\n",
       "      <td>-0.311811</td>\n",
       "      <td>10.735245</td>\n",
       "      <td>-7.958227</td>\n",
       "      <td>12.260830</td>\n",
       "      <td>-16.724079</td>\n",
       "      <td>-9.970756</td>\n",
       "      <td>10.041531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1943448</td>\n",
       "      <td>213571_119_20_8_34</td>\n",
       "      <td>10.710432</td>\n",
       "      <td>12.292395</td>\n",
       "      <td>-7.973412</td>\n",
       "      <td>-0.278399</td>\n",
       "      <td>-0.020010</td>\n",
       "      <td>-0.047961</td>\n",
       "      <td>-0.175173</td>\n",
       "      <td>-0.298520</td>\n",
       "      <td>-0.282361</td>\n",
       "      <td>-0.288717</td>\n",
       "      <td>-0.329531</td>\n",
       "      <td>0.078586</td>\n",
       "      <td>-0.123880</td>\n",
       "      <td>-0.143407</td>\n",
       "      <td>-0.215496</td>\n",
       "      <td>-0.146395</td>\n",
       "      <td>-0.153816</td>\n",
       "      <td>-0.279970</td>\n",
       "      <td>-0.133996</td>\n",
       "      <td>-0.142479</td>\n",
       "      <td>-0.278372</td>\n",
       "      <td>-0.212501</td>\n",
       "      <td>0.018624</td>\n",
       "      <td>-0.311243</td>\n",
       "      <td>-0.261731</td>\n",
       "      <td>-0.289561</td>\n",
       "      <td>10.759145</td>\n",
       "      <td>-7.973412</td>\n",
       "      <td>12.292395</td>\n",
       "      <td>-16.810815</td>\n",
       "      <td>-9.965255</td>\n",
       "      <td>10.064739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1943449</td>\n",
       "      <td>213571_119_20_47_28</td>\n",
       "      <td>-4.772736</td>\n",
       "      <td>6.632952</td>\n",
       "      <td>-5.253120</td>\n",
       "      <td>0.022527</td>\n",
       "      <td>0.004690</td>\n",
       "      <td>-0.001649</td>\n",
       "      <td>-0.056093</td>\n",
       "      <td>0.149062</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.063125</td>\n",
       "      <td>0.098664</td>\n",
       "      <td>0.116747</td>\n",
       "      <td>0.092619</td>\n",
       "      <td>0.147705</td>\n",
       "      <td>0.208289</td>\n",
       "      <td>-0.064188</td>\n",
       "      <td>0.124847</td>\n",
       "      <td>-0.011851</td>\n",
       "      <td>0.014270</td>\n",
       "      <td>0.396818</td>\n",
       "      <td>0.019339</td>\n",
       "      <td>0.148477</td>\n",
       "      <td>0.701049</td>\n",
       "      <td>0.031916</td>\n",
       "      <td>0.065185</td>\n",
       "      <td>0.046811</td>\n",
       "      <td>6.383266</td>\n",
       "      <td>-5.253120</td>\n",
       "      <td>6.632952</td>\n",
       "      <td>-1.150614</td>\n",
       "      <td>-10.989484</td>\n",
       "      <td>5.853092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1943450</td>\n",
       "      <td>213572_120_18_49_52</td>\n",
       "      <td>11.930108</td>\n",
       "      <td>12.281738</td>\n",
       "      <td>-7.968170</td>\n",
       "      <td>-0.335220</td>\n",
       "      <td>-0.036391</td>\n",
       "      <td>-0.049013</td>\n",
       "      <td>-0.213610</td>\n",
       "      <td>-0.302684</td>\n",
       "      <td>-0.331858</td>\n",
       "      <td>-0.333541</td>\n",
       "      <td>-0.387611</td>\n",
       "      <td>-0.111310</td>\n",
       "      <td>-0.253340</td>\n",
       "      <td>-0.214632</td>\n",
       "      <td>-0.234080</td>\n",
       "      <td>-0.161921</td>\n",
       "      <td>-0.122315</td>\n",
       "      <td>-0.359167</td>\n",
       "      <td>-0.180575</td>\n",
       "      <td>-0.152499</td>\n",
       "      <td>-0.311146</td>\n",
       "      <td>-0.264115</td>\n",
       "      <td>0.201282</td>\n",
       "      <td>-0.340114</td>\n",
       "      <td>-0.286505</td>\n",
       "      <td>-0.305532</td>\n",
       "      <td>10.750884</td>\n",
       "      <td>-7.968170</td>\n",
       "      <td>12.281738</td>\n",
       "      <td>-16.780902</td>\n",
       "      <td>-9.967198</td>\n",
       "      <td>10.056808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1943451</td>\n",
       "      <td>213575_120_21_45_3</td>\n",
       "      <td>27.631021</td>\n",
       "      <td>18.159103</td>\n",
       "      <td>-10.792393</td>\n",
       "      <td>-0.693227</td>\n",
       "      <td>-0.055289</td>\n",
       "      <td>-0.066945</td>\n",
       "      <td>-0.384623</td>\n",
       "      <td>-0.727818</td>\n",
       "      <td>-0.720160</td>\n",
       "      <td>-0.730078</td>\n",
       "      <td>-0.894959</td>\n",
       "      <td>-0.546958</td>\n",
       "      <td>-0.651012</td>\n",
       "      <td>-0.564419</td>\n",
       "      <td>-0.696275</td>\n",
       "      <td>-0.228950</td>\n",
       "      <td>-0.407791</td>\n",
       "      <td>-0.666669</td>\n",
       "      <td>-0.379489</td>\n",
       "      <td>-0.599024</td>\n",
       "      <td>-0.654697</td>\n",
       "      <td>-0.634334</td>\n",
       "      <td>-0.429966</td>\n",
       "      <td>-0.755454</td>\n",
       "      <td>-0.634547</td>\n",
       "      <td>-0.690522</td>\n",
       "      <td>15.290472</td>\n",
       "      <td>-10.792393</td>\n",
       "      <td>18.159103</td>\n",
       "      <td>-33.037109</td>\n",
       "      <td>-8.904607</td>\n",
       "      <td>14.428023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1943452 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            cano_locdt_index      score  cosine_errors_mean  \\\n",
       "0               0_1_15_19_48  10.979028           12.364199   \n",
       "1                0_4_15_44_7  -9.142813            6.705316   \n",
       "2              0_20_14_53_42 -27.536726            0.693698   \n",
       "3              0_29_15_22_43 -27.299505            0.698366   \n",
       "4              0_37_14_37_10 -27.538051            0.701460   \n",
       "...                      ...        ...                 ...   \n",
       "1943447  213570_119_19_17_45  26.599059           12.260830   \n",
       "1943448   213571_119_20_8_34  10.710432           12.292395   \n",
       "1943449  213571_119_20_47_28  -4.772736            6.632952   \n",
       "1943450  213572_120_18_49_52  11.930108           12.281738   \n",
       "1943451   213575_120_21_45_3  27.631021           18.159103   \n",
       "\n",
       "         euclidean_errors_mean  reconstructions_mean_latent_features_0  \\\n",
       "0                    -8.007674                               -0.295668   \n",
       "1                    -5.288274                                0.051264   \n",
       "2                    -2.400572                                0.503422   \n",
       "3                    -2.402813                                0.503466   \n",
       "4                    -2.404299                                0.504996   \n",
       "...                        ...                                     ...   \n",
       "1943447              -7.958227                               -0.341522   \n",
       "1943448              -7.973412                               -0.278399   \n",
       "1943449              -5.253120                                0.022527   \n",
       "1943450              -7.968170                               -0.335220   \n",
       "1943451             -10.792393                               -0.693227   \n",
       "\n",
       "         reconstructions_mean_latent_features_1  \\\n",
       "0                                     -0.029343   \n",
       "1                                      0.099559   \n",
       "2                                      0.239930   \n",
       "3                                      0.243847   \n",
       "4                                      0.246576   \n",
       "...                                         ...   \n",
       "1943447                               -0.035077   \n",
       "1943448                               -0.020010   \n",
       "1943449                                0.004690   \n",
       "1943450                               -0.036391   \n",
       "1943451                               -0.055289   \n",
       "\n",
       "         reconstructions_mean_latent_features_2  \\\n",
       "0                                     -0.062085   \n",
       "1                                      0.167285   \n",
       "2                                      0.367019   \n",
       "3                                      0.376149   \n",
       "4                                      0.381254   \n",
       "...                                         ...   \n",
       "1943447                               -0.047967   \n",
       "1943448                               -0.047961   \n",
       "1943449                               -0.001649   \n",
       "1943450                               -0.049013   \n",
       "1943451                               -0.066945   \n",
       "\n",
       "         reconstructions_mean_latent_features_3  \\\n",
       "0                                     -0.187661   \n",
       "1                                     -0.003343   \n",
       "2                                      0.307875   \n",
       "3                                      0.309773   \n",
       "4                                      0.311620   \n",
       "...                                         ...   \n",
       "1943447                               -0.216096   \n",
       "1943448                               -0.175173   \n",
       "1943449                               -0.056093   \n",
       "1943450                               -0.213610   \n",
       "1943451                               -0.384623   \n",
       "\n",
       "         reconstructions_mean_latent_features_4  \\\n",
       "0                                     -0.305402   \n",
       "1                                      0.094564   \n",
       "2                                      0.521885   \n",
       "3                                      0.519667   \n",
       "4                                      0.516418   \n",
       "...                                         ...   \n",
       "1943447                               -0.315000   \n",
       "1943448                               -0.298520   \n",
       "1943449                                0.149062   \n",
       "1943450                               -0.302684   \n",
       "1943451                               -0.727818   \n",
       "\n",
       "         reconstructions_mean_latent_features_5  \\\n",
       "0                                     -0.292156   \n",
       "1                                     -0.003356   \n",
       "2                                      0.452024   \n",
       "3                                      0.452806   \n",
       "4                                      0.453405   \n",
       "...                                         ...   \n",
       "1943447                               -0.339600   \n",
       "1943448                               -0.282361   \n",
       "1943449                                0.000331   \n",
       "1943450                               -0.331858   \n",
       "1943451                               -0.720160   \n",
       "\n",
       "         reconstructions_mean_latent_features_6  \\\n",
       "0                                     -0.295825   \n",
       "1                                      0.060736   \n",
       "2                                      0.497343   \n",
       "3                                      0.497322   \n",
       "4                                      0.497762   \n",
       "...                                         ...   \n",
       "1943447                               -0.342677   \n",
       "1943448                               -0.288717   \n",
       "1943449                                0.063125   \n",
       "1943450                               -0.333541   \n",
       "1943451                               -0.730078   \n",
       "\n",
       "         reconstructions_mean_latent_features_7  \\\n",
       "0                                     -0.345110   \n",
       "1                                      0.106433   \n",
       "2                                      0.678100   \n",
       "3                                      0.678940   \n",
       "4                                      0.678617   \n",
       "...                                         ...   \n",
       "1943447                               -0.402981   \n",
       "1943448                               -0.329531   \n",
       "1943449                                0.098664   \n",
       "1943450                               -0.387611   \n",
       "1943451                               -0.894959   \n",
       "\n",
       "         reconstructions_mean_latent_features_8  \\\n",
       "0                                      0.083790   \n",
       "1                                      0.033333   \n",
       "2                                      0.502930   \n",
       "3                                      0.502650   \n",
       "4                                      0.502843   \n",
       "...                                         ...   \n",
       "1943447                               -0.142024   \n",
       "1943448                                0.078586   \n",
       "1943449                                0.116747   \n",
       "1943450                               -0.111310   \n",
       "1943451                               -0.546958   \n",
       "\n",
       "         reconstructions_mean_latent_features_9  \\\n",
       "0                                     -0.119797   \n",
       "1                                      0.021833   \n",
       "2                                      0.479732   \n",
       "3                                      0.477642   \n",
       "4                                      0.477464   \n",
       "...                                         ...   \n",
       "1943447                               -0.274363   \n",
       "1943448                               -0.123880   \n",
       "1943449                                0.092619   \n",
       "1943450                               -0.253340   \n",
       "1943451                               -0.651012   \n",
       "\n",
       "         reconstructions_mean_latent_features_10  \\\n",
       "0                                      -0.139597   \n",
       "1                                       0.102818   \n",
       "2                                       0.443191   \n",
       "3                                       0.441043   \n",
       "4                                       0.440238   \n",
       "...                                          ...   \n",
       "1943447                                -0.229468   \n",
       "1943448                                -0.143407   \n",
       "1943449                                 0.147705   \n",
       "1943450                                -0.214632   \n",
       "1943451                                -0.564419   \n",
       "\n",
       "         reconstructions_mean_latent_features_11  \\\n",
       "0                                      -0.221676   \n",
       "1                                       0.093015   \n",
       "2                                       0.534467   \n",
       "3                                       0.531946   \n",
       "4                                       0.527018   \n",
       "...                                          ...   \n",
       "1943447                                -0.256957   \n",
       "1943448                                -0.215496   \n",
       "1943449                                 0.208289   \n",
       "1943450                                -0.234080   \n",
       "1943451                                -0.696275   \n",
       "\n",
       "         reconstructions_mean_latent_features_12  \\\n",
       "0                                      -0.176965   \n",
       "1                                       0.014612   \n",
       "2                                       0.235679   \n",
       "3                                       0.238838   \n",
       "4                                       0.240219   \n",
       "...                                          ...   \n",
       "1943447                                -0.158140   \n",
       "1943448                                -0.146395   \n",
       "1943449                                -0.064188   \n",
       "1943450                                -0.161921   \n",
       "1943451                                -0.228950   \n",
       "\n",
       "         reconstructions_mean_latent_features_13  \\\n",
       "0                                      -0.176095   \n",
       "1                                       0.077044   \n",
       "2                                       0.367595   \n",
       "3                                       0.365970   \n",
       "4                                       0.365524   \n",
       "...                                          ...   \n",
       "1943447                                -0.123354   \n",
       "1943448                                -0.153816   \n",
       "1943449                                 0.124847   \n",
       "1943450                                -0.122315   \n",
       "1943451                                -0.407791   \n",
       "\n",
       "         reconstructions_mean_latent_features_14  \\\n",
       "0                                      -0.293721   \n",
       "1                                       0.040543   \n",
       "2                                       0.459611   \n",
       "3                                       0.463696   \n",
       "4                                       0.466002   \n",
       "...                                          ...   \n",
       "1943447                                -0.370937   \n",
       "1943448                                -0.279970   \n",
       "1943449                                -0.011851   \n",
       "1943450                                -0.359167   \n",
       "1943451                                -0.666669   \n",
       "\n",
       "         reconstructions_mean_latent_features_15  \\\n",
       "0                                      -0.130228   \n",
       "1                                       0.035237   \n",
       "2                                       0.277773   \n",
       "3                                       0.277994   \n",
       "4                                       0.278495   \n",
       "...                                          ...   \n",
       "1943447                                -0.188145   \n",
       "1943448                                -0.133996   \n",
       "1943449                                 0.014270   \n",
       "1943450                                -0.180575   \n",
       "1943451                                -0.379489   \n",
       "\n",
       "         reconstructions_mean_latent_features_16  \\\n",
       "0                                      -0.126550   \n",
       "1                                       0.326648   \n",
       "2                                       0.582493   \n",
       "3                                       0.579809   \n",
       "4                                       0.576738   \n",
       "...                                          ...   \n",
       "1943447                                -0.177943   \n",
       "1943448                                -0.142479   \n",
       "1943449                                 0.396818   \n",
       "1943450                                -0.152499   \n",
       "1943451                                -0.599024   \n",
       "\n",
       "         reconstructions_mean_latent_features_17  \\\n",
       "0                                      -0.290750   \n",
       "1                                       0.037958   \n",
       "2                                       0.455292   \n",
       "3                                       0.455426   \n",
       "4                                       0.456497   \n",
       "...                                          ...   \n",
       "1943447                                -0.315919   \n",
       "1943448                                -0.278372   \n",
       "1943449                                 0.019339   \n",
       "1943450                                -0.311146   \n",
       "1943451                                -0.654697   \n",
       "\n",
       "         reconstructions_mean_latent_features_18  \\\n",
       "0                                      -0.199623   \n",
       "1                                       0.106233   \n",
       "2                                       0.431844   \n",
       "3                                       0.428629   \n",
       "4                                       0.426791   \n",
       "...                                          ...   \n",
       "1943447                                -0.277883   \n",
       "1943448                                -0.212501   \n",
       "1943449                                 0.148477   \n",
       "1943450                                -0.264115   \n",
       "1943451                                -0.634334   \n",
       "\n",
       "         reconstructions_mean_latent_features_19  \\\n",
       "0                                       0.023329   \n",
       "1                                       1.376777   \n",
       "2                                       1.822399   \n",
       "3                                       1.871941   \n",
       "4                                       1.891553   \n",
       "...                                          ...   \n",
       "1943447                                 0.161597   \n",
       "1943448                                 0.018624   \n",
       "1943449                                 0.701049   \n",
       "1943450                                 0.201282   \n",
       "1943451                                -0.429966   \n",
       "\n",
       "         reconstructions_mean_latent_features_20  \\\n",
       "0                                      -0.327014   \n",
       "1                                       0.039747   \n",
       "2                                       0.524805   \n",
       "3                                       0.525425   \n",
       "4                                       0.525191   \n",
       "...                                          ...   \n",
       "1943447                                -0.350359   \n",
       "1943448                                -0.311243   \n",
       "1943449                                 0.031916   \n",
       "1943450                                -0.340114   \n",
       "1943451                                -0.755454   \n",
       "\n",
       "         reconstructions_mean_latent_features_21  \\\n",
       "0                                      -0.275622   \n",
       "1                                       0.052369   \n",
       "2                                       0.453604   \n",
       "3                                       0.453050   \n",
       "4                                       0.453053   \n",
       "...                                          ...   \n",
       "1943447                                -0.294658   \n",
       "1943448                                -0.261731   \n",
       "1943449                                 0.065185   \n",
       "1943450                                -0.286505   \n",
       "1943451                                -0.634547   \n",
       "\n",
       "         reconstructions_mean_latent_features_22  \\\n",
       "0                                      -0.305517   \n",
       "1                                       0.040240   \n",
       "2                                       0.475764   \n",
       "3                                       0.475354   \n",
       "4                                       0.475336   \n",
       "...                                          ...   \n",
       "1943447                                -0.311811   \n",
       "1943448                                -0.289561   \n",
       "1943449                                 0.046811   \n",
       "1943450                                -0.305532   \n",
       "1943451                                -0.690522   \n",
       "\n",
       "         latent_representations_latent_features_0  \\\n",
       "0                                       10.813115   \n",
       "1                                        6.438653   \n",
       "2                                        1.799086   \n",
       "3                                        1.802612   \n",
       "4                                        1.804948   \n",
       "...                                           ...   \n",
       "1943447                                 10.735245   \n",
       "1943448                                 10.759145   \n",
       "1943449                                  6.383266   \n",
       "1943450                                 10.750884   \n",
       "1943451                                 15.290472   \n",
       "\n",
       "         latent_representations_latent_features_1  \\\n",
       "0                                       -8.007674   \n",
       "1                                       -5.288274   \n",
       "2                                       -2.400572   \n",
       "3                                       -2.402813   \n",
       "4                                       -2.404299   \n",
       "...                                           ...   \n",
       "1943447                                 -7.958227   \n",
       "1943448                                 -7.973412   \n",
       "1943449                                 -5.253120   \n",
       "1943450                                 -7.968170   \n",
       "1943451                                -10.792393   \n",
       "\n",
       "         latent_representations_latent_features_2  \\\n",
       "0                                       12.364199   \n",
       "1                                        6.705316   \n",
       "2                                        0.693698   \n",
       "3                                        0.698366   \n",
       "4                                        0.701460   \n",
       "...                                           ...   \n",
       "1943447                                 12.260830   \n",
       "1943448                                 12.292395   \n",
       "1943449                                  6.632952   \n",
       "1943450                                 12.281738   \n",
       "1943451                                 18.159103   \n",
       "\n",
       "         latent_representations_latent_features_3  \\\n",
       "0                                      -17.007282   \n",
       "1                                       -1.351625   \n",
       "2                                       15.269121   \n",
       "3                                       15.256314   \n",
       "4                                       15.247843   \n",
       "...                                           ...   \n",
       "1943447                                -16.724079   \n",
       "1943448                                -16.810815   \n",
       "1943449                                 -1.150614   \n",
       "1943450                                -16.780902   \n",
       "1943451                                -33.037109   \n",
       "\n",
       "         latent_representations_latent_features_4  \\\n",
       "0                                       -9.952607   \n",
       "1                                      -10.976459   \n",
       "2                                      -12.062453   \n",
       "3                                      -12.061617   \n",
       "4                                      -12.061057   \n",
       "...                                           ...   \n",
       "1943447                                 -9.970756   \n",
       "1943448                                 -9.965255   \n",
       "1943449                                -10.989484   \n",
       "1943450                                 -9.967198   \n",
       "1943451                                 -8.904607   \n",
       "\n",
       "         latent_representations_latent_features_5  \n",
       "0                                       10.117329  \n",
       "1                                        5.906591  \n",
       "2                                        1.437511  \n",
       "3                                        1.440938  \n",
       "4                                        1.443211  \n",
       "...                                           ...  \n",
       "1943447                                 10.041531  \n",
       "1943448                                 10.064739  \n",
       "1943449                                  5.853092  \n",
       "1943450                                 10.056808  \n",
       "1943451                                 14.428023  \n",
       "\n",
       "[1943452 rows x 33 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_.shape,df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_.shape,df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_[df_test_.cano == 141087].drop_duplicates(\"txkey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[df_test.cano == 141087]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "df_train_.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_test_.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_[df_test_.cano == 73737]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.cano_locdt_index == \"73737_116\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[output.cano_locdt_index == \"73737_116\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[output.cano_locdt_index.str.contains(\"73737\",na = False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_[df_test_.cano == 73737].sort_values(by = [\"cano\",\"locdt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_.shape, df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[df_test.cano == 73737].sort_values(by = [\"cano\",\"locdt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_[df_test_.cano == 73737].sort_values(by = [\"cano\",\"locdt\"]).drop_duplicates(subset =[\"cano\",\"locdt\",\"cosine_errors_mean\",\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.cano_locdt_index.str.contains(\"73737\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
