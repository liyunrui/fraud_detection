{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import lightgbm as lgb\n",
    "from bayes_opt import BayesianOptimization\n",
    "import sys\n",
    "sys.path.append(\"../fraud_detection/src/\")\n",
    "from util import lgb_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train application df shape: (1521787, 29)\n",
      "Test application df shape: (421665, 28)\n",
      "Process train/test application - done in 67s\n",
      "Train application df shape: (1521787, 43)\n",
      "Test application df shape: (421665, 42)\n",
      "Add bacno/cano feature - done in 17s\n",
      "Train application df shape: (1521787, 115)\n",
      "Test application df shape: (421665, 114)\n",
      "Add iterm-related feature - done in 108s\n",
      "Train application df shape: (1521787, 185)\n",
      "Test application df shape: (421665, 184)\n",
      "Add conam-related feature - done in 130s\n",
      "Train application df shape: (1521787, 209)\n",
      "Test application df shape: (421665, 208)\n",
      "Add hour-related feature - done in 545s\n",
      "Train application df shape: (1521787, 210)\n",
      "Test application df shape: (421665, 209)\n",
      "Add cano/conam feature - done in 42s\n",
      "Train application df shape: (1521787, 230)\n",
      "Test application df shape: (421665, 229)\n",
      "Add cano/bacno latent feature - done in 4s\n",
      "Train application df shape: (1521787, 265)\n",
      "Test application df shape: (421665, 264)\n",
      "Add locdt-related feature - done in 188s\n",
      "Train application df shape: (1521787, 272)\n",
      "Test application df shape: (421665, 271)\n",
      "Add mchno-related feature - done in 121s\n",
      "Train application df shape: (1521787, 279)\n",
      "Test application df shape: (421665, 278)\n",
      "Add scity-related feature - done in 120s\n",
      "Train application df shape: (1521787, 286)\n",
      "Test application df shape: (421665, 285)\n",
      "Add stocn-related feature - done in 122s\n",
      "Train application df shape: (1521787, 306)\n",
      "Test application df shape: (421665, 305)\n",
      "Add mchno/bacno latent feature - done in 6s\n",
      "Train application df shape: (1521787, 320)\n",
      "Test application df shape: (421665, 319)\n",
      "Add time second-level feature on bacno - done in 249s\n",
      "Train application df shape: (1521787, 334)\n",
      "Test application df shape: (421665, 333)\n",
      "Add time second-level feature on cano - done in 260s\n",
      "Train application df shape: (1521787, 348)\n",
      "Test application df shape: (421665, 347)\n",
      "Add time second-level feature on mchno - done in 271s\n",
      "Train application df shape: (1521787, 390)\n",
      "Test application df shape: (421665, 389)\n",
      "Add time second-level feature on csmcu/stocn/scity - done in 424s\n",
      "Train application df shape: (1521787, 446)\n",
      "Test application df shape: (421665, 445)\n",
      "Add time second-level feature on acqic/csmcu/stocn/scity - done in 661s\n",
      "Train application df shape: (1521787, 488)\n",
      "Test application df shape: (421665, 487)\n",
      "Add conam-related feature v3 - done in 449s\n",
      "Train application df shape: (1521787, 572)\n",
      "Test application df shape: (421665, 571)\n",
      "Add locdt-related feature v2 - done in 707s\n",
      "Train application df shape: (1521787, 641)\n",
      "Test application df shape: (421665, 640)\n",
      "Add conam-related feature v4 - done in 761s\n",
      "Train application df shape: (1521787, 661)\n",
      "Test application df shape: (421665, 660)\n",
      "Add cano/mchno latent feature - done in 11s\n",
      "Train application df shape: (1521787, 681)\n",
      "Test application df shape: (421665, 680)\n",
      "Add cano/locdt latent feature - done in 11s\n",
      "Train application df shape: (1521787, 701)\n",
      "Test application df shape: (421665, 700)\n",
      "Add mchno/locdt latent feature - done in 10s\n",
      "Run LightGBM with kfold - done in 20s\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../fraud_detection/src/\")\n",
    "import time\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import numpy as np\n",
    "from contextlib import contextmanager\n",
    "import gc \n",
    "from util import s_to_time_format, string_to_datetime, hour_to_range, kfold_lightgbm, kfold_xgb\n",
    "from util import rolling_stats_target_by_cols\n",
    "#from util import _time_elapsed_between_last_transactions,time_elapsed_between_last_transactions\n",
    "#from util import num_transaction_in_past_n_days\n",
    "#from util import add_auto_encoder_feature\n",
    "#from util import group_target_by_cols_split_by_users\n",
    "from time import strftime, localtime\n",
    "import logging\n",
    "import sys\n",
    "from config import Configs\n",
    "\n",
    "# logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "#log_file = '{}-{}-{}.log'.format(opt.model_name, opt.dataset, strftime(\"%y%m%d-%H%M\", localtime()))\n",
    "log_file = '../fraud_detection/result/fs_{}.log'.format(strftime(\"%y%m%d-%H%M\", localtime()))\n",
    "logger.addHandler(logging.FileHandler(log_file))\n",
    "\n",
    "def group_target_by_cols(df_train, df_test, recipe):\n",
    "    df = pd.concat([df_train, df_test], axis = 0)\n",
    "    for m in range(len(recipe)):\n",
    "        cols = recipe[m][0]\n",
    "        for n in range(len(recipe[m][1])):\n",
    "            target = recipe[m][1][n][0]\n",
    "            method = recipe[m][1][n][1]\n",
    "            name_grouped_target = method+\"_\"+target+'_BY_'+'_'.join(cols)\n",
    "            tmp = df[cols + [target]].groupby(cols).agg(method)\n",
    "            tmp = tmp.reset_index().rename(index=str, columns={target: name_grouped_target})\n",
    "            df_train = df_train.merge(tmp, how='left', on=cols)\n",
    "            df_test = df_test.merge(tmp, how='left', on=cols)\n",
    "\n",
    "        # reduced memory    \n",
    "        del tmp\n",
    "        gc.collect()\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    logger.info(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
    "    \n",
    "def main(args):\n",
    "    with timer(\"Process train/test application\"):\n",
    "        #-------------------------\n",
    "        # load dataset\n",
    "        #-------------------------\n",
    "        df_train = pd.read_csv(args.train_file)\n",
    "        df_test = pd.read_csv(args.test_file)\n",
    "        #-------------------------\n",
    "        # pre-processing\n",
    "        #-------------------------\n",
    "\n",
    "        for cat in Configs.CATEGORY:\n",
    "            df_train[cat] = df_train[cat].astype('category') #.cat.codes\n",
    "            df_test[cat] = df_test[cat].astype('category')\n",
    "            \n",
    "        for df in [df_train, df_test]:\n",
    "            # pre-processing\n",
    "            df[\"loctm_\"] = df.loctm.astype(int).astype(str)\n",
    "            df.loctm_ = df.loctm_.apply(s_to_time_format).apply(string_to_datetime)\n",
    "            # # time-related feature\n",
    "            df[\"loctm_hour_of_day\"] = df.loctm_.apply(lambda x: x.hour).astype('category')\n",
    "            df[\"loctm_minute_of_hour\"] = df.loctm_.apply(lambda x: x.minute)\n",
    "            df[\"loctm_second_of_min\"] = df.loctm_.apply(lambda x: x.second)\n",
    "            # df[\"loctm_absolute_time\"] = [h*60+m for h,m in zip(df.loctm_hour_of_day,df.loctm_minute_of_hour)]\n",
    "            df[\"hour_range\"] = df.loctm_.apply(lambda x: hour_to_range(x.hour)).astype(\"category\")\n",
    "            # removed the columns no need\n",
    "            df.drop(columns = [\"loctm_\"], axis = 1, inplace = True)\n",
    "            # auxiliary fields\n",
    "            df[\"day_hr_min\"] = [\"{}:{}:{}\".format(i,j,k) for i,j,k in zip(df.locdt,df.loctm_hour_of_day,df.loctm_minute_of_hour)]\n",
    "            df[\"day_hr_min_sec\"] = [\"{}:{}:{}:{}\".format(i,j,k,z) for i,j,k,z in zip(df.locdt,df.loctm_hour_of_day,df.loctm_minute_of_hour,df.loctm_second_of_min)]\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add bacno/cano feature\"):\n",
    "        df_train, df_test = group_target_by_cols(df_train, df_test, Configs.CONAM_AGG_RECIPE_1)\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add iterm-related feature\"):\n",
    "        df_train, df_test = group_target_by_cols(df_train, df_test, Configs.ITERM_AGG_RECIPE)\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add conam-related feature\"):\n",
    "        df_train, df_test = group_target_by_cols(df_train, df_test, Configs.CONAM_AGG_RECIPE_2)\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add hour-related feature\"):\n",
    "        df_train, df_test = group_target_by_cols(df_train, df_test, Configs.HOUR_AGG_RECIPE)\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add cano/conam feature\"):\n",
    "        df_train, df_test = group_target_by_cols(df_train, df_test, Configs.CANO_CONAM_COUNT_RECIPE)\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add cano/bacno latent feature\"):\n",
    "        df = pd.read_csv(\"../fraud_detection/features/bacno_latent_features_w_cano.csv\")\n",
    "        df_train = df_train.merge(df, on = \"bacno\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"bacno\", how = \"left\")\n",
    "        df = pd.read_csv(\"../fraud_detection/features/bacno_cano_latent_features.csv\")\n",
    "        df_train = df_train.merge(df, on = \"cano\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"cano\", how = \"left\")\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add locdt-related feature\"):\n",
    "        df_train, df_test = group_target_by_cols(df_train, df_test, Configs.LOCDT_CONAM_RECIPE)\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add mchno-related feature\"):\n",
    "        df_train, df_test = group_target_by_cols(df_train, df_test, Configs.MCHNO_CONAM_RECIPE)\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add scity-related feature\"):\n",
    "        df_train, df_test = group_target_by_cols(df_train, df_test, Configs.SCITY_CONAM_RECIPE)\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add stocn-related feature\"):\n",
    "        df_train, df_test = group_target_by_cols(df_train, df_test, Configs.STOCN_CONAM_RECIPE)\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add mchno/bacno latent feature\"):\n",
    "        df = pd.read_csv(\"../fraud_detection/features/bacno_latent_features_w_mchno.csv\")\n",
    "        df_train = df_train.merge(df, on = \"bacno\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"bacno\", how = \"left\")\n",
    "        df = pd.read_csv(\"../fraud_detection/features/bacno_mchno_latent_features.csv\")\n",
    "        df_train = df_train.merge(df, on = \"mchno\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"mchno\", how = \"left\")\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add time second-level feature on bacno\"):\n",
    "        df_train, df_test = group_target_by_cols(\n",
    "            df_train, \n",
    "            df_test, \n",
    "            Configs.HOUR_AGG_SEC_LEVEL_RECIPE_BACNO,\n",
    "            )\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add time second-level feature on cano\"):\n",
    "        df_train, df_test = group_target_by_cols(\n",
    "            df_train, \n",
    "            df_test, \n",
    "            Configs.HOUR_AGG_SEC_LEVEL_RECIPE_CANO,\n",
    "            )\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add time second-level feature on mchno\"):\n",
    "        df_train, df_test = group_target_by_cols(\n",
    "            df_train, \n",
    "            df_test, \n",
    "            Configs.HOUR_AGG_SEC_LEVEL_RECIPE_MCHNO,\n",
    "            )\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add time second-level feature on csmcu/stocn/scity\"):\n",
    "        df_train, df_test = group_target_by_cols(\n",
    "            df_train, \n",
    "            df_test, \n",
    "            Configs.HOUR_AGG_SEC_LEVEL_RECIPE,\n",
    "            )\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add time second-level feature on acqic/csmcu/stocn/scity\"):\n",
    "        df_train, df_test = group_target_by_cols(\n",
    "            df_train, \n",
    "            df_test, \n",
    "            Configs.HOUR_AGG_SEC_LEVEL_RECIPE_2,\n",
    "            )\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add conam-related feature v3\"):\n",
    "        df_train, df_test = group_target_by_cols(\n",
    "            df_train, \n",
    "            df_test, \n",
    "            Configs.CONAM_AGG_RECIPE_3,\n",
    "            )\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add locdt-related feature v2\"):\n",
    "        df_train, df_test = group_target_by_cols(df_train, df_test, Configs.LOCDT_CONAM_RECIPE_2)\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add conam-related feature v4\"):\n",
    "        df_train, df_test = group_target_by_cols(\n",
    "            df_train, \n",
    "            df_test, \n",
    "            Configs.CONAM_AGG_RECIPE_4,\n",
    "            )\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add cano/mchno latent feature\"):\n",
    "        df = pd.read_csv(\"../fraud_detection/features/cano_latent_features_w_mchno.csv\")\n",
    "        df_train = df_train.merge(df, on = \"cano\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"cano\", how = \"left\")\n",
    "        df = pd.read_csv(\"../fraud_detection/features/cano_mchno_latent_features.csv\")\n",
    "        df_train = df_train.merge(df, on = \"mchno\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"mchno\", how = \"left\")\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))   \n",
    "\n",
    "    with timer(\"Add cano/locdt latent feature\"):\n",
    "        df = pd.read_csv(\"../fraud_detection/features/cano_latent_features_w_locdt.csv\")\n",
    "        df_train = df_train.merge(df, on = \"cano\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"cano\", how = \"left\")\n",
    "        df = pd.read_csv(\"../fraud_detection/features/cano_locdt_latent_features.csv\")\n",
    "        df_train = df_train.merge(df, on = \"locdt\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"locdt\", how = \"left\")\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add mchno/locdt latent feature\"):\n",
    "        df = pd.read_csv(\"../fraud_detection/features/mchno_latent_features_w_locdt.csv\")\n",
    "        df_train = df_train.merge(df, on = \"mchno\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"mchno\", how = \"left\")\n",
    "        df = pd.read_csv(\"../fraud_detection/features/mchno_locdt_latent_features.csv\")\n",
    "        df_train = df_train.merge(df, on = \"locdt\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"locdt\", how = \"left\")\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    #return df_train, df_test\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    with timer(\"Run LightGBM with kfold\"):\n",
    "        if args.feature_selection:\n",
    "            logger.info(\"==============Feature Selection==============\")\n",
    "            for df in [df_train, df_test]:\n",
    "                # drop random features (by null hypothesis)\n",
    "                df.drop(Configs.FEATURE_GRAVEYARD, axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "                # drop unused features features_with_no_imp_at_least_twice\n",
    "                df.drop(Configs.FEATURE_USELESSNESS, axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "                gc.collect()   \n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        for df in [df_train, df_test]:\n",
    "            df.drop(columns = [\"loctm_hour_of_day\",\n",
    "                               \"loctm_minute_of_hour\", \n",
    "                               \"loctm_second_of_min\",\n",
    "                               \"day_hr_min\",\n",
    "                               \"day_hr_min_sec\",\n",
    "                               ], axis = 1, inplace = True)\n",
    "   \n",
    "    return df_train, df_test\n",
    "\n",
    "args = {\n",
    " \"train_file\":\"/data/yunrui_li/fraud/dataset/train.csv\",\n",
    " \"test_file\":\"/data/yunrui_li/fraud/dataset/test.csv\",\n",
    " \"result_path\":\"/data/yunrui_li/fraud/fraud_detection/result/submission.csv\",\n",
    " \"feature_selection\":True,\n",
    " \"feature_importance_plot\": True,\n",
    " \"SEED\": 1030,\n",
    " \"NUM_FOLDS\": 2, # 5\n",
    " \"CPU_USE_RATE\":1.0,\n",
    " \"STRATIFIED\": True,\n",
    " \"TEST_NULL_HYPO\":False,\n",
    " \"NUM_LEAVES\":31,\n",
    " \"COLSAMPLE_BYTREE\":1.0,\n",
    " \"SUBSAMPLE\": 1.0,\n",
    " \"SUBSAMPLE_FREQ\": 0,\n",
    " \"MAX_DEPTH\": -1,\n",
    " \"REG_ALPHA\": 0.0,\n",
    " \"REG_LAMBDA\": 0.0,\n",
    " \"MIN_SPLIT_GAIN\": 0.0,\n",
    " \"MIN_CHILD_WEIGHT\": 0.001,\n",
    " \"MAX_BIN\": 255,\n",
    " \"SCALE_POS_WEIGHT\": 3\n",
    "    \n",
    "}\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "args = AttrDict(args)\n",
    "df_train, df_test = main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = [f for f in df_train.columns if f not in [\"fraud_ind\"]]\n",
    "X,y = df_train[feats], df_train.fraud_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1521787, 695), (1521787,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-788ee363e0a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert 1==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def lgb_f1_score(y_pred, y_true):\n",
    "    \"\"\"evaluation metric\"\"\"\n",
    "    #print (\"y_pred\",y_pred)\n",
    "    #print (\"y_true\",y_true)\n",
    "    y_hat = np.round(y_pred)\n",
    "    return 'f1', f1_score(y_true.get_label(), y_hat), True\n",
    "\n",
    "def bayes_parameter_opt_lgb(X, y, \n",
    "                            init_round=15, \n",
    "                            opt_round=25, \n",
    "                            n_folds=5, \n",
    "                            random_seed=1030,\n",
    "                            n_estimators=10000,\n",
    "                            learning_rate=0.05, \n",
    "                            output_process=True):\n",
    "    # prepare data\n",
    "    train_data = lgb.Dataset(data=X, label=y, categorical_feature='auto', free_raw_data = False)\n",
    "    # parameters\n",
    "    def lgb_eval(num_leaves, feature_fraction, bagging_fraction,\n",
    "                 #max_depth, \n",
    "                 lambda_l1, lambda_l2, min_split_gain, \n",
    "                 min_child_weight, max_bin, scale_pos_weight):\n",
    "        params = {'application':'binary',\n",
    "                  'num_iterations': n_estimators, \n",
    "                  'learning_rate':learning_rate, \n",
    "                  'early_stopping_round':100, \n",
    "                  'n_jobs':5,\n",
    "                  }\n",
    "        params[\"num_leaves\"] = int(round(num_leaves))\n",
    "        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n",
    "        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n",
    "        #params['max_depth'] = int(round(max_depth))\n",
    "        params['lambda_l1'] = max(lambda_l1, 0)\n",
    "        params['lambda_l2'] = max(lambda_l2, 0)\n",
    "        params['min_split_gain'] = min_split_gain\n",
    "        params['min_child_weight'] = min_child_weight\n",
    "        params['max_bin'] = int(round(max_bin))\n",
    "        params['scale_pos_weight'] = scale_pos_weight\n",
    "\n",
    "        cv_result = lgb.cv(params, \n",
    "                           train_data, \n",
    "                           nfold=n_folds,\n",
    "                           seed=random_seed, \n",
    "                           stratified=True, \n",
    "                           categorical_feature = \"auto\",\n",
    "                           feval=lgb_f1_score)\n",
    "        print (cv_result)\n",
    "        return max(cv_result['f1-mean'])\n",
    "    # range \n",
    "    lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (24, 45),\n",
    "                                            'feature_fraction': (0.5, 1.0),\n",
    "                                            'bagging_fraction': (0.5, 1.0),\n",
    "                                            'lambda_l1': (0, 5),\n",
    "                                            'lambda_l2': (0, 3),\n",
    "                                            'min_split_gain': (0.0, 0.1),\n",
    "                                            'min_child_weight': (1, 50),\n",
    "                                            'scale_pos_weight': (1, 10),\n",
    "                                            'max_bin': (255,355),\n",
    "                                           }, \n",
    "                                 random_state=0)\n",
    "    # optimize\n",
    "    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n",
    "    \n",
    "    # output optimization process\n",
    "    if output_process==True: \n",
    "        pd.DataFrame(lgbBO.res).sort_values(by = \"target\", ascending=False).to_csv(\"../fraud_detection/result/bayes_opt_result.csv\")\n",
    "    return lgbBO.max[\"target\"], lgbBO.max[\"params\"] # best score and best parameter\n",
    "#     return lgbBO\n",
    "#     # return best parameters\n",
    "#     return lgbBO.res['max']['max_params']\n",
    "\n",
    "opt_score, opt_params = bayes_parameter_opt_lgb(X, y, \n",
    "                                     init_round=5, \n",
    "                                     opt_round=10, \n",
    "                                     n_folds=5, \n",
    "                                     random_seed=1030, \n",
    "                                     n_estimators=10000, \n",
    "                                     learning_rate=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_params.max[\"target\"], opt_params.max[\"params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 1000\n",
    "pd.DataFrame(opt_params.res).sort_values(by = \"target\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"A\":[1,2,3,4],\"B\":[1,1,1,1]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"A\"] = df[\"A\"].copy().sample(frac = 1.0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " df.A.copy().sample(frac = 1.0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier(\n",
    "    n_jobs = 3,\n",
    "    boosting_type = \"rf\",\n",
    "    # nthread=int(multiprocessing.cpu_count()*args.CPU_USE_RATE),\n",
    "    n_estimators=10000,\n",
    "    )\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(lgb.cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
