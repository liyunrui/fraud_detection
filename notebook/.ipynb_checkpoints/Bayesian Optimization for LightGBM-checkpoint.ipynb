{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import lightgbm as lgb\n",
    "from bayes_opt import BayesianOptimization\n",
    "import sys\n",
    "sys.path.append(\"../fraud_detection/src/\")\n",
    "from util import lgb_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train application df shape: (1521787, 477)\n",
      "Test application df shape: (421665, 476)\n",
      "Load train/test features extracted - done in 124s\n",
      "Train application df shape: (1521787, 487)\n",
      "Test application df shape: (421665, 486)\n",
      "Add bacno time aggregate average feature - done in 236s\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../fraud_detection/src/\")\n",
    "import time\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import numpy as np\n",
    "from contextlib import contextmanager\n",
    "import gc \n",
    "from util import s_to_time_format, string_to_datetime, hour_to_range, kfold_lightgbm, kfold_xgb\n",
    "from util import rolling_stats_target_by_cols\n",
    "#from util import _time_elapsed_between_last_transactions,time_elapsed_between_last_transactions\n",
    "#from util import num_transaction_in_past_n_days\n",
    "#from util import add_auto_encoder_feature\n",
    "#from util import group_target_by_cols_split_by_users\n",
    "from time import strftime, localtime\n",
    "import logging\n",
    "import sys\n",
    "from config import Configs\n",
    "\n",
    "# logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "#log_file = '{}-{}-{}.log'.format(opt.model_name, opt.dataset, strftime(\"%y%m%d-%H%M\", localtime()))\n",
    "log_file = '../fraud_detection/result/fs_{}.log'.format(strftime(\"%y%m%d-%H%M\", localtime()))\n",
    "logger.addHandler(logging.FileHandler(log_file))\n",
    "\n",
    "def group_target_by_cols(df_train, df_test, recipe):\n",
    "    df = pd.concat([df_train, df_test], axis = 0)\n",
    "    for m in range(len(recipe)):\n",
    "        cols = recipe[m][0]\n",
    "        for n in range(len(recipe[m][1])):\n",
    "            target = recipe[m][1][n][0]\n",
    "            method = recipe[m][1][n][1]\n",
    "            name_grouped_target = method+\"_\"+target+'_BY_'+'_'.join(cols)\n",
    "            tmp = df[cols + [target]].groupby(cols).agg(method)\n",
    "            tmp = tmp.reset_index().rename(index=str, columns={target: name_grouped_target})\n",
    "            df_train = df_train.merge(tmp, how='left', on=cols)\n",
    "            df_test = df_test.merge(tmp, how='left', on=cols)\n",
    "\n",
    "        # reduced memory    \n",
    "        del tmp\n",
    "        gc.collect()\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    logger.info(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
    "    \n",
    "def main(args):\n",
    "    if args.load_feature == True:\n",
    "        with timer(\"Load train/test features extracted\"):\n",
    "            #-------------------------\n",
    "            # load dataset\n",
    "            #-------------------------\n",
    "            df_train = pd.read_csv(\"../fraud_detection/features/train.csv\")\n",
    "            df_test = pd.read_csv(\"../fraud_detection/features/test.csv\")\n",
    "\n",
    "            #-------------------------\n",
    "            # pre-processing\n",
    "            #-------------------------\n",
    "\n",
    "            for cat in Configs.CATEGORY:\n",
    "                df_train[cat] = df_train[cat].astype('category') #.cat.codes\n",
    "                df_test[cat] = df_test[cat].astype('category')\n",
    "            for df in [df_train, df_test]:\n",
    "                df[\"hour_range\"] = df[\"hour_range\"].astype('category')\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))     \n",
    "\n",
    "    else:\n",
    "        with timer(\"Process train/test application\"):\n",
    "            #-------------------------\n",
    "            # load dataset\n",
    "            #-------------------------\n",
    "            df_train = pd.read_csv(args.train_file)\n",
    "            df_test = pd.read_csv(args.test_file)\n",
    "\n",
    "            #-------------------------\n",
    "            # pre-processing\n",
    "            #-------------------------\n",
    "\n",
    "            for cat in Configs.CATEGORY:\n",
    "                df_train[cat] = df_train[cat].astype('category') #.cat.codes\n",
    "                df_test[cat] = df_test[cat].astype('category')\n",
    "                \n",
    "            for df in [df_train, df_test]:\n",
    "                # pre-processing\n",
    "                df[\"loctm_\"] = df.loctm.astype(int).astype(str)\n",
    "                df.loctm_ = df.loctm_.apply(s_to_time_format).apply(string_to_datetime)\n",
    "                # # time-related feature\n",
    "                df[\"loctm_hour_of_day\"] = df.loctm_.apply(lambda x: x.hour).astype('category')\n",
    "                df[\"loctm_minute_of_hour\"] = df.loctm_.apply(lambda x: x.minute)\n",
    "                df[\"loctm_second_of_min\"] = df.loctm_.apply(lambda x: x.second)\n",
    "                # df[\"loctm_absolute_time\"] = [h*60+m for h,m in zip(df.loctm_hour_of_day,df.loctm_minute_of_hour)]\n",
    "                df[\"hour_range\"] = df.loctm_.apply(lambda x: hour_to_range(x.hour)).astype(\"category\")\n",
    "                # removed the columns no need\n",
    "                df.drop(columns = [\"loctm_\"], axis = 1, inplace = True)\n",
    "                # auxiliary fields\n",
    "                df[\"day_hr_min\"] = [\"{}:{}:{}\".format(i,j,k) for i,j,k in zip(df.locdt,df.loctm_hour_of_day,df.loctm_minute_of_hour)]\n",
    "                df[\"day_hr_min_sec\"] = [\"{}:{}:{}:{}\".format(i,j,k,z) for i,j,k,z in zip(df.locdt,df.loctm_hour_of_day,df.loctm_minute_of_hour,df.loctm_second_of_min)]\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add bacno/cano feature\"):\n",
    "            df_train, df_test = group_target_by_cols(df_train, df_test, Configs.CONAM_AGG_RECIPE_1)\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add iterm-related feature\"):\n",
    "            df_train, df_test = group_target_by_cols(df_train, df_test, Configs.ITERM_AGG_RECIPE)\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add conam-related feature\"):\n",
    "            df_train, df_test = group_target_by_cols(df_train, df_test, Configs.CONAM_AGG_RECIPE_2)\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add hour-related feature\"):\n",
    "            df_train, df_test = group_target_by_cols(df_train, df_test, Configs.HOUR_AGG_RECIPE)\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add cano/conam feature\"):\n",
    "            df_train, df_test = group_target_by_cols(df_train, df_test, Configs.CANO_CONAM_COUNT_RECIPE)\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add cano/bacno latent feature\"):\n",
    "            df = pd.read_csv(\"../fraud_detection/features/bacno_latent_features_w_cano.csv\")\n",
    "            df_train = df_train.merge(df, on = \"bacno\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"bacno\", how = \"left\")\n",
    "            df = pd.read_csv(\"../fraud_detection/features/bacno_cano_latent_features.csv\")\n",
    "            df_train = df_train.merge(df, on = \"cano\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"cano\", how = \"left\")\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add locdt-related feature\"):\n",
    "            df_train, df_test = group_target_by_cols(df_train, df_test, Configs.LOCDT_CONAM_RECIPE)\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add mchno-related feature\"):\n",
    "            df_train, df_test = group_target_by_cols(df_train, df_test, Configs.MCHNO_CONAM_RECIPE)\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add scity-related feature\"):\n",
    "            df_train, df_test = group_target_by_cols(df_train, df_test, Configs.SCITY_CONAM_RECIPE)\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add stocn-related feature\"):\n",
    "            df_train, df_test = group_target_by_cols(df_train, df_test, Configs.STOCN_CONAM_RECIPE)\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add mchno/bacno latent feature\"):\n",
    "            df = pd.read_csv(\"../fraud_detection/features/bacno_latent_features_w_mchno.csv\")\n",
    "            df_train = df_train.merge(df, on = \"bacno\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"bacno\", how = \"left\")\n",
    "            df = pd.read_csv(\"../fraud_detection/features/bacno_mchno_latent_features.csv\")\n",
    "            df_train = df_train.merge(df, on = \"mchno\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"mchno\", how = \"left\")\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add time second-level feature on bacno\"):\n",
    "            df_train, df_test = group_target_by_cols(\n",
    "                df_train, \n",
    "                df_test, \n",
    "                Configs.HOUR_AGG_SEC_LEVEL_RECIPE_BACNO,\n",
    "                )\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add time second-level feature on cano\"):\n",
    "            df_train, df_test = group_target_by_cols(\n",
    "                df_train, \n",
    "                df_test, \n",
    "                Configs.HOUR_AGG_SEC_LEVEL_RECIPE_CANO,\n",
    "                )\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add time second-level feature on mchno\"):\n",
    "            df_train, df_test = group_target_by_cols(\n",
    "                df_train, \n",
    "                df_test, \n",
    "                Configs.HOUR_AGG_SEC_LEVEL_RECIPE_MCHNO,\n",
    "                )\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add time second-level feature on csmcu/stocn/scity\"):\n",
    "            df_train, df_test = group_target_by_cols(\n",
    "                df_train, \n",
    "                df_test, \n",
    "                Configs.HOUR_AGG_SEC_LEVEL_RECIPE,\n",
    "                )\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add time second-level feature on acqic/csmcu/stocn/scity\"):\n",
    "            df_train, df_test = group_target_by_cols(\n",
    "                df_train, \n",
    "                df_test, \n",
    "                Configs.HOUR_AGG_SEC_LEVEL_RECIPE_2,\n",
    "                )\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add conam-related feature v3\"):\n",
    "            df_train, df_test = group_target_by_cols(\n",
    "                df_train, \n",
    "                df_test, \n",
    "                Configs.CONAM_AGG_RECIPE_3,\n",
    "                )\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add locdt-related feature v2\"):\n",
    "            df_train, df_test = group_target_by_cols(df_train, df_test, Configs.LOCDT_CONAM_RECIPE_2)\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add conam-related feature v4\"):\n",
    "            df_train, df_test = group_target_by_cols(\n",
    "                df_train, \n",
    "                df_test, \n",
    "                Configs.CONAM_AGG_RECIPE_4,\n",
    "                )\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add cano/mchno latent feature\"):\n",
    "            df = pd.read_csv(\"../fraud_detection/features/cano_latent_features_w_mchno.csv\")\n",
    "            df_train = df_train.merge(df, on = \"cano\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"cano\", how = \"left\")\n",
    "            df = pd.read_csv(\"../fraud_detection/features/cano_mchno_latent_features.csv\")\n",
    "            df_train = df_train.merge(df, on = \"mchno\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"mchno\", how = \"left\")\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add cano/locdt latent feature\"):\n",
    "            df = pd.read_csv(\"../fraud_detection/features/cano_latent_features_w_locdt.csv\")\n",
    "            df_train = df_train.merge(df, on = \"cano\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"cano\", how = \"left\")\n",
    "            df = pd.read_csv(\"../fraud_detection/features/cano_locdt_latent_features.csv\")\n",
    "            df_train = df_train.merge(df, on = \"locdt\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"locdt\", how = \"left\")\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add mchno/locdt latent feature\"):\n",
    "            df = pd.read_csv(\"../fraud_detection/features/mchno_latent_features_w_locdt.csv\")\n",
    "            df_train = df_train.merge(df, on = \"mchno\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"mchno\", how = \"left\")\n",
    "            df = pd.read_csv(\"../fraud_detection/features/mchno_locdt_latent_features.csv\")\n",
    "            df_train = df_train.merge(df, on = \"locdt\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"locdt\", how = \"left\")\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        with timer(\"Add mchno time aggregate average feature\"):\n",
    "            # df = pd.read_csv(\"../features/average_mchno_time_agg.csv\")\n",
    "            # df_train = df_train.merge(df, on = \"txkey\", how = \"left\")\n",
    "            # df_test = df_test.merge(df, on = \"txkey\", how = \"left\")\n",
    "            df = pd.read_csv(\"../fraud_detection/features/average_mchno_mean_conam_in_past_7_days.csv\")\n",
    "            df_train = df_train.merge(df, on = \"mchno\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"mchno\", how = \"left\")\n",
    "\n",
    "            df = pd.read_csv(\"../fraud_detection/features/average_mchno_mean_conam_in_past_14_days.csv\")\n",
    "            df_train = df_train.merge(df, on = \"mchno\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"mchno\", how = \"left\")\n",
    "\n",
    "            df = pd.read_csv(\"../fraud_detection/features/average_mchno_std_conam_in_past_7_days.csv\")\n",
    "            df_train = df_train.merge(df, on = \"mchno\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"mchno\", how = \"left\")\n",
    "\n",
    "            df = pd.read_csv(\"../fraud_detection/features/average_mchno_std_conam_in_past_14_days.csv\")\n",
    "            df_train = df_train.merge(df, on = \"mchno\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"mchno\", how = \"left\")\n",
    "\n",
    "            df = pd.read_csv(\"../fraud_detection/features/average_mchno_min_conam_in_past_7_days.csv\")\n",
    "            df_train = df_train.merge(df, on = \"mchno\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"mchno\", how = \"left\")\n",
    "\n",
    "            df = pd.read_csv(\"../fraud_detection/features/average_mchno_min_conam_in_past_14_days.csv\")\n",
    "            df_train = df_train.merge(df, on = \"mchno\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"mchno\", how = \"left\")\n",
    "\n",
    "            df = pd.read_csv(\"../fraud_detection/features/average_mchno_max_conam_in_past_7_days.csv\")\n",
    "            df_train = df_train.merge(df, on = \"mchno\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"mchno\", how = \"left\")\n",
    "\n",
    "            df = pd.read_csv(\"../fraud_detection/features/average_mchno_max_conam_in_past_14_days.csv\")\n",
    "            df_train = df_train.merge(df, on = \"mchno\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"mchno\", how = \"left\")\n",
    "\n",
    "            df = pd.read_csv(\"../fraud_detection/features/average_mchno_median_conam_in_past_7_days.csv\")\n",
    "            df_train = df_train.merge(df, on = \"mchno\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"mchno\", how = \"left\")\n",
    "\n",
    "            df = pd.read_csv(\"../fraud_detection/features/average_mchno_median_conam_in_past_14_days.csv\")\n",
    "            df_train = df_train.merge(df, on = \"mchno\", how = \"left\")\n",
    "            df_test = df_test.merge(df, on = \"mchno\", how = \"left\")\n",
    "\n",
    "            logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "            logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add bacno time aggregate average feature\"):\n",
    "        df = pd.read_csv(\"../fraud_detection/features/average_bacno_min_conam_in_past_7_days.csv\").iloc[:,1:]\n",
    "        df_train = df_train.merge(df, on = \"bacno\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"bacno\", how = \"left\")\n",
    "\n",
    "        df = pd.read_csv(\"../fraud_detection/features/average_bacno_max_conam_in_past_7_days.csv\").iloc[:,1:]\n",
    "        df_train = df_train.merge(df, on = \"bacno\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"bacno\", how = \"left\")\n",
    "\n",
    "        df = pd.read_csv(\"../fraud_detection/features/average_bacno_mean_conam_in_past_7_days.csv\").iloc[:,1:]\n",
    "        df_train = df_train.merge(df, on = \"bacno\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"bacno\", how = \"left\")\n",
    "\n",
    "        df = pd.read_csv(\"../fraud_detection/features/average_bacno_median_conam_in_past_7_days.csv\").iloc[:,1:]\n",
    "        df_train = df_train.merge(df, on = \"bacno\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"bacno\", how = \"left\")\n",
    "\n",
    "        df = pd.read_csv(\"../fraud_detection/features/average_bacno_std_conam_in_past_7_days.csv\").iloc[:,1:]\n",
    "        df_train = df_train.merge(df, on = \"bacno\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"bacno\", how = \"left\")\n",
    "\n",
    "        df = pd.read_csv(\"../fraud_detection/features/average_bacno_min_conam_in_past_14_days.csv\").iloc[:,1:]\n",
    "        df_train = df_train.merge(df, on = \"bacno\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"bacno\", how = \"left\")\n",
    "\n",
    "        df = pd.read_csv(\"../fraud_detection/features/average_bacno_max_conam_in_past_14_days.csv\").iloc[:,1:]\n",
    "        df_train = df_train.merge(df, on = \"bacno\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"bacno\", how = \"left\")\n",
    "\n",
    "        df = pd.read_csv(\"../fraud_detection/features/average_bacno_mean_conam_in_past_14_days.csv\").iloc[:,1:]\n",
    "        df_train = df_train.merge(df, on = \"bacno\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"bacno\", how = \"left\")\n",
    "\n",
    "        df = pd.read_csv(\"../fraud_detection/features/average_bacno_median_conam_in_past_14_days.csv\").iloc[:,1:]\n",
    "        df_train = df_train.merge(df, on = \"bacno\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"bacno\", how = \"left\")\n",
    "\n",
    "        df = pd.read_csv(\"../fraud_detection/features/average_bacno_std_conam_in_past_14_days.csv\").iloc[:,1:]\n",
    "        df_train = df_train.merge(df, on = \"bacno\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"bacno\", how = \"left\")\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "#     with timer(\"Add mcc time aggregate average feature\"):\n",
    "#         df = pd.read_csv(\"../fraud_detection/features/average_mcc_median_conam_in_past_7_days.csv\")\n",
    "#         df_train = df_train.merge(df, on = \"mcc\", how = \"left\")\n",
    "#         df_test = df_test.merge(df, on = \"mcc\", how = \"left\")\n",
    "\n",
    "#         df = pd.read_csv(\"../fraud_detection/features/average_mcc_max_conam_in_past_7_days.csv\")\n",
    "#         df_train = df_train.merge(df, on = \"mcc\", how = \"left\")\n",
    "#         df_test = df_test.merge(df, on = \"mcc\", how = \"left\")\n",
    "\n",
    "#         df = pd.read_csv(\"../fraud_detection/features/average_mcc_min_conam_in_past_7_days.csv\")\n",
    "#         df_train = df_train.merge(df, on = \"mcc\", how = \"left\")\n",
    "#         df_test = df_test.merge(df, on = \"mcc\", how = \"left\")\n",
    "\n",
    "#         df = pd.read_csv(\"../fraud_detection/features/average_mcc_mean_conam_in_past_7_days.csv\")\n",
    "#         df_train = df_train.merge(df, on = \"mcc\", how = \"left\")\n",
    "#         df_test = df_test.merge(df, on = \"mcc\", how = \"left\")\n",
    "\n",
    "#         df = pd.read_csv(\"../fraud_detection/features/average_mcc_std_conam_in_past_7_days.csv\")\n",
    "#         df_train = df_train.merge(df, on = \"mcc\", how = \"left\")\n",
    "#         df_test = df_test.merge(df, on = \"mcc\", how = \"left\")\n",
    "\n",
    "#         logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "#         logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "#     with timer(\"Add scity time aggregate feature\"):\n",
    "#         df = pd.read_csv(\"../fraud_detection/features/scity_mean_conam_in_past_7_days.csv\")\n",
    "#         df_train = df_train.merge(df, on = [\"scity\",\"locdt\"], how = \"left\")\n",
    "#         df_test = df_test.merge(df, on = [\"scity\",\"locdt\"], how = \"left\")\n",
    "\n",
    "#         df = pd.read_csv(\"../fraud_detection/features/scity_mean_conam_in_past_14_days.csv\")\n",
    "#         df_train = df_train.merge(df, on = [\"scity\",\"locdt\"], how = \"left\")\n",
    "#         df_test = df_test.merge(df, on = [\"scity\",\"locdt\"], how = \"left\")\n",
    "\n",
    "#         logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "#         logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "#     with timer(\"Add stocn time aggregate feature\"):\n",
    "#         df = pd.read_csv(\"../fraud_detection/features/stocn_mean_conam_in_past_7_days.csv\")\n",
    "#         df_train = df_train.merge(df, on = [\"stocn\",\"locdt\"], how = \"left\")\n",
    "#         df_test = df_test.merge(df, on = [\"stocn\",\"locdt\"], how = \"left\")\n",
    "\n",
    "#         df = pd.read_csv(\"../fraud_detection/features/stocn_mean_conam_in_past_14_days.csv\")\n",
    "#         df_train = df_train.merge(df, on = [\"stocn\",\"locdt\"], how = \"left\")\n",
    "#         df_test = df_test.merge(df, on = [\"stocn\",\"locdt\"], how = \"left\")\n",
    "        \n",
    "#         logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "#         logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "#     with timer(\"Add acqic time aggregate feature\"):\n",
    "#         df = pd.read_csv(\"../fraud_detection/features/acqic_mean_conam_in_past_7_days.csv\")\n",
    "#         df_train = df_train.merge(df, on = [\"acqic\",\"locdt\"], how = \"left\")\n",
    "#         df_test = df_test.merge(df, on = [\"acqic\",\"locdt\"], how = \"left\")\n",
    "\n",
    "#         df = pd.read_csv(\"../fraud_detection/features/acqic_mean_conam_in_past_14_days.csv\")\n",
    "#         df_train = df_train.merge(df, on = [\"acqic\",\"locdt\"], how = \"left\")\n",
    "#         df_test = df_test.merge(df, on = [\"acqic\",\"locdt\"], how = \"left\")\n",
    "        \n",
    "#         logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "#         logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "#     with timer(\"Add mchno time aggregate feature\"):\n",
    "#         df = pd.read_csv(\"../fraud_detection/features/mchno_mean_conam_in_past_7_days.csv\")\n",
    "#         df_train = df_train.merge(df, on = [\"mchno\",\"locdt\"], how = \"left\")\n",
    "#         df_test = df_test.merge(df, on = [\"mchno\",\"locdt\"], how = \"left\")\n",
    "\n",
    "#         df = pd.read_csv(\"../fraud_detection/features/mchno_mean_conam_in_past_14_days.csv\")\n",
    "#         df_train = df_train.merge(df, on = [\"mchno\",\"locdt\"], how = \"left\")\n",
    "#         df_test = df_test.merge(df, on = [\"mchno\",\"locdt\"], how = \"left\")\n",
    "        \n",
    "#         logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "#         logger.info(\"Test application df shape: {}\".format(df_test.shape))   \n",
    "        \n",
    "    return df_train, df_test\n",
    "\n",
    "args = {\n",
    " \"train_file\":\"/data/yunrui_li/fraud/dataset/train.csv\",\n",
    " \"test_file\":\"/data/yunrui_li/fraud/dataset/test.csv\",\n",
    " \"result_path\":\"/data/yunrui_li/fraud/fraud_detection/result/submission.csv\",\n",
    " \"feature_selection\":True,\n",
    " \"feature_importance_plot\": False,\n",
    " \"SEED\": 1030,\n",
    " \"NUM_FOLDS\": 5, # 5\n",
    " \"CPU_USE_RATE\":1.0,\n",
    " \"STRATIFIED\": True,\n",
    " \"NUM_LEAVES\":31,\n",
    " \"COLSAMPLE_BYTREE\":1.0,\n",
    " \"SUBSAMPLE\": 1.0,\n",
    " \"SUBSAMPLE_FREQ\": 0,\n",
    " \"MAX_DEPTH\": -1,\n",
    " \"REG_ALPHA\": 0.0,\n",
    " \"REG_LAMBDA\": 0.0,\n",
    " \"MIN_SPLIT_GAIN\": 0.0,\n",
    " \"MIN_CHILD_WEIGHT\": 0.001,\n",
    " \"MAX_BIN\": 255,\n",
    " \"SCALE_POS_WEIGHT\": 3,\n",
    " \"TEST_NULL_HYPO\":False,\n",
    " \"model\": \"lgb\",\n",
    " \"ensemble\":False,\n",
    " \"seed\":1030,\n",
    " \"load_feature\": True\n",
    "}\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "args = AttrDict(args)\n",
    "df_train, df_test = main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [df_train, df_test]:\n",
    "    # drop random features (by null hypothesis)\n",
    "    df.drop(Configs.FEATURE_GRAVEYARD, axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "    # drop unused features features_with_no_imp_at_least_twice\n",
    "    df.drop(Configs.FEATURE_USELESSNESS, axis=1, inplace=True, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = [f for f in df_train.columns if f not in [\"fraud_ind\"]]\n",
    "X,y = df_train[feats], df_train.fraud_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1521787, 479), (1521787,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-788ee363e0a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert 1==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... | featur... | scale_... |\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "lgb_eval() missing 1 required positional argument: 'num_leaves'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/fraud/lib/python3.6/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (0.8646440511781974, 0.9145568099117258, 6.4248703846447945)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-20b58d5558a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m                                      \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1030\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                                      \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                                      learning_rate=0.2)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-20b58d5558a8>\u001b[0m in \u001b[0;36mbayes_parameter_opt_lgb\u001b[0;34m(X, y, init_round, opt_round, n_folds, random_seed, n_estimators, learning_rate, output_process)\u001b[0m\n\u001b[1;32m     64\u001b[0m                                  random_state=0)\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mlgbBO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_round\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt_round\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# output optimization process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/fraud/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, xi, **gp_params)\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTMIZATION_END\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/fraud/lib/python3.6/site-packages/bayes_opt/bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOPTMIZATION_STEP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.5/envs/fraud/lib/python3.6/site-packages/bayes_opt/target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: lgb_eval() missing 1 required positional argument: 'num_leaves'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def lgb_f1_score(y_pred, y_true):\n",
    "    \"\"\"evaluation metric\"\"\"\n",
    "    #print (\"y_pred\",y_pred)\n",
    "    #print (\"y_true\",y_true)\n",
    "    y_hat = np.round(y_pred)\n",
    "    return 'f1', f1_score(y_true.get_label(), y_hat), True\n",
    "\n",
    "def bayes_parameter_opt_lgb(X, y, \n",
    "                            init_round=15, \n",
    "                            opt_round=25, \n",
    "                            n_folds=5, \n",
    "                            random_seed=1030,\n",
    "                            n_estimators=10000,\n",
    "                            learning_rate=0.05, \n",
    "                            output_process=True):\n",
    "    # prepare data\n",
    "    train_data = lgb.Dataset(data=X, label=y, categorical_feature='auto', free_raw_data = False)\n",
    "    # parameters\n",
    "    def lgb_eval(\n",
    "        #num_leaves, \n",
    "        feature_fraction, bagging_fraction,\n",
    "                 #max_depth, \n",
    "                 #lambda_l1, lambda_l2, min_split_gain, \n",
    "                 #min_child_weight, max_bin, \n",
    "                 scale_pos_weight):\n",
    "        params = {'application':'binary',\n",
    "                  'num_iterations': n_estimators, \n",
    "                  'learning_rate':learning_rate, \n",
    "                  'early_stopping_round':100, \n",
    "                  'n_jobs':-1,\n",
    "                  }\n",
    "#         params[\"num_leaves\"] = int(round(num_leaves))\n",
    "        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n",
    "        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n",
    "        #params['max_depth'] = int(round(max_depth))\n",
    "#         params['lambda_l1'] = max(lambda_l1, 0)\n",
    "#         params['lambda_l2'] = max(lambda_l2, 0)\n",
    "#         params['min_split_gain'] = min_split_gain\n",
    "#         params['min_child_weight'] = min_child_weight\n",
    "#         params['max_bin'] = int(round(max_bin))\n",
    "        params['scale_pos_weight'] = scale_pos_weight\n",
    "\n",
    "        cv_result = lgb.cv(params, \n",
    "                           train_data, \n",
    "                           nfold=n_folds,\n",
    "                           seed=random_seed, \n",
    "                           stratified=True, \n",
    "                           categorical_feature = \"auto\",\n",
    "                           feval=lgb_f1_score)\n",
    "        print (cv_result)\n",
    "        return max(cv_result['f1-mean'])\n",
    "    # range \n",
    "    lgbBO = BayesianOptimization(lgb_eval, {\n",
    "#         'num_leaves': (24, 45),\n",
    "                                            'feature_fraction': (0.7, 1.0),\n",
    "                                            'bagging_fraction': (0.7, 1.0),\n",
    "#                                             'lambda_l1': (0, 5),\n",
    "#                                             'lambda_l2': (0, 3),\n",
    "#                                             'min_split_gain': (0.0, 0.1),\n",
    "#                                             'min_child_weight': (1, 50),\n",
    "                                            'scale_pos_weight': (1, 10),\n",
    "#                                             'max_bin': (255,355),\n",
    "                                           }, \n",
    "                                 random_state=0)\n",
    "    # optimize\n",
    "    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n",
    "    \n",
    "    # output optimization process\n",
    "    if output_process==True: \n",
    "        pd.DataFrame(lgbBO.res).sort_values(by = \"target\", ascending=False).to_csv(\"../fraud_detection/result/bayes_opt_result.csv\")\n",
    "    return lgbBO.max[\"target\"], lgbBO.max[\"params\"] # best score and best parameter\n",
    "#     return lgbBO\n",
    "#     # return best parameters\n",
    "#     return lgbBO.res['max']['max_params']\n",
    "\n",
    "opt_score, opt_params = bayes_parameter_opt_lgb(X, y, \n",
    "                                     init_round=10, \n",
    "                                     opt_round=15, \n",
    "                                     n_folds=5, \n",
    "                                     random_seed=1030, \n",
    "                                     n_estimators=10000, \n",
    "                                     learning_rate=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.8890783754749252,\n",
       " 'feature_fraction': 0.9350060741234096,\n",
       " 'lambda_l1': 4.89309171116382,\n",
       " 'lambda_l2': 2.397475692650171,\n",
       " 'max_bin': 301.14793622529317,\n",
       " 'min_child_weight': 39.245929638036316,\n",
       " 'min_split_gain': 0.011827442586893323,\n",
       " 'num_leaves': 37.438341447878,\n",
       " 'scale_pos_weight': 2.2901795866814174}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8602295952767033"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_params.max[\"target\"], opt_params.max[\"params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 1000\n",
    "pd.DataFrame(opt_params.res).sort_values(by = \"target\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"A\":[1,2,3,4],\"B\":[1,1,1,1]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"A\"] = df[\"A\"].copy().sample(frac = 1.0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " df.A.copy().sample(frac = 1.0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = lgb.LGBMClassifier(\n",
    "    n_jobs = 3,\n",
    "    boosting_type = \"rf\",\n",
    "    # nthread=int(multiprocessing.cpu_count()*args.CPU_USE_RATE),\n",
    "    n_estimators=10000,\n",
    "    )\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(lgb.cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
