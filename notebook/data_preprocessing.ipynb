{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"../fraud_detection/src/\")\n",
    "from util import s_to_time_format, string_to_datetime,hour_to_range\n",
    "\n",
    "df_train = pd.read_csv(\"/data/yunrui_li/fraud/dataset/train.csv\")\n",
    "df_test = pd.read_csv(\"/data/yunrui_li/fraud/dataset/test.csv\")\n",
    "\n",
    "\n",
    "for df in [df_train, df_test]:\n",
    "    # pre-processing\n",
    "    df[\"loctm_\"] = df.loctm.astype(int).astype(str)\n",
    "    df.loctm_ = df.loctm_.apply(s_to_time_format).apply(string_to_datetime)\n",
    "    # time-related feature\n",
    "    df[\"loctm_hour_of_day\"] = df.loctm_.apply(lambda x: x.hour)\n",
    "    df[\"loctm_minute_of_hour\"] = df.loctm_.apply(lambda x: x.minute)\n",
    "    df[\"loctm_second_of_min\"] = df.loctm_.apply(lambda x: x.second)\n",
    "\n",
    "    # removed the columns no need\n",
    "    df.drop(columns = [\"loctm_\", \"loctm\",\"txkey\"], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def value_to_count(df_train, df_test):\n",
    "\n",
    "    # continuous_feats = [\"locdt\",\"conam\",\"loctm_hour_of_day\",\n",
    "    #                 \"loctm_minute_of_hour\",\"loctm_second_of_min\"]\n",
    "\n",
    "    # feats = [f for f in df_test.columns.tolist() if f not in continuous_feats]\n",
    "    feats = ['acqic', 'bacno', 'cano', 'conam', 'contp', 'csmcu', 'ecfg', 'etymd',\n",
    "       'flbmk', 'flg_3dsmk', 'hcefg', 'insfg', 'iterm', 'mcc',\n",
    "       'mchno', 'ovrlt', 'scity', 'stocn', 'stscd']\n",
    "\n",
    "    df = pd.concat([df_train[feats], df_test[feats]], axis = 0)\n",
    "    for f in tqdm(feats):\n",
    "        count_dict = df[f].value_counts(dropna = False).to_dict() \n",
    "        df_train[f] = df_train[f].apply(lambda v: count_dict[v])\n",
    "        df_test[f] = df_test[f].apply(lambda v: count_dict[v])\n",
    "        \n",
    "#     continuous_feats = ['locdt', 'loctm_hour_of_day', 'loctm_minute_of_hour', 'loctm_second_of_min']\n",
    "#     for f in tqdm(continuous_feats):\n",
    "#         df_train_[f] = df_train[f]\n",
    "#         df_test_[f] = df_test[f]\n",
    "        \n",
    "#     if mode == 'train':\n",
    "#         df_train_[\"fraud_ind\"] = df_train[\"fraud_ind\"]\n",
    "\n",
    "    return df_train, df_test\n",
    "\n",
    "def feature_normalization_auto(df_train, df_test):\n",
    "    \"\"\"\n",
    "    return two inputs of autoencoder, one is for train and another one is for test\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n",
    "    feats = ['acqic', 'bacno', 'cano', 'conam', 'contp', 'csmcu', 'ecfg', 'etymd',\n",
    "       'flbmk', 'flg_3dsmk', 'hcefg', 'insfg', 'iterm', 'locdt', 'mcc',\n",
    "       'mchno', 'ovrlt', 'scity', 'stocn', 'stscd', 'loctm_hour_of_day',\n",
    "       'loctm_minute_of_hour', 'loctm_second_of_min']\n",
    "    df = pd.concat([df_train[feats], df_test[feats]], axis = 0)\n",
    "\n",
    "\n",
    "    for f in tqdm(feats):\n",
    "        try:\n",
    "            #scaler = MinMaxScaler()\n",
    "            max_ = df[f].max()\n",
    "            min_ = df[f].min()\n",
    "            df_train[f] = df_train[f].apply(lambda x: (x-min_)/(max_-min_))\n",
    "            df_test[f] = df_test[f].apply(lambda x: (x-min_)/(max_-min_))\n",
    "        except:\n",
    "            print(f)\n",
    "    return df_train, df_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 100\n",
    "df_train[\"cano_locdt_index\"] = [\"{}_{}\".format(str(i),str(j)) for i,j in zip(df_train.cano,df_train.locdt)]\n",
    "df_test[\"cano_locdt_index\"] = [\"{}_{}\".format(str(i),str(j)) for i,j in zip(df_test.cano,df_test.locdt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:13<00:00,  1.36it/s]\n",
      "100%|██████████| 23/23 [00:31<00:00,  1.37s/it]\n"
     ]
    }
   ],
   "source": [
    "#from autoencoder import value_to_count,feature_normalization_auto\n",
    "df_train, df_test = value_to_count(df_train, df_test)\n",
    "df_train, df_test = feature_normalization_auto(df_train, df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acqic</th>\n",
       "      <th>bacno</th>\n",
       "      <th>cano</th>\n",
       "      <th>conam</th>\n",
       "      <th>contp</th>\n",
       "      <th>csmcu</th>\n",
       "      <th>ecfg</th>\n",
       "      <th>etymd</th>\n",
       "      <th>flbmk</th>\n",
       "      <th>flg_3dsmk</th>\n",
       "      <th>fraud_ind</th>\n",
       "      <th>hcefg</th>\n",
       "      <th>insfg</th>\n",
       "      <th>iterm</th>\n",
       "      <th>locdt</th>\n",
       "      <th>mcc</th>\n",
       "      <th>mchno</th>\n",
       "      <th>ovrlt</th>\n",
       "      <th>scity</th>\n",
       "      <th>stocn</th>\n",
       "      <th>stscd</th>\n",
       "      <th>loctm_hour_of_day</th>\n",
       "      <th>loctm_minute_of_hour</th>\n",
       "      <th>loctm_second_of_min</th>\n",
       "      <th>cano_locdt_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.393835</td>\n",
       "      <td>0.070789</td>\n",
       "      <td>0.070789</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142137</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.324602</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.268908</td>\n",
       "      <td>0.538547</td>\n",
       "      <td>0.646630</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.196151</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.440678</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>38038_33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.191005</td>\n",
       "      <td>0.019713</td>\n",
       "      <td>0.019713</td>\n",
       "      <td>0.363024</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142137</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.038308</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.067227</td>\n",
       "      <td>0.233053</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.864407</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>45725_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.393835</td>\n",
       "      <td>0.015233</td>\n",
       "      <td>0.015233</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142137</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.324602</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.042017</td>\n",
       "      <td>0.538547</td>\n",
       "      <td>0.646630</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.196151</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.406780</td>\n",
       "      <td>0.983051</td>\n",
       "      <td>188328_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.794559</td>\n",
       "      <td>0.065412</td>\n",
       "      <td>0.050179</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.033613</td>\n",
       "      <td>0.719283</td>\n",
       "      <td>0.015788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.018220</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.491525</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>29967_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.497653</td>\n",
       "      <td>0.068996</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.002165</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.042017</td>\n",
       "      <td>0.568704</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.355932</td>\n",
       "      <td>0.491525</td>\n",
       "      <td>81305_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1521782</td>\n",
       "      <td>0.042507</td>\n",
       "      <td>0.008961</td>\n",
       "      <td>0.008961</td>\n",
       "      <td>0.002075</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010258</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.612633</td>\n",
       "      <td>0.005172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009036</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.025210</td>\n",
       "      <td>0.088207</td>\n",
       "      <td>0.026539</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>15189_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1521783</td>\n",
       "      <td>0.036421</td>\n",
       "      <td>0.051971</td>\n",
       "      <td>0.049283</td>\n",
       "      <td>0.004056</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010258</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.612633</td>\n",
       "      <td>0.005172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009036</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.100840</td>\n",
       "      <td>0.159732</td>\n",
       "      <td>0.095499</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.015205</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.389831</td>\n",
       "      <td>0.644068</td>\n",
       "      <td>116252_13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1521784</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.011649</td>\n",
       "      <td>0.011649</td>\n",
       "      <td>0.076722</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010258</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.612633</td>\n",
       "      <td>0.005172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009036</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.118164</td>\n",
       "      <td>0.083791</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>93598_29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1521785</td>\n",
       "      <td>0.245115</td>\n",
       "      <td>0.023297</td>\n",
       "      <td>0.023297</td>\n",
       "      <td>0.076722</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010258</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>0.005172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009036</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.193277</td>\n",
       "      <td>0.118164</td>\n",
       "      <td>0.083791</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>197460_24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1521786</td>\n",
       "      <td>0.794559</td>\n",
       "      <td>0.011649</td>\n",
       "      <td>0.011649</td>\n",
       "      <td>0.005489</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010258</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009036</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.100840</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001177</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.016947</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.610169</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>176440_13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1521787 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            acqic     bacno      cano     conam  contp     csmcu  ecfg  \\\n",
       "0        0.393835  0.070789  0.070789  1.000000    1.0  0.142137   1.0   \n",
       "1        0.191005  0.019713  0.019713  0.363024    1.0  0.142137   1.0   \n",
       "2        0.393835  0.015233  0.015233  1.000000    1.0  0.142137   1.0   \n",
       "3        0.794559  0.065412  0.050179  0.001142    1.0  1.000000   1.0   \n",
       "4        0.497653  0.068996  0.055556  0.002165    1.0  1.000000   1.0   \n",
       "...           ...       ...       ...       ...    ...       ...   ...   \n",
       "1521782  0.042507  0.008961  0.008961  0.002075    1.0  0.010258   0.0   \n",
       "1521783  0.036421  0.051971  0.049283  0.004056    1.0  0.010258   0.0   \n",
       "1521784  1.000000  0.011649  0.011649  0.076722    1.0  0.010258   0.0   \n",
       "1521785  0.245115  0.023297  0.023297  0.076722    1.0  0.010258   0.0   \n",
       "1521786  0.794559  0.011649  0.011649  0.005489    1.0  0.010258   1.0   \n",
       "\n",
       "            etymd     flbmk  flg_3dsmk  fraud_ind     hcefg  insfg  iterm  \\\n",
       "0        0.324602  1.000000        1.0          0  1.000000    1.0    1.0   \n",
       "1        0.955971  1.000000        1.0          0  0.038308    1.0    1.0   \n",
       "2        0.324602  1.000000        1.0          0  1.000000    1.0    1.0   \n",
       "3        1.000000  1.000000        1.0          0  1.000000    1.0    1.0   \n",
       "4        0.783407  1.000000        1.0          0  1.000000    1.0    1.0   \n",
       "...           ...       ...        ...        ...       ...    ...    ...   \n",
       "1521782  0.612633  0.005172        0.0          0  0.009036    1.0    1.0   \n",
       "1521783  0.612633  0.005172        0.0          0  0.009036    1.0    1.0   \n",
       "1521784  0.612633  0.005172        0.0          0  0.009036    1.0    1.0   \n",
       "1521785  0.955971  0.005172        0.0          0  0.009036    1.0    1.0   \n",
       "1521786  1.000000  0.005172        0.0          0  0.009036    1.0    1.0   \n",
       "\n",
       "            locdt       mcc     mchno  ovrlt     scity  stocn  stscd  \\\n",
       "0        0.268908  0.538547  0.646630    1.0  0.196151    1.0    1.0   \n",
       "1        0.067227  0.233053  1.000000    1.0  1.000000    1.0    1.0   \n",
       "2        0.042017  0.538547  0.646630    1.0  0.196151    1.0    1.0   \n",
       "3        0.033613  0.719283  0.015788    1.0  0.018220    1.0    1.0   \n",
       "4        0.042017  0.568704  0.000738    1.0  1.000000    1.0    1.0   \n",
       "...           ...       ...       ...    ...       ...    ...    ...   \n",
       "1521782  0.025210  0.088207  0.026539    1.0  1.000000    1.0    1.0   \n",
       "1521783  0.100840  0.159732  0.095499    1.0  0.015205    1.0    1.0   \n",
       "1521784  0.235294  0.118164  0.083791    1.0  1.000000    1.0    1.0   \n",
       "1521785  0.193277  0.118164  0.083791    1.0  1.000000    1.0    1.0   \n",
       "1521786  0.100840  1.000000  0.001177    1.0  0.016947    1.0    1.0   \n",
       "\n",
       "         loctm_hour_of_day  loctm_minute_of_hour  loctm_second_of_min  \\\n",
       "0                 0.739130              0.440678             0.881356   \n",
       "1                 0.434783              0.864407             0.237288   \n",
       "2                 0.652174              0.406780             0.983051   \n",
       "3                 0.739130              0.491525             0.779661   \n",
       "4                 0.782609              0.355932             0.491525   \n",
       "...                    ...                   ...                  ...   \n",
       "1521782           0.826087              0.271186             0.711864   \n",
       "1521783           0.434783              0.389831             0.644068   \n",
       "1521784           1.000000              0.779661             0.305085   \n",
       "1521785           0.913043              0.881356             0.305085   \n",
       "1521786           0.695652              0.610169             0.050847   \n",
       "\n",
       "        cano_locdt_index  \n",
       "0               38038_33  \n",
       "1                45725_9  \n",
       "2               188328_6  \n",
       "3                29967_5  \n",
       "4                81305_6  \n",
       "...                  ...  \n",
       "1521782          15189_4  \n",
       "1521783        116252_13  \n",
       "1521784         93598_29  \n",
       "1521785        197460_24  \n",
       "1521786        176440_13  \n",
       "\n",
       "[1521787 rows x 25 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把卡號中有fraud的拿掉\n",
    "fraud_cano_id = [int(i) for i in df_train[df_train.fraud_ind == 1].cano.unique().tolist()]\n",
    "fraud_cano_id = df_train[df_train.fraud_ind == 1].cano.unique().tolist()\n",
    "\n",
    "df_train_normal_cano_id = df_train[~df_train.cano.isin(fraud_cano_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34956, 25)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_normal_cano_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fraud_cano_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    34956\n",
       "Name: fraud_ind, dtype: int64"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_normal_cano_id.fraud_ind.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    34956\n",
       "Name: cano, dtype: int64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_normal_cano_id.cano.isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ldap_home/yunrui.li/.pyenv/versions/3.6.5/envs/deepts/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_train_normal_cano_id.sort_values(by = [\"cano\", \"locdt\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acqic</th>\n",
       "      <th>bacno</th>\n",
       "      <th>cano</th>\n",
       "      <th>conam</th>\n",
       "      <th>contp</th>\n",
       "      <th>csmcu</th>\n",
       "      <th>ecfg</th>\n",
       "      <th>etymd</th>\n",
       "      <th>flbmk</th>\n",
       "      <th>flg_3dsmk</th>\n",
       "      <th>fraud_ind</th>\n",
       "      <th>hcefg</th>\n",
       "      <th>insfg</th>\n",
       "      <th>iterm</th>\n",
       "      <th>locdt</th>\n",
       "      <th>mcc</th>\n",
       "      <th>mchno</th>\n",
       "      <th>ovrlt</th>\n",
       "      <th>scity</th>\n",
       "      <th>stocn</th>\n",
       "      <th>stscd</th>\n",
       "      <th>loctm_hour_of_day</th>\n",
       "      <th>loctm_minute_of_hour</th>\n",
       "      <th>loctm_second_of_min</th>\n",
       "      <th>cano_locdt_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.191005</td>\n",
       "      <td>0.149642</td>\n",
       "      <td>0.149642</td>\n",
       "      <td>0.363024</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142137</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.038308</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.050420</td>\n",
       "      <td>0.233053</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>185426_7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.191005</td>\n",
       "      <td>0.249104</td>\n",
       "      <td>0.248208</td>\n",
       "      <td>0.363024</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142137</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.038308</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.042017</td>\n",
       "      <td>0.233053</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>91061_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.191005</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.363024</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142137</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.038308</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.075630</td>\n",
       "      <td>0.233053</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>0.440678</td>\n",
       "      <td>71502_10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.191005</td>\n",
       "      <td>0.668459</td>\n",
       "      <td>0.668459</td>\n",
       "      <td>0.363024</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142137</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.038308</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.126050</td>\n",
       "      <td>0.233053</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.864407</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>188447_16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.393835</td>\n",
       "      <td>0.115591</td>\n",
       "      <td>0.106631</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142137</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.324602</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.100840</td>\n",
       "      <td>0.538547</td>\n",
       "      <td>0.037760</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.196151</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.355932</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>145699_13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1521651</td>\n",
       "      <td>0.794559</td>\n",
       "      <td>0.120072</td>\n",
       "      <td>0.120072</td>\n",
       "      <td>0.005750</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010258</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>0.005172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009036</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.193277</td>\n",
       "      <td>0.135549</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.009243</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>161412_24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1521691</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.076722</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010258</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.612633</td>\n",
       "      <td>0.005172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009036</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.067227</td>\n",
       "      <td>0.118164</td>\n",
       "      <td>0.083791</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983051</td>\n",
       "      <td>0.762712</td>\n",
       "      <td>156143_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1521700</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.407706</td>\n",
       "      <td>0.132616</td>\n",
       "      <td>0.076722</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010258</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.612633</td>\n",
       "      <td>0.005172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009036</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.118164</td>\n",
       "      <td>0.083791</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.118644</td>\n",
       "      <td>0.254237</td>\n",
       "      <td>116703_15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1521764</td>\n",
       "      <td>0.321774</td>\n",
       "      <td>0.175627</td>\n",
       "      <td>0.175627</td>\n",
       "      <td>0.007861</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010258</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.612633</td>\n",
       "      <td>0.005172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009036</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.201681</td>\n",
       "      <td>0.198603</td>\n",
       "      <td>0.051728</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.898305</td>\n",
       "      <td>10882_25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1521772</td>\n",
       "      <td>0.013570</td>\n",
       "      <td>0.120072</td>\n",
       "      <td>0.120072</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010258</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>0.005172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009036</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.126050</td>\n",
       "      <td>0.159732</td>\n",
       "      <td>0.060297</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.009134</td>\n",
       "      <td>0.015165</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.118644</td>\n",
       "      <td>0.355932</td>\n",
       "      <td>161412_16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34956 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            acqic     bacno      cano     conam  contp     csmcu  ecfg  \\\n",
       "53       0.191005  0.149642  0.149642  0.363024    1.0  0.142137   1.0   \n",
       "57       0.191005  0.249104  0.248208  0.363024    1.0  0.142137   1.0   \n",
       "62       0.191005  1.000000  1.000000  0.363024    1.0  0.142137   1.0   \n",
       "73       0.191005  0.668459  0.668459  0.363024    1.0  0.142137   1.0   \n",
       "124      0.393835  0.115591  0.106631  1.000000    1.0  0.142137   1.0   \n",
       "...           ...       ...       ...       ...    ...       ...   ...   \n",
       "1521651  0.794559  0.120072  0.120072  0.005750    1.0  0.010258   1.0   \n",
       "1521691  1.000000  0.111111  0.111111  0.076722    1.0  0.010258   0.0   \n",
       "1521700  1.000000  0.407706  0.132616  0.076722    1.0  0.010258   0.0   \n",
       "1521764  0.321774  0.175627  0.175627  0.007861    1.0  0.010258   0.0   \n",
       "1521772  0.013570  0.120072  0.120072  0.000000    1.0  0.010258   0.0   \n",
       "\n",
       "            etymd     flbmk  flg_3dsmk  fraud_ind     hcefg  insfg  iterm  \\\n",
       "53       0.955971  1.000000        1.0          0  0.038308    1.0    1.0   \n",
       "57       0.955971  1.000000        1.0          0  0.038308    1.0    1.0   \n",
       "62       0.955971  1.000000        1.0          0  0.038308    1.0    1.0   \n",
       "73       0.955971  1.000000        1.0          0  0.038308    1.0    1.0   \n",
       "124      0.324602  1.000000        1.0          0  1.000000    1.0    1.0   \n",
       "...           ...       ...        ...        ...       ...    ...    ...   \n",
       "1521651  0.783407  0.005172        0.0          0  0.009036    1.0    1.0   \n",
       "1521691  0.612633  0.005172        0.0          0  0.009036    1.0    1.0   \n",
       "1521700  0.612633  0.005172        0.0          0  0.009036    1.0    1.0   \n",
       "1521764  0.612633  0.005172        0.0          0  0.009036    1.0    1.0   \n",
       "1521772  0.955971  0.005172        0.0          0  0.009036    1.0    1.0   \n",
       "\n",
       "            locdt       mcc     mchno  ovrlt     scity     stocn  stscd  \\\n",
       "53       0.050420  0.233053  1.000000    1.0  1.000000  1.000000    1.0   \n",
       "57       0.042017  0.233053  1.000000    1.0  1.000000  1.000000    1.0   \n",
       "62       0.075630  0.233053  1.000000    1.0  1.000000  1.000000    1.0   \n",
       "73       0.126050  0.233053  1.000000    1.0  1.000000  1.000000    1.0   \n",
       "124      0.100840  0.538547  0.037760    1.0  0.196151  1.000000    1.0   \n",
       "...           ...       ...       ...    ...       ...       ...    ...   \n",
       "1521651  0.193277  0.135549  0.000063    1.0  0.009243  1.000000    1.0   \n",
       "1521691  0.067227  0.118164  0.083791    1.0  1.000000  1.000000    1.0   \n",
       "1521700  0.117647  0.118164  0.083791    1.0  1.000000  1.000000    1.0   \n",
       "1521764  0.201681  0.198603  0.051728    1.0  1.000000  1.000000    1.0   \n",
       "1521772  0.126050  0.159732  0.060297    1.0  0.009134  0.015165    1.0   \n",
       "\n",
       "         loctm_hour_of_day  loctm_minute_of_hour  loctm_second_of_min  \\\n",
       "53                0.434783              0.745763             0.000000   \n",
       "57                0.434783              0.847458             0.152542   \n",
       "62                0.434783              0.779661             0.440678   \n",
       "73                0.434783              0.864407             0.220339   \n",
       "124               0.652174              0.355932             0.050847   \n",
       "...                    ...                   ...                  ...   \n",
       "1521651           0.608696              1.000000             0.813559   \n",
       "1521691           1.000000              0.983051             0.762712   \n",
       "1521700           0.521739              0.118644             0.254237   \n",
       "1521764           0.565217              0.067797             0.898305   \n",
       "1521772           0.826087              0.118644             0.355932   \n",
       "\n",
       "        cano_locdt_index  \n",
       "53              185426_7  \n",
       "57               91061_6  \n",
       "62              71502_10  \n",
       "73             188447_16  \n",
       "124            145699_13  \n",
       "...                  ...  \n",
       "1521651        161412_24  \n",
       "1521691         156143_9  \n",
       "1521700        116703_15  \n",
       "1521764         10882_25  \n",
       "1521772        161412_16  \n",
       "\n",
       "[34956 rows x 25 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_normal_cano_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def partition_(df, num_features):\n",
    "    data = []\n",
    "    for i in range(len(df)):\n",
    "        out = None\n",
    "        if i == 0:\n",
    "            out = np.concatenate(((np.zeros((2,num_features))),df.iloc[:1].values))\n",
    "        elif i== 1:\n",
    "            out = np.concatenate(((np.zeros((1,num_features))),df.iloc[:i+1].values))\n",
    "        else:\n",
    "            out = df.iloc[i+1-3:i+1].values\n",
    "        data.append(out)\n",
    "    return data\n",
    "\n",
    "def partition(df_, sequence_length = 3):\n",
    "    feats = [f for f in df_.columns if f not in {\"fraud_ind\"}]\n",
    "    sequences = []\n",
    "    for _, df in df_[feats].groupby(by = \"cano\"):\n",
    "        data = partition_(df[feats], num_features = len(feats))\n",
    "        for d in data:\n",
    "            sequences.append(d)\n",
    "    return sequences\n",
    "\n",
    "df_train_sequences = partition(df_train_normal_cano_id.iloc[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate(df_train_sequences)\n",
    "X_train = pd.DataFrame(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08064516129032258\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# sequence_length = 3\n",
    "# feats = [f for f in df_train_normal_cano_id.columns if f not in {\"fraud_ind\"}]\n",
    "# sequences = []\n",
    "# for _, df in df_train_normal_cano_id[feats].groupby(by = \"cano\"):\n",
    "#     print (_)\n",
    "#     data = partition_(df[feats], num_features = len(feats))\n",
    "    \n",
    "#     for d in data:\n",
    "#         sequences.append(d)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1606"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4818, 24)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = np.concatenate(sequences)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.218003</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00778997</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.376934</td>\n",
       "      <td>0.0447597</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.508475</td>\n",
       "      <td>0.423729</td>\n",
       "      <td>3749_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.218003</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00778997</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.376934</td>\n",
       "      <td>0.0447597</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.508475</td>\n",
       "      <td>0.423729</td>\n",
       "      <td>3749_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.497653</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0263015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.568704</td>\n",
       "      <td>0.000345271</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>210775_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.218003</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00778997</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.376934</td>\n",
       "      <td>0.0447597</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.508475</td>\n",
       "      <td>0.423729</td>\n",
       "      <td>3749_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.497653</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0263015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.568704</td>\n",
       "      <td>0.000345271</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>210775_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.393835</td>\n",
       "      <td>0.0842294</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.152505</td>\n",
       "      <td>1</td>\n",
       "      <td>0.142137</td>\n",
       "      <td>1</td>\n",
       "      <td>0.324602</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538547</td>\n",
       "      <td>0.0111428</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196151</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.79661</td>\n",
       "      <td>0.423729</td>\n",
       "      <td>75818_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.497653</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0263015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.568704</td>\n",
       "      <td>0.000345271</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>210775_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.393835</td>\n",
       "      <td>0.0842294</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.152505</td>\n",
       "      <td>1</td>\n",
       "      <td>0.142137</td>\n",
       "      <td>1</td>\n",
       "      <td>0.324602</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538547</td>\n",
       "      <td>0.0111428</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196151</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.79661</td>\n",
       "      <td>0.423729</td>\n",
       "      <td>75818_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.321774</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0141944</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0742844</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0520966</td>\n",
       "      <td>0.00801971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.0508475</td>\n",
       "      <td>0</td>\n",
       "      <td>82304_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.393835</td>\n",
       "      <td>0.0842294</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.152505</td>\n",
       "      <td>1</td>\n",
       "      <td>0.142137</td>\n",
       "      <td>1</td>\n",
       "      <td>0.324602</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538547</td>\n",
       "      <td>0.0111428</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196151</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.79661</td>\n",
       "      <td>0.423729</td>\n",
       "      <td>75818_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.321774</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0141944</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0742844</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0520966</td>\n",
       "      <td>0.00801971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.0508475</td>\n",
       "      <td>0</td>\n",
       "      <td>82304_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.0377347</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0223768</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.719283</td>\n",
       "      <td>9.41649e-05</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.40678</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>183589_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.321774</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0141944</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0742844</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0520966</td>\n",
       "      <td>0.00801971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.0508475</td>\n",
       "      <td>0</td>\n",
       "      <td>82304_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.0377347</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0223768</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.719283</td>\n",
       "      <td>9.41649e-05</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.40678</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>183589_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.0268044</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>5.94654e-06</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0146078</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.719283</td>\n",
       "      <td>0.337911</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0225626</td>\n",
       "      <td>0.0142457</td>\n",
       "      <td>1</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.627119</td>\n",
       "      <td>170610_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.0377347</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0223768</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.719283</td>\n",
       "      <td>9.41649e-05</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.40678</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>183589_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.0268044</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>5.94654e-06</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0146078</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.719283</td>\n",
       "      <td>0.337911</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0225626</td>\n",
       "      <td>0.0142457</td>\n",
       "      <td>1</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.627119</td>\n",
       "      <td>170610_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0842294</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00244997</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.612633</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0126378</td>\n",
       "      <td>0.0609404</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>195913_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.0268044</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>5.94654e-06</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0146078</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.719283</td>\n",
       "      <td>0.337911</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0225626</td>\n",
       "      <td>0.0142457</td>\n",
       "      <td>1</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.627119</td>\n",
       "      <td>170610_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0842294</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00244997</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.612633</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0126378</td>\n",
       "      <td>0.0609404</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>195913_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.0268044</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0146078</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.719283</td>\n",
       "      <td>0.337911</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0225626</td>\n",
       "      <td>0.0142457</td>\n",
       "      <td>1</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>170610_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0842294</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00244997</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.612633</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0126378</td>\n",
       "      <td>0.0609404</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>195913_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.0268044</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0146078</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.719283</td>\n",
       "      <td>0.337911</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0225626</td>\n",
       "      <td>0.0142457</td>\n",
       "      <td>1</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>170610_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.0268044</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>5.94654e-06</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0146078</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.719283</td>\n",
       "      <td>0.337911</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0225626</td>\n",
       "      <td>0.0142457</td>\n",
       "      <td>1</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>170610_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.0268044</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0146078</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.719283</td>\n",
       "      <td>0.337911</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0225626</td>\n",
       "      <td>0.0142457</td>\n",
       "      <td>1</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>170610_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.0268044</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>5.94654e-06</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0146078</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.719283</td>\n",
       "      <td>0.337911</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0225626</td>\n",
       "      <td>0.0142457</td>\n",
       "      <td>1</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>170610_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.0161231</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0132727</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135549</td>\n",
       "      <td>0.00153803</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.474576</td>\n",
       "      <td>0.932203</td>\n",
       "      <td>82829_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.0268044</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>5.94654e-06</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0146078</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.719283</td>\n",
       "      <td>0.337911</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0225626</td>\n",
       "      <td>0.0142457</td>\n",
       "      <td>1</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.847458</td>\n",
       "      <td>170610_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.0161231</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0132727</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135549</td>\n",
       "      <td>0.00153803</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.474576</td>\n",
       "      <td>0.932203</td>\n",
       "      <td>82829_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.0268044</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0146078</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.719283</td>\n",
       "      <td>0.337911</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0225626</td>\n",
       "      <td>0.0142457</td>\n",
       "      <td>1</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.762712</td>\n",
       "      <td>170610_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.0161231</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0132727</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.135549</td>\n",
       "      <td>0.00153803</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.474576</td>\n",
       "      <td>0.932203</td>\n",
       "      <td>82829_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.0268044</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0146078</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.719283</td>\n",
       "      <td>0.337911</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0225626</td>\n",
       "      <td>0.0142457</td>\n",
       "      <td>1</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.762712</td>\n",
       "      <td>170610_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.177223</td>\n",
       "      <td>0.0842294</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00161746</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.612633</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.224434</td>\n",
       "      <td>0.73072</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0560761</td>\n",
       "      <td>0.0335168</td>\n",
       "      <td>1</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.186441</td>\n",
       "      <td>0.728814</td>\n",
       "      <td>195913_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.0268044</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0146078</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.719283</td>\n",
       "      <td>0.337911</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0225626</td>\n",
       "      <td>0.0142457</td>\n",
       "      <td>1</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.762712</td>\n",
       "      <td>170610_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.177223</td>\n",
       "      <td>0.0842294</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00161746</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.612633</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.224434</td>\n",
       "      <td>0.73072</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0560761</td>\n",
       "      <td>0.0335168</td>\n",
       "      <td>1</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.186441</td>\n",
       "      <td>0.728814</td>\n",
       "      <td>195913_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.0268044</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>5.94654e-06</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0146078</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.719283</td>\n",
       "      <td>0.337911</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0225626</td>\n",
       "      <td>0.0142457</td>\n",
       "      <td>1</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.610169</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>170610_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.177223</td>\n",
       "      <td>0.0842294</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00161746</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.612633</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.224434</td>\n",
       "      <td>0.73072</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0560761</td>\n",
       "      <td>0.0335168</td>\n",
       "      <td>1</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.186441</td>\n",
       "      <td>0.728814</td>\n",
       "      <td>195913_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.0268044</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>5.94654e-06</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0146078</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.719283</td>\n",
       "      <td>0.337911</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0225626</td>\n",
       "      <td>0.0142457</td>\n",
       "      <td>1</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.610169</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>170610_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.497653</td>\n",
       "      <td>0.101254</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00482264</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.568704</td>\n",
       "      <td>0.00260523</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.491525</td>\n",
       "      <td>0.661017</td>\n",
       "      <td>114592_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.0268044</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>5.94654e-06</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0146078</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.719283</td>\n",
       "      <td>0.337911</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0225626</td>\n",
       "      <td>0.0142457</td>\n",
       "      <td>1</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.610169</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>170610_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.497653</td>\n",
       "      <td>0.101254</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00482264</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.568704</td>\n",
       "      <td>0.00260523</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.491525</td>\n",
       "      <td>0.661017</td>\n",
       "      <td>114592_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.218003</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00602385</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.198603</td>\n",
       "      <td>0.0517279</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>0.0847458</td>\n",
       "      <td>82829_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.497653</td>\n",
       "      <td>0.101254</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00482264</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.568704</td>\n",
       "      <td>0.00260523</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.491525</td>\n",
       "      <td>0.661017</td>\n",
       "      <td>114592_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.218003</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00602385</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.198603</td>\n",
       "      <td>0.0517279</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>0.0847458</td>\n",
       "      <td>82829_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.497653</td>\n",
       "      <td>0.155914</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00372848</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.568704</td>\n",
       "      <td>0.0021501</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.288136</td>\n",
       "      <td>0.610169</td>\n",
       "      <td>63818_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.218003</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00602385</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.198603</td>\n",
       "      <td>0.0517279</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>0.0847458</td>\n",
       "      <td>82829_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.497653</td>\n",
       "      <td>0.155914</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00372848</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.568704</td>\n",
       "      <td>0.0021501</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.288136</td>\n",
       "      <td>0.610169</td>\n",
       "      <td>63818_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.0268044</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>1.18931e-05</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0146078</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.719283</td>\n",
       "      <td>0.337911</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0225626</td>\n",
       "      <td>0.0142457</td>\n",
       "      <td>1</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>170610_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.497653</td>\n",
       "      <td>0.155914</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00372848</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.568704</td>\n",
       "      <td>0.0021501</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.288136</td>\n",
       "      <td>0.610169</td>\n",
       "      <td>63818_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.0268044</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>1.18931e-05</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0146078</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.719283</td>\n",
       "      <td>0.337911</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0225626</td>\n",
       "      <td>0.0142457</td>\n",
       "      <td>1</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>170610_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0112271</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.0652618</td>\n",
       "      <td>0.000894567</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>183589_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.0268044</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>1.18931e-05</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0146078</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.719283</td>\n",
       "      <td>0.337911</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0225626</td>\n",
       "      <td>0.0142457</td>\n",
       "      <td>1</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>170610_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0112271</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.0652618</td>\n",
       "      <td>0.000894567</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>183589_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.119275</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00462046</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.0206677</td>\n",
       "      <td>0.000313883</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0444515</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.728814</td>\n",
       "      <td>0.542373</td>\n",
       "      <td>113661_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0112271</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.0652618</td>\n",
       "      <td>0.000894567</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>183589_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.119275</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00462046</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.0206677</td>\n",
       "      <td>0.000313883</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0444515</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.728814</td>\n",
       "      <td>0.542373</td>\n",
       "      <td>113661_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.497653</td>\n",
       "      <td>0.160394</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00211102</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.568704</td>\n",
       "      <td>0.00866317</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0</td>\n",
       "      <td>2894_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.119275</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00462046</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.0206677</td>\n",
       "      <td>0.000313883</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0444515</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.728814</td>\n",
       "      <td>0.542373</td>\n",
       "      <td>113661_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.497653</td>\n",
       "      <td>0.160394</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00211102</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.568704</td>\n",
       "      <td>0.00866317</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0</td>\n",
       "      <td>2894_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.497653</td>\n",
       "      <td>0.0842294</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00472155</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.719283</td>\n",
       "      <td>0.0180483</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.440678</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>195913_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.497653</td>\n",
       "      <td>0.160394</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00211102</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.568704</td>\n",
       "      <td>0.00866317</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.881356</td>\n",
       "      <td>0</td>\n",
       "      <td>2894_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.497653</td>\n",
       "      <td>0.0842294</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00472155</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.719283</td>\n",
       "      <td>0.0180483</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.440678</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>195913_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.794559</td>\n",
       "      <td>0.0842294</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00264621</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.612633</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.179415</td>\n",
       "      <td>0.0827396</td>\n",
       "      <td>1</td>\n",
       "      <td>0.11814</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>0.0338983</td>\n",
       "      <td>75818_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.497653</td>\n",
       "      <td>0.0842294</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00472155</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.719283</td>\n",
       "      <td>0.0180483</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.440678</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>195913_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.794559</td>\n",
       "      <td>0.0842294</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00264621</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.612633</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.179415</td>\n",
       "      <td>0.0827396</td>\n",
       "      <td>1</td>\n",
       "      <td>0.11814</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>0.0338983</td>\n",
       "      <td>75818_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.218003</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00982369</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.198603</td>\n",
       "      <td>0.0517279</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>0.0338983</td>\n",
       "      <td>82829_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.794559</td>\n",
       "      <td>0.0842294</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00264621</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.612633</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.179415</td>\n",
       "      <td>0.0827396</td>\n",
       "      <td>1</td>\n",
       "      <td>0.11814</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>0.0338983</td>\n",
       "      <td>75818_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.218003</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00982369</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.198603</td>\n",
       "      <td>0.0517279</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>0.0338983</td>\n",
       "      <td>82829_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.497653</td>\n",
       "      <td>0.101254</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00199209</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.568704</td>\n",
       "      <td>0.00210302</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.508475</td>\n",
       "      <td>0.610169</td>\n",
       "      <td>114592_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.218003</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00982369</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.198603</td>\n",
       "      <td>0.0517279</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>0.0338983</td>\n",
       "      <td>82829_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.497653</td>\n",
       "      <td>0.101254</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00199209</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.568704</td>\n",
       "      <td>0.00210302</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.508475</td>\n",
       "      <td>0.610169</td>\n",
       "      <td>114592_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.0268044</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>6.54119e-05</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0146078</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.719283</td>\n",
       "      <td>0.337911</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0225626</td>\n",
       "      <td>0.0142457</td>\n",
       "      <td>1</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>0.542373</td>\n",
       "      <td>170610_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.497653</td>\n",
       "      <td>0.101254</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00199209</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.568704</td>\n",
       "      <td>0.00210302</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.508475</td>\n",
       "      <td>0.610169</td>\n",
       "      <td>114592_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.0268044</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>6.54119e-05</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0146078</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.719283</td>\n",
       "      <td>0.337911</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0225626</td>\n",
       "      <td>0.0142457</td>\n",
       "      <td>1</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>0.542373</td>\n",
       "      <td>170610_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.497653</td>\n",
       "      <td>0.0842294</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00400797</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.376934</td>\n",
       "      <td>0.00668571</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.644068</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>195913_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.0268044</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>6.54119e-05</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0146078</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.719283</td>\n",
       "      <td>0.337911</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0225626</td>\n",
       "      <td>0.0142457</td>\n",
       "      <td>1</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.220339</td>\n",
       "      <td>0.542373</td>\n",
       "      <td>170610_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.497653</td>\n",
       "      <td>0.0842294</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00400797</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.376934</td>\n",
       "      <td>0.00668571</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.644068</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>195913_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.0425072</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.003782</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.376934</td>\n",
       "      <td>0.0010829</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0444515</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.118644</td>\n",
       "      <td>0.118644</td>\n",
       "      <td>113661_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.497653</td>\n",
       "      <td>0.0842294</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00400797</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.376934</td>\n",
       "      <td>0.00668571</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.644068</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>195913_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.0425072</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.003782</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.376934</td>\n",
       "      <td>0.0010829</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0444515</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.118644</td>\n",
       "      <td>0.118644</td>\n",
       "      <td>113661_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.0425072</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0108643</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.719283</td>\n",
       "      <td>0.0143131</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0444515</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.559322</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>113661_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.0425072</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.003782</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.376934</td>\n",
       "      <td>0.0010829</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0444515</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.118644</td>\n",
       "      <td>0.118644</td>\n",
       "      <td>113661_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.0425072</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0108643</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.719283</td>\n",
       "      <td>0.0143131</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0444515</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.559322</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>113661_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.218003</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00570273</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.198603</td>\n",
       "      <td>0.0517279</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.0677966</td>\n",
       "      <td>0.762712</td>\n",
       "      <td>82829_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.0425072</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0108643</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.719283</td>\n",
       "      <td>0.0143131</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0444515</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.559322</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>113661_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.218003</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00570273</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.198603</td>\n",
       "      <td>0.0517279</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.0677966</td>\n",
       "      <td>0.762712</td>\n",
       "      <td>82829_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.245115</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0242738</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.376934</td>\n",
       "      <td>0.000156942</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0269556</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>119102_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.218003</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00570273</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.198603</td>\n",
       "      <td>0.0517279</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.0677966</td>\n",
       "      <td>0.762712</td>\n",
       "      <td>82829_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.245115</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0242738</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.376934</td>\n",
       "      <td>0.000156942</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0269556</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>119102_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.0425072</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00333601</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.719283</td>\n",
       "      <td>0.0143131</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0444515</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.288136</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>113661_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.245115</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0242738</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.376934</td>\n",
       "      <td>0.000156942</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0269556</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>119102_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.0425072</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00333601</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.719283</td>\n",
       "      <td>0.0143131</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0444515</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.288136</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>113661_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.0806124</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00350251</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.198603</td>\n",
       "      <td>0.000674849</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.728814</td>\n",
       "      <td>0.0508475</td>\n",
       "      <td>82829_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.0425072</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00333601</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.719283</td>\n",
       "      <td>0.0143131</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0444515</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.288136</td>\n",
       "      <td>0.338983</td>\n",
       "      <td>113661_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.0806124</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00350251</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.198603</td>\n",
       "      <td>0.000674849</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.728814</td>\n",
       "      <td>0.0508475</td>\n",
       "      <td>82829_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.0377347</td>\n",
       "      <td>0.155914</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00431124</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.376934</td>\n",
       "      <td>0.000439436</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.305085</td>\n",
       "      <td>0.20339</td>\n",
       "      <td>63818_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.0806124</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.0806452</td>\n",
       "      <td>0.00350251</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00840336</td>\n",
       "      <td>0.198603</td>\n",
       "      <td>0.000674849</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.728814</td>\n",
       "      <td>0.0508475</td>\n",
       "      <td>82829_2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0          1          2            3  4          5  6          7  \\\n",
       "0           0          0          0            0  0          0  0          0   \n",
       "1           0          0          0            0  0          0  0          0   \n",
       "2    0.218003  0.0806452  0.0806452   0.00778997  1          1  0   0.955971   \n",
       "3           0          0          0            0  0          0  0          0   \n",
       "4    0.218003  0.0806452  0.0806452   0.00778997  1          1  0   0.955971   \n",
       "5    0.497653  0.0806452  0.0806452    0.0263015  1          1  1   0.783407   \n",
       "6    0.218003  0.0806452  0.0806452   0.00778997  1          1  0   0.955971   \n",
       "7    0.497653  0.0806452  0.0806452    0.0263015  1          1  1   0.783407   \n",
       "8    0.393835  0.0842294  0.0806452     0.152505  1   0.142137  1   0.324602   \n",
       "9    0.497653  0.0806452  0.0806452    0.0263015  1          1  1   0.783407   \n",
       "10   0.393835  0.0842294  0.0806452     0.152505  1   0.142137  1   0.324602   \n",
       "11   0.321774  0.0806452  0.0806452    0.0141944  1          1  1  0.0742844   \n",
       "12   0.393835  0.0842294  0.0806452     0.152505  1   0.142137  1   0.324602   \n",
       "13   0.321774  0.0806452  0.0806452    0.0141944  1          1  1  0.0742844   \n",
       "14  0.0377347  0.0806452  0.0806452    0.0223768  1          1  1   0.783407   \n",
       "15   0.321774  0.0806452  0.0806452    0.0141944  1          1  1  0.0742844   \n",
       "16  0.0377347  0.0806452  0.0806452    0.0223768  1          1  1   0.783407   \n",
       "17  0.0268044  0.0806452  0.0806452  5.94654e-06  1  0.0146078  0   0.955971   \n",
       "18  0.0377347  0.0806452  0.0806452    0.0223768  1          1  1   0.783407   \n",
       "19  0.0268044  0.0806452  0.0806452  5.94654e-06  1  0.0146078  0   0.955971   \n",
       "20          1  0.0842294  0.0806452   0.00244997  1          1  0   0.612633   \n",
       "21  0.0268044  0.0806452  0.0806452  5.94654e-06  1  0.0146078  0   0.955971   \n",
       "22          1  0.0842294  0.0806452   0.00244997  1          1  0   0.612633   \n",
       "23  0.0268044  0.0806452  0.0806452            0  1  0.0146078  0   0.955971   \n",
       "24          1  0.0842294  0.0806452   0.00244997  1          1  0   0.612633   \n",
       "25  0.0268044  0.0806452  0.0806452            0  1  0.0146078  0   0.955971   \n",
       "26  0.0268044  0.0806452  0.0806452  5.94654e-06  1  0.0146078  0   0.955971   \n",
       "27  0.0268044  0.0806452  0.0806452            0  1  0.0146078  0   0.955971   \n",
       "28  0.0268044  0.0806452  0.0806452  5.94654e-06  1  0.0146078  0   0.955971   \n",
       "29  0.0161231  0.0806452  0.0806452    0.0132727  1          1  1          1   \n",
       "30  0.0268044  0.0806452  0.0806452  5.94654e-06  1  0.0146078  0   0.955971   \n",
       "31  0.0161231  0.0806452  0.0806452    0.0132727  1          1  1          1   \n",
       "32  0.0268044  0.0806452  0.0806452            0  1  0.0146078  0   0.955971   \n",
       "33  0.0161231  0.0806452  0.0806452    0.0132727  1          1  1          1   \n",
       "34  0.0268044  0.0806452  0.0806452            0  1  0.0146078  0   0.955971   \n",
       "35   0.177223  0.0842294  0.0806452   0.00161746  1          1  0   0.612633   \n",
       "36  0.0268044  0.0806452  0.0806452            0  1  0.0146078  0   0.955971   \n",
       "37   0.177223  0.0842294  0.0806452   0.00161746  1          1  0   0.612633   \n",
       "38  0.0268044  0.0806452  0.0806452  5.94654e-06  1  0.0146078  0   0.955971   \n",
       "39   0.177223  0.0842294  0.0806452   0.00161746  1          1  0   0.612633   \n",
       "40  0.0268044  0.0806452  0.0806452  5.94654e-06  1  0.0146078  0   0.955971   \n",
       "41   0.497653   0.101254  0.0806452   0.00482264  1          1  1          1   \n",
       "42  0.0268044  0.0806452  0.0806452  5.94654e-06  1  0.0146078  0   0.955971   \n",
       "43   0.497653   0.101254  0.0806452   0.00482264  1          1  1          1   \n",
       "44   0.218003  0.0806452  0.0806452   0.00602385  1          1  0   0.955971   \n",
       "45   0.497653   0.101254  0.0806452   0.00482264  1          1  1          1   \n",
       "46   0.218003  0.0806452  0.0806452   0.00602385  1          1  0   0.955971   \n",
       "47   0.497653   0.155914  0.0806452   0.00372848  1          1  1   0.783407   \n",
       "48   0.218003  0.0806452  0.0806452   0.00602385  1          1  0   0.955971   \n",
       "49   0.497653   0.155914  0.0806452   0.00372848  1          1  1   0.783407   \n",
       "50  0.0268044  0.0806452  0.0806452  1.18931e-05  1  0.0146078  0   0.955971   \n",
       "51   0.497653   0.155914  0.0806452   0.00372848  1          1  1   0.783407   \n",
       "52  0.0268044  0.0806452  0.0806452  1.18931e-05  1  0.0146078  0   0.955971   \n",
       "53          1  0.0806452  0.0806452    0.0112271  1          1  1          1   \n",
       "54  0.0268044  0.0806452  0.0806452  1.18931e-05  1  0.0146078  0   0.955971   \n",
       "55          1  0.0806452  0.0806452    0.0112271  1          1  1          1   \n",
       "56   0.119275  0.0806452  0.0806452   0.00462046  1          1  1          1   \n",
       "57          1  0.0806452  0.0806452    0.0112271  1          1  1          1   \n",
       "58   0.119275  0.0806452  0.0806452   0.00462046  1          1  1          1   \n",
       "59   0.497653   0.160394  0.0806452   0.00211102  1          1  1   0.783407   \n",
       "60   0.119275  0.0806452  0.0806452   0.00462046  1          1  1          1   \n",
       "61   0.497653   0.160394  0.0806452   0.00211102  1          1  1   0.783407   \n",
       "62   0.497653  0.0842294  0.0806452   0.00472155  1          1  1          1   \n",
       "63   0.497653   0.160394  0.0806452   0.00211102  1          1  1   0.783407   \n",
       "64   0.497653  0.0842294  0.0806452   0.00472155  1          1  1          1   \n",
       "65   0.794559  0.0842294  0.0806452   0.00264621  1          1  0   0.612633   \n",
       "66   0.497653  0.0842294  0.0806452   0.00472155  1          1  1          1   \n",
       "67   0.794559  0.0842294  0.0806452   0.00264621  1          1  0   0.612633   \n",
       "68   0.218003  0.0806452  0.0806452   0.00982369  1          1  0   0.955971   \n",
       "69   0.794559  0.0842294  0.0806452   0.00264621  1          1  0   0.612633   \n",
       "70   0.218003  0.0806452  0.0806452   0.00982369  1          1  0   0.955971   \n",
       "71   0.497653   0.101254  0.0806452   0.00199209  1          1  1          1   \n",
       "72   0.218003  0.0806452  0.0806452   0.00982369  1          1  0   0.955971   \n",
       "73   0.497653   0.101254  0.0806452   0.00199209  1          1  1          1   \n",
       "74  0.0268044  0.0806452  0.0806452  6.54119e-05  1  0.0146078  0   0.955971   \n",
       "75   0.497653   0.101254  0.0806452   0.00199209  1          1  1          1   \n",
       "76  0.0268044  0.0806452  0.0806452  6.54119e-05  1  0.0146078  0   0.955971   \n",
       "77   0.497653  0.0842294  0.0806452   0.00400797  1          1  1          1   \n",
       "78  0.0268044  0.0806452  0.0806452  6.54119e-05  1  0.0146078  0   0.955971   \n",
       "79   0.497653  0.0842294  0.0806452   0.00400797  1          1  1          1   \n",
       "80  0.0425072  0.0806452  0.0806452     0.003782  1          1  1   0.783407   \n",
       "81   0.497653  0.0842294  0.0806452   0.00400797  1          1  1          1   \n",
       "82  0.0425072  0.0806452  0.0806452     0.003782  1          1  1   0.783407   \n",
       "83  0.0425072  0.0806452  0.0806452    0.0108643  1          1  1          1   \n",
       "84  0.0425072  0.0806452  0.0806452     0.003782  1          1  1   0.783407   \n",
       "85  0.0425072  0.0806452  0.0806452    0.0108643  1          1  1          1   \n",
       "86   0.218003  0.0806452  0.0806452   0.00570273  1          1  0   0.955971   \n",
       "87  0.0425072  0.0806452  0.0806452    0.0108643  1          1  1          1   \n",
       "88   0.218003  0.0806452  0.0806452   0.00570273  1          1  0   0.955971   \n",
       "89   0.245115  0.0806452  0.0806452    0.0242738  1          1  1   0.783407   \n",
       "90   0.218003  0.0806452  0.0806452   0.00570273  1          1  0   0.955971   \n",
       "91   0.245115  0.0806452  0.0806452    0.0242738  1          1  1   0.783407   \n",
       "92  0.0425072  0.0806452  0.0806452   0.00333601  1          1  1   0.783407   \n",
       "93   0.245115  0.0806452  0.0806452    0.0242738  1          1  1   0.783407   \n",
       "94  0.0425072  0.0806452  0.0806452   0.00333601  1          1  1   0.783407   \n",
       "95  0.0806124  0.0806452  0.0806452   0.00350251  1          1  1          1   \n",
       "96  0.0425072  0.0806452  0.0806452   0.00333601  1          1  1   0.783407   \n",
       "97  0.0806124  0.0806452  0.0806452   0.00350251  1          1  1          1   \n",
       "98  0.0377347   0.155914  0.0806452   0.00431124  1          1  1          1   \n",
       "99  0.0806124  0.0806452  0.0806452   0.00350251  1          1  1          1   \n",
       "\n",
       "    8  9 10 11 12          13         14           15 16         17  \\\n",
       "0   0  0  0  0  0           0          0            0  0          0   \n",
       "1   0  0  0  0  0           0          0            0  0          0   \n",
       "2   1  1  1  1  1           0   0.376934    0.0447597  1          1   \n",
       "3   0  0  0  0  0           0          0            0  0          0   \n",
       "4   1  1  1  1  1           0   0.376934    0.0447597  1          1   \n",
       "5   1  1  1  1  1           0   0.568704  0.000345271  1          1   \n",
       "6   1  1  1  1  1           0   0.376934    0.0447597  1          1   \n",
       "7   1  1  1  1  1           0   0.568704  0.000345271  1          1   \n",
       "8   1  1  1  1  1           0   0.538547    0.0111428  1   0.196151   \n",
       "9   1  1  1  1  1           0   0.568704  0.000345271  1          1   \n",
       "10  1  1  1  1  1           0   0.538547    0.0111428  1   0.196151   \n",
       "11  0  1  1  1  1           0  0.0520966   0.00801971  1          1   \n",
       "12  1  1  1  1  1           0   0.538547    0.0111428  1   0.196151   \n",
       "13  0  1  1  1  1           0  0.0520966   0.00801971  1          1   \n",
       "14  1  1  1  1  1           0   0.719283  9.41649e-05  1          1   \n",
       "15  0  1  1  1  1           0  0.0520966   0.00801971  1          1   \n",
       "16  1  1  1  1  1           0   0.719283  9.41649e-05  1          1   \n",
       "17  1  1  1  1  1           0   0.719283     0.337911  1  0.0225626   \n",
       "18  1  1  1  1  1           0   0.719283  9.41649e-05  1          1   \n",
       "19  1  1  1  1  1           0   0.719283     0.337911  1  0.0225626   \n",
       "20  1  1  1  1  1           0  0.0126378    0.0609404  1          1   \n",
       "21  1  1  1  1  1           0   0.719283     0.337911  1  0.0225626   \n",
       "22  1  1  1  1  1           0  0.0126378    0.0609404  1          1   \n",
       "23  1  1  1  1  1           0   0.719283     0.337911  1  0.0225626   \n",
       "24  1  1  1  1  1           0  0.0126378    0.0609404  1          1   \n",
       "25  1  1  1  1  1           0   0.719283     0.337911  1  0.0225626   \n",
       "26  1  1  1  1  1           0   0.719283     0.337911  1  0.0225626   \n",
       "27  1  1  1  1  1           0   0.719283     0.337911  1  0.0225626   \n",
       "28  1  1  1  1  1           0   0.719283     0.337911  1  0.0225626   \n",
       "29  1  1  1  1  1           0   0.135549   0.00153803  1          1   \n",
       "30  1  1  1  1  1           0   0.719283     0.337911  1  0.0225626   \n",
       "31  1  1  1  1  1           0   0.135549   0.00153803  1          1   \n",
       "32  1  1  1  1  1           0   0.719283     0.337911  1  0.0225626   \n",
       "33  1  1  1  1  1           0   0.135549   0.00153803  1          1   \n",
       "34  1  1  1  1  1           0   0.719283     0.337911  1  0.0225626   \n",
       "35  1  1  1  1  1           0   0.224434      0.73072  1  0.0560761   \n",
       "36  1  1  1  1  1           0   0.719283     0.337911  1  0.0225626   \n",
       "37  1  1  1  1  1           0   0.224434      0.73072  1  0.0560761   \n",
       "38  1  1  1  1  1           0   0.719283     0.337911  1  0.0225626   \n",
       "39  1  1  1  1  1           0   0.224434      0.73072  1  0.0560761   \n",
       "40  1  1  1  1  1           0   0.719283     0.337911  1  0.0225626   \n",
       "41  1  1  1  1  1           0   0.568704   0.00260523  1          1   \n",
       "42  1  1  1  1  1           0   0.719283     0.337911  1  0.0225626   \n",
       "43  1  1  1  1  1           0   0.568704   0.00260523  1          1   \n",
       "44  1  1  1  1  1           0   0.198603    0.0517279  1          1   \n",
       "45  1  1  1  1  1           0   0.568704   0.00260523  1          1   \n",
       "46  1  1  1  1  1           0   0.198603    0.0517279  1          1   \n",
       "47  1  1  1  1  1           0   0.568704    0.0021501  1          1   \n",
       "48  1  1  1  1  1           0   0.198603    0.0517279  1          1   \n",
       "49  1  1  1  1  1           0   0.568704    0.0021501  1          1   \n",
       "50  1  1  1  1  1           0   0.719283     0.337911  1  0.0225626   \n",
       "51  1  1  1  1  1           0   0.568704    0.0021501  1          1   \n",
       "52  1  1  1  1  1           0   0.719283     0.337911  1  0.0225626   \n",
       "53  1  1  1  1  1  0.00840336  0.0652618  0.000894567  1          1   \n",
       "54  1  1  1  1  1           0   0.719283     0.337911  1  0.0225626   \n",
       "55  1  1  1  1  1  0.00840336  0.0652618  0.000894567  1          1   \n",
       "56  1  1  1  1  1  0.00840336  0.0206677  0.000313883  1  0.0444515   \n",
       "57  1  1  1  1  1  0.00840336  0.0652618  0.000894567  1          1   \n",
       "58  1  1  1  1  1  0.00840336  0.0206677  0.000313883  1  0.0444515   \n",
       "59  1  1  1  1  1  0.00840336   0.568704   0.00866317  1          1   \n",
       "60  1  1  1  1  1  0.00840336  0.0206677  0.000313883  1  0.0444515   \n",
       "61  1  1  1  1  1  0.00840336   0.568704   0.00866317  1          1   \n",
       "62  1  1  1  1  1  0.00840336   0.719283    0.0180483  1          1   \n",
       "63  1  1  1  1  1  0.00840336   0.568704   0.00866317  1          1   \n",
       "64  1  1  1  1  1  0.00840336   0.719283    0.0180483  1          1   \n",
       "65  1  1  1  1  1  0.00840336   0.179415    0.0827396  1    0.11814   \n",
       "66  1  1  1  1  1  0.00840336   0.719283    0.0180483  1          1   \n",
       "67  1  1  1  1  1  0.00840336   0.179415    0.0827396  1    0.11814   \n",
       "68  1  1  1  1  1  0.00840336   0.198603    0.0517279  1          1   \n",
       "69  1  1  1  1  1  0.00840336   0.179415    0.0827396  1    0.11814   \n",
       "70  1  1  1  1  1  0.00840336   0.198603    0.0517279  1          1   \n",
       "71  1  1  1  1  1  0.00840336   0.568704   0.00210302  1          1   \n",
       "72  1  1  1  1  1  0.00840336   0.198603    0.0517279  1          1   \n",
       "73  1  1  1  1  1  0.00840336   0.568704   0.00210302  1          1   \n",
       "74  1  1  1  1  1  0.00840336   0.719283     0.337911  1  0.0225626   \n",
       "75  1  1  1  1  1  0.00840336   0.568704   0.00210302  1          1   \n",
       "76  1  1  1  1  1  0.00840336   0.719283     0.337911  1  0.0225626   \n",
       "77  1  1  1  1  1  0.00840336   0.376934   0.00668571  1          1   \n",
       "78  1  1  1  1  1  0.00840336   0.719283     0.337911  1  0.0225626   \n",
       "79  1  1  1  1  1  0.00840336   0.376934   0.00668571  1          1   \n",
       "80  1  1  1  1  1  0.00840336   0.376934    0.0010829  1  0.0444515   \n",
       "81  1  1  1  1  1  0.00840336   0.376934   0.00668571  1          1   \n",
       "82  1  1  1  1  1  0.00840336   0.376934    0.0010829  1  0.0444515   \n",
       "83  1  1  1  1  1  0.00840336   0.719283    0.0143131  1  0.0444515   \n",
       "84  1  1  1  1  1  0.00840336   0.376934    0.0010829  1  0.0444515   \n",
       "85  1  1  1  1  1  0.00840336   0.719283    0.0143131  1  0.0444515   \n",
       "86  1  1  1  1  1  0.00840336   0.198603    0.0517279  1          1   \n",
       "87  1  1  1  1  1  0.00840336   0.719283    0.0143131  1  0.0444515   \n",
       "88  1  1  1  1  1  0.00840336   0.198603    0.0517279  1          1   \n",
       "89  1  1  1  1  1  0.00840336   0.376934  0.000156942  1  0.0269556   \n",
       "90  1  1  1  1  1  0.00840336   0.198603    0.0517279  1          1   \n",
       "91  1  1  1  1  1  0.00840336   0.376934  0.000156942  1  0.0269556   \n",
       "92  1  1  1  1  1  0.00840336   0.719283    0.0143131  1  0.0444515   \n",
       "93  1  1  1  1  1  0.00840336   0.376934  0.000156942  1  0.0269556   \n",
       "94  1  1  1  1  1  0.00840336   0.719283    0.0143131  1  0.0444515   \n",
       "95  1  1  1  1  1  0.00840336   0.198603  0.000674849  1          1   \n",
       "96  1  1  1  1  1  0.00840336   0.719283    0.0143131  1  0.0444515   \n",
       "97  1  1  1  1  1  0.00840336   0.198603  0.000674849  1          1   \n",
       "98  1  1  1  1  1  0.00840336   0.376934  0.000439436  1          1   \n",
       "99  1  1  1  1  1  0.00840336   0.198603  0.000674849  1          1   \n",
       "\n",
       "           18 19        20         21         22        23  \n",
       "0           0  0         0          0          0         0  \n",
       "1           0  0         0          0          0         0  \n",
       "2           1  1  0.869565   0.508475   0.423729    3749_1  \n",
       "3           0  0         0          0          0         0  \n",
       "4           1  1  0.869565   0.508475   0.423729    3749_1  \n",
       "5           1  1  0.304348   0.338983   0.677966  210775_1  \n",
       "6           1  1  0.869565   0.508475   0.423729    3749_1  \n",
       "7           1  1  0.304348   0.338983   0.677966  210775_1  \n",
       "8           1  1  0.434783    0.79661   0.423729   75818_1  \n",
       "9           1  1  0.304348   0.338983   0.677966  210775_1  \n",
       "10          1  1  0.434783    0.79661   0.423729   75818_1  \n",
       "11          1  1  0.608696  0.0508475          0   82304_1  \n",
       "12          1  1  0.434783    0.79661   0.423729   75818_1  \n",
       "13          1  1  0.608696  0.0508475          0   82304_1  \n",
       "14          1  1  0.869565    0.40678   0.237288  183589_1  \n",
       "15          1  1  0.608696  0.0508475          0   82304_1  \n",
       "16          1  1  0.869565    0.40678   0.237288  183589_1  \n",
       "17  0.0142457  1  0.608696   0.271186   0.627119  170610_1  \n",
       "18          1  1  0.869565    0.40678   0.237288  183589_1  \n",
       "19  0.0142457  1  0.608696   0.271186   0.627119  170610_1  \n",
       "20          1  1  0.608696   0.101695   0.237288  195913_1  \n",
       "21  0.0142457  1  0.608696   0.271186   0.627119  170610_1  \n",
       "22          1  1  0.608696   0.101695   0.237288  195913_1  \n",
       "23  0.0142457  1  0.956522   0.915254   0.237288  170610_1  \n",
       "24          1  1  0.608696   0.101695   0.237288  195913_1  \n",
       "25  0.0142457  1  0.956522   0.915254   0.237288  170610_1  \n",
       "26  0.0142457  1  0.608696   0.135593   0.847458  170610_1  \n",
       "27  0.0142457  1  0.956522   0.915254   0.237288  170610_1  \n",
       "28  0.0142457  1  0.608696   0.135593   0.847458  170610_1  \n",
       "29          1  1  0.695652   0.474576   0.932203   82829_1  \n",
       "30  0.0142457  1  0.608696   0.135593   0.847458  170610_1  \n",
       "31          1  1  0.695652   0.474576   0.932203   82829_1  \n",
       "32  0.0142457  1  0.434783   0.169492   0.762712  170610_1  \n",
       "33          1  1  0.695652   0.474576   0.932203   82829_1  \n",
       "34  0.0142457  1  0.434783   0.169492   0.762712  170610_1  \n",
       "35  0.0335168  1  0.826087   0.186441   0.728814  195913_1  \n",
       "36  0.0142457  1  0.434783   0.169492   0.762712  170610_1  \n",
       "37  0.0335168  1  0.826087   0.186441   0.728814  195913_1  \n",
       "38  0.0142457  1  0.652174   0.610169   0.372881  170610_1  \n",
       "39  0.0335168  1  0.826087   0.186441   0.728814  195913_1  \n",
       "40  0.0142457  1  0.652174   0.610169   0.372881  170610_1  \n",
       "41          1  1  0.304348   0.491525   0.661017  114592_1  \n",
       "42  0.0142457  1  0.652174   0.610169   0.372881  170610_1  \n",
       "43          1  1  0.304348   0.491525   0.661017  114592_1  \n",
       "44          1  1  0.521739   0.338983  0.0847458   82829_1  \n",
       "45          1  1  0.304348   0.491525   0.661017  114592_1  \n",
       "46          1  1  0.521739   0.338983  0.0847458   82829_1  \n",
       "47          1  1  0.304348   0.288136   0.610169   63818_1  \n",
       "48          1  1  0.521739   0.338983  0.0847458   82829_1  \n",
       "49          1  1  0.304348   0.288136   0.610169   63818_1  \n",
       "50  0.0142457  1  0.391304   0.915254   0.711864  170610_1  \n",
       "51          1  1  0.304348   0.288136   0.610169   63818_1  \n",
       "52  0.0142457  1  0.391304   0.915254   0.711864  170610_1  \n",
       "53          1  1  0.521739   0.711864   0.677966  183589_2  \n",
       "54  0.0142457  1  0.391304   0.915254   0.711864  170610_1  \n",
       "55          1  1  0.521739   0.711864   0.677966  183589_2  \n",
       "56          1  1  0.826087   0.728814   0.542373  113661_2  \n",
       "57          1  1  0.521739   0.711864   0.677966  183589_2  \n",
       "58          1  1  0.826087   0.728814   0.542373  113661_2  \n",
       "59          1  1  0.347826   0.881356          0    2894_2  \n",
       "60          1  1  0.826087   0.728814   0.542373  113661_2  \n",
       "61          1  1  0.347826   0.881356          0    2894_2  \n",
       "62          1  1  0.869565   0.440678   0.813559  195913_2  \n",
       "63          1  1  0.347826   0.881356          0    2894_2  \n",
       "64          1  1  0.869565   0.440678   0.813559  195913_2  \n",
       "65          1  1         1   0.338983  0.0338983   75818_2  \n",
       "66          1  1  0.869565   0.440678   0.813559  195913_2  \n",
       "67          1  1         1   0.338983  0.0338983   75818_2  \n",
       "68          1  1  0.956522   0.711864  0.0338983   82829_2  \n",
       "69          1  1         1   0.338983  0.0338983   75818_2  \n",
       "70          1  1  0.956522   0.711864  0.0338983   82829_2  \n",
       "71          1  1  0.826087   0.508475   0.610169  114592_2  \n",
       "72          1  1  0.956522   0.711864  0.0338983   82829_2  \n",
       "73          1  1  0.826087   0.508475   0.610169  114592_2  \n",
       "74  0.0142457  1  0.695652   0.220339   0.542373  170610_2  \n",
       "75          1  1  0.826087   0.508475   0.610169  114592_2  \n",
       "76  0.0142457  1  0.695652   0.220339   0.542373  170610_2  \n",
       "77          1  1  0.869565   0.644068   0.135593  195913_2  \n",
       "78  0.0142457  1  0.695652   0.220339   0.542373  170610_2  \n",
       "79          1  1  0.869565   0.644068   0.135593  195913_2  \n",
       "80          1  1  0.521739   0.118644   0.118644  113661_2  \n",
       "81          1  1  0.869565   0.644068   0.135593  195913_2  \n",
       "82          1  1  0.521739   0.118644   0.118644  113661_2  \n",
       "83          1  1  0.565217   0.559322   0.152542  113661_2  \n",
       "84          1  1  0.521739   0.118644   0.118644  113661_2  \n",
       "85          1  1  0.565217   0.559322   0.152542  113661_2  \n",
       "86          1  1  0.869565  0.0677966   0.762712   82829_2  \n",
       "87          1  1  0.565217   0.559322   0.152542  113661_2  \n",
       "88          1  1  0.869565  0.0677966   0.762712   82829_2  \n",
       "89          1  1  0.782609   0.271186   0.915254  119102_2  \n",
       "90          1  1  0.869565  0.0677966   0.762712   82829_2  \n",
       "91          1  1  0.782609   0.271186   0.915254  119102_2  \n",
       "92          1  1  0.565217   0.288136   0.338983  113661_2  \n",
       "93          1  1  0.782609   0.271186   0.915254  119102_2  \n",
       "94          1  1  0.565217   0.288136   0.338983  113661_2  \n",
       "95          1  1  0.565217   0.728814  0.0508475   82829_2  \n",
       "96          1  1  0.565217   0.288136   0.338983  113661_2  \n",
       "97          1  1  0.565217   0.728814  0.0508475   82829_2  \n",
       "98          1  1  0.565217   0.305085    0.20339   63818_2  \n",
       "99          1  1  0.565217   0.728814  0.0508475   82829_2  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_rows = 100\n",
    "\n",
    "X_train.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ldap_home/yunrui.li/.pyenv/versions/3.6.5/envs/deepts/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/ldap_home/yunrui.li/.pyenv/versions/3.6.5/envs/deepts/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/ldap_home/yunrui.li/.pyenv/versions/3.6.5/envs/deepts/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/ldap_home/yunrui.li/.pyenv/versions/3.6.5/envs/deepts/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/ldap_home/yunrui.li/.pyenv/versions/3.6.5/envs/deepts/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/ldap_home/yunrui.li/.pyenv/versions/3.6.5/envs/deepts/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import trange\n",
    "import sys\n",
    "sys.path.append(\"../DeepADoTS/src/algorithms/\")\n",
    "# from .algorithm_utils import Algorithm, PyTorchUtils\n",
    "# from .autoencoder import AutoEncoderModule\n",
    "#from lstm_enc_dec_axl import LSTMEDModule\n",
    "import abc\n",
    "import logging\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class Algorithm(metaclass=abc.ABCMeta):\n",
    "    def __init__(self, module_name, name, seed, details=False):\n",
    "        self.logger = logging.getLogger(module_name)\n",
    "        self.name = name\n",
    "        self.seed = seed\n",
    "        self.details = details\n",
    "        self.prediction_details = {}\n",
    "\n",
    "        if self.seed is not None:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Train the algorithm on the given dataset\n",
    "        \"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        :return anomaly score\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "class PyTorchUtils(metaclass=abc.ABCMeta):\n",
    "    def __init__(self, seed, gpu):\n",
    "        self.gpu = gpu\n",
    "        self.seed = seed\n",
    "        if self.seed is not None:\n",
    "            torch.manual_seed(self.seed)\n",
    "            torch.cuda.manual_seed(self.seed)\n",
    "        self.framework = 0\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return torch.device(f'cuda:{self.gpu}' if torch.cuda.is_available() and self.gpu is not None else 'cpu')\n",
    "\n",
    "    def to_var(self, t, **kwargs):\n",
    "        # ToDo: check whether cuda Variable.\n",
    "        t = t.to(self.device)\n",
    "        return Variable(t, **kwargs)\n",
    "\n",
    "    def to_device(self, model):\n",
    "        model.to(self.device)\n",
    "\n",
    "\n",
    "class TensorflowUtils(metaclass=abc.ABCMeta):\n",
    "    def __init__(self, seed, gpu):\n",
    "        self.gpu = gpu\n",
    "        self.seed = seed\n",
    "        if self.seed is not None:\n",
    "            tf.set_random_seed(seed)\n",
    "        self.framework = 1\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        local_device_protos = device_lib.list_local_devices()\n",
    "        gpus = [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "        return tf.device(gpus[self.gpu] if gpus and self.gpu is not None else '/cpu:0')\n",
    "    \n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.stats import multivariate_normal\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "class AutoEncoder(Algorithm, PyTorchUtils):\n",
    "    def __init__(self, name: str='AutoEncoder', num_epochs: int=10, batch_size: int=20, lr: float=1e-3,\n",
    "                 hidden_size: int=5, sequence_length: int=30, train_gaussian_percentage: float=0.25,\n",
    "                 seed: int=None, gpu: int=None, details=True):\n",
    "        Algorithm.__init__(self, __name__, name, seed, details=details)\n",
    "        PyTorchUtils.__init__(self, seed, gpu)\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.train_gaussian_percentage = train_gaussian_percentage\n",
    "\n",
    "        self.aed = None\n",
    "        self.mean, self.cov = None, None\n",
    "\n",
    "    def fit(self, X: pd.DataFrame):\n",
    "        X.interpolate(inplace=True)\n",
    "        X.bfill(inplace=True)\n",
    "        data = X.values\n",
    "        sequences = [data[i:i + self.sequence_length] for i in range(data.shape[0] - self.sequence_length + 1)]\n",
    "        indices = np.random.permutation(len(sequences))\n",
    "        split_point = int(self.train_gaussian_percentage * len(sequences))\n",
    "        train_loader = DataLoader(dataset=sequences, batch_size=self.batch_size, drop_last=True,\n",
    "                                  sampler=SubsetRandomSampler(indices[:-split_point]), pin_memory=True)\n",
    "        train_gaussian_loader = DataLoader(dataset=sequences, batch_size=self.batch_size, drop_last=True,\n",
    "                                           sampler=SubsetRandomSampler(indices[-split_point:]), pin_memory=True)\n",
    "\n",
    "        self.aed = AutoEncoderModule(X.shape[1], self.sequence_length, self.hidden_size, seed=self.seed, gpu=self.gpu)\n",
    "        self.to_device(self.aed)  # .double()\n",
    "        optimizer = torch.optim.Adam(self.aed.parameters(), lr=self.lr)\n",
    "\n",
    "        self.aed.train()\n",
    "        for epoch in trange(self.num_epochs):\n",
    "            logging.debug(f'Epoch {epoch+1}/{self.num_epochs}.')\n",
    "            for ts_batch in train_loader:\n",
    "                output = self.aed(self.to_var(ts_batch))\n",
    "                loss = nn.MSELoss(size_average=False)(output, self.to_var(ts_batch.float()))\n",
    "                self.aed.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        self.aed.eval()\n",
    "        error_vectors = []\n",
    "        for ts_batch in train_gaussian_loader:\n",
    "            output = self.aed(self.to_var(ts_batch))\n",
    "            error = nn.L1Loss(reduce=False)(output, self.to_var(ts_batch.float()))\n",
    "            error_vectors += list(error.view(-1, X.shape[1]).data.cpu().numpy())\n",
    "\n",
    "        self.mean = np.mean(error_vectors, axis=0)\n",
    "        self.cov = np.cov(error_vectors, rowvar=False)\n",
    "\n",
    "    def predict(self, X: pd.DataFrame) -> np.array:\n",
    "        X.interpolate(inplace=True)\n",
    "        X.bfill(inplace=True)\n",
    "        data = X.values\n",
    "        sequences = [data[i:i + self.sequence_length] for i in range(data.shape[0] - self.sequence_length + 1)]\n",
    "        data_loader = DataLoader(dataset=sequences, batch_size=self.batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "        self.aed.eval()\n",
    "        mvnormal = multivariate_normal(self.mean, self.cov, allow_singular=True)\n",
    "        scores = []\n",
    "        outputs = []\n",
    "        errors = []\n",
    "        for idx, ts in enumerate(data_loader):\n",
    "            output = self.aed(self.to_var(ts))\n",
    "            error = nn.L1Loss(reduce=False)(output, self.to_var(ts.float()))\n",
    "            score = -mvnormal.logpdf(error.view(-1, X.shape[1]).data.cpu().numpy())\n",
    "            scores.append(score.reshape(ts.size(0), self.sequence_length))\n",
    "            if self.details:\n",
    "                outputs.append(output.data.numpy())\n",
    "                errors.append(error.data.numpy())\n",
    "\n",
    "        # stores seq_len-many scores per timestamp and averages them\n",
    "        scores = np.concatenate(scores)\n",
    "        lattice = np.full((self.sequence_length, X.shape[0]), np.nan)\n",
    "        for i, score in enumerate(scores):\n",
    "            lattice[i % self.sequence_length, i:i + self.sequence_length] = score\n",
    "        scores = np.nanmean(lattice, axis=0)\n",
    "\n",
    "        if self.details:\n",
    "            outputs = np.concatenate(outputs)\n",
    "            lattice = np.full((self.sequence_length, X.shape[0], X.shape[1]), np.nan)\n",
    "            for i, output in enumerate(outputs):\n",
    "                lattice[i % self.sequence_length, i:i + self.sequence_length, :] = output\n",
    "            self.prediction_details.update({'reconstructions_mean': np.nanmean(lattice, axis=0).T})\n",
    "\n",
    "            errors = np.concatenate(errors)\n",
    "            lattice = np.full((self.sequence_length, X.shape[0], X.shape[1]), np.nan)\n",
    "            for i, error in enumerate(errors):\n",
    "                lattice[i % self.sequence_length, i:i + self.sequence_length, :] = error\n",
    "            self.prediction_details.update({'errors_mean': np.nanmean(lattice, axis=0).T})\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "class AutoEncoderModule(nn.Module, PyTorchUtils):\n",
    "    def __init__(self, n_features: int, sequence_length: int, hidden_size: int, seed: int, gpu: int):\n",
    "        # Each point is a flattened window and thus has as many features as sequence_length * features\n",
    "        super().__init__()\n",
    "        PyTorchUtils.__init__(self, seed, gpu)\n",
    "        input_length = n_features * sequence_length\n",
    "\n",
    "        # creates powers of two between eight and the next smaller power from the input_length\n",
    "        dec_steps = 2 ** np.arange(max(np.ceil(np.log2(hidden_size)), 2), np.log2(input_length))[1:]\n",
    "        dec_setup = np.concatenate([[hidden_size], dec_steps.repeat(2), [input_length]])\n",
    "        enc_setup = dec_setup[::-1]\n",
    "\n",
    "        layers = np.array([[nn.Linear(int(a), int(b)), nn.Tanh()] for a, b in enc_setup.reshape(-1, 2)]).flatten()[:-1]\n",
    "        self._encoder = nn.Sequential(*layers)\n",
    "        self.to_device(self._encoder)\n",
    "\n",
    "        layers = np.array([[nn.Linear(int(a), int(b)), nn.Tanh()] for a, b in dec_setup.reshape(-1, 2)]).flatten()[:-1]\n",
    "        self._decoder = nn.Sequential(*layers)\n",
    "        self.to_device(self._decoder)\n",
    "\n",
    "    def forward(self, ts_batch, return_latent: bool=False):\n",
    "        flattened_sequence = ts_batch.view(ts_batch.size(0), -1)\n",
    "        enc = self._encoder(flattened_sequence.float())\n",
    "        dec = self._decoder(enc)\n",
    "        reconstructed_sequence = dec.view(ts_batch.size())\n",
    "        return (reconstructed_sequence, enc) if return_latent else reconstructed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.stats import multivariate_normal\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from tqdm import trange\n",
    "\n",
    "# from .algorithm_utils import Algorithm, PyTorchUtils\n",
    "\n",
    "\n",
    "class LSTMED(Algorithm, PyTorchUtils):\n",
    "    def __init__(self, name: str='LSTM-ED', num_epochs: int=10, batch_size: int=20, lr: float=1e-3,\n",
    "                 hidden_size: int=5, sequence_length: int=30, train_gaussian_percentage: float=0.25,\n",
    "                 n_layers: tuple=(1, 1), use_bias: tuple=(True, True), dropout: tuple=(0, 0),\n",
    "                 seed: int=None, gpu: int = None, details=True):\n",
    "        Algorithm.__init__(self, __name__, name, seed, details=details)\n",
    "        PyTorchUtils.__init__(self, seed, gpu)\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.train_gaussian_percentage = train_gaussian_percentage\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.use_bias = use_bias\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.lstmed = None\n",
    "        self.mean, self.cov = None, None\n",
    "\n",
    "    def fit(self, X: pd.DataFrame):\n",
    "        X.interpolate(inplace=True)\n",
    "        X.bfill(inplace=True)\n",
    "        data = X.values\n",
    "        sequences = [data[i:i + self.sequence_length] for i in range(data.shape[0] - self.sequence_length + 1)]\n",
    "        indices = np.random.permutation(len(sequences))\n",
    "        split_point = int(self.train_gaussian_percentage * len(sequences))\n",
    "        train_loader = DataLoader(dataset=sequences, batch_size=self.batch_size, drop_last=True,\n",
    "                                  sampler=SubsetRandomSampler(indices[:-split_point]), pin_memory=True)\n",
    "        train_gaussian_loader = DataLoader(dataset=sequences, batch_size=self.batch_size, drop_last=True,\n",
    "                                           sampler=SubsetRandomSampler(indices[-split_point:]), pin_memory=True)\n",
    "\n",
    "        self.lstmed = LSTMEDModule(X.shape[1], self.hidden_size,\n",
    "                                   self.n_layers, self.use_bias, self.dropout,\n",
    "                                   seed=self.seed, gpu=self.gpu)\n",
    "        self.to_device(self.lstmed)\n",
    "        optimizer = torch.optim.Adam(self.lstmed.parameters(), lr=self.lr)\n",
    "\n",
    "        self.lstmed.train()\n",
    "        for epoch in trange(self.num_epochs):\n",
    "            logging.debug(f'Epoch {epoch+1}/{self.num_epochs}.')\n",
    "            for ts_batch in train_loader:\n",
    "                output = self.lstmed(self.to_var(ts_batch))\n",
    "                loss = nn.MSELoss(size_average=False)(output, self.to_var(ts_batch.float()))\n",
    "                self.lstmed.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        self.lstmed.eval()\n",
    "        error_vectors = []\n",
    "        for ts_batch in train_gaussian_loader:\n",
    "            output = self.lstmed(self.to_var(ts_batch))\n",
    "            error = nn.L1Loss(reduce=False)(output, self.to_var(ts_batch.float()))\n",
    "            error_vectors += list(error.view(-1, X.shape[1]).data.cpu().numpy())\n",
    "\n",
    "        self.mean = np.mean(error_vectors, axis=0)\n",
    "        self.cov = np.cov(error_vectors, rowvar=False)\n",
    "\n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        X.interpolate(inplace=True)\n",
    "        X.bfill(inplace=True)\n",
    "        data = X.values\n",
    "        sequences = [data[i:i + self.sequence_length] for i in range(data.shape[0] - self.sequence_length + 1)]\n",
    "        data_loader = DataLoader(dataset=sequences, batch_size=self.batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "        self.lstmed.eval()\n",
    "        mvnormal = multivariate_normal(self.mean, self.cov, allow_singular=True)\n",
    "        scores = []\n",
    "        outputs = []\n",
    "        errors = []\n",
    "        for idx, ts in enumerate(data_loader):\n",
    "            output = self.lstmed(self.to_var(ts))\n",
    "            error = nn.L1Loss(reduce=False)(output, self.to_var(ts.float()))\n",
    "            score = -mvnormal.logpdf(error.view(-1, X.shape[1]).data.cpu().numpy())\n",
    "            scores.append(score.reshape(ts.size(0), self.sequence_length))\n",
    "            if self.details:\n",
    "                outputs.append(output.data.numpy())\n",
    "                errors.append(error.data.numpy())\n",
    "\n",
    "        # stores seq_len-many scores per timestamp and averages them\n",
    "        scores = np.concatenate(scores)\n",
    "        lattice = np.full((self.sequence_length, data.shape[0]), np.nan)\n",
    "        for i, score in enumerate(scores):\n",
    "            lattice[i % self.sequence_length, i:i + self.sequence_length] = score\n",
    "        scores = np.nanmean(lattice, axis=0)\n",
    "\n",
    "        if self.details:\n",
    "            outputs = np.concatenate(outputs)\n",
    "            lattice = np.full((self.sequence_length, X.shape[0], X.shape[1]), np.nan)\n",
    "            for i, output in enumerate(outputs):\n",
    "                lattice[i % self.sequence_length, i:i + self.sequence_length, :] = output\n",
    "            self.prediction_details.update({'reconstructions_mean': np.nanmean(lattice, axis=0).T})\n",
    "\n",
    "            errors = np.concatenate(errors)\n",
    "            lattice = np.full((self.sequence_length, X.shape[0], X.shape[1]), np.nan)\n",
    "            for i, error in enumerate(errors):\n",
    "                lattice[i % self.sequence_length, i:i + self.sequence_length, :] = error\n",
    "            self.prediction_details.update({'errors_mean': np.nanmean(lattice, axis=0).T})\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "class LSTMEDModule(nn.Module, PyTorchUtils):\n",
    "    def __init__(self, n_features: int, hidden_size: int,\n",
    "                 n_layers: tuple, use_bias: tuple, dropout: tuple,\n",
    "                 seed: int, gpu: int):\n",
    "        super().__init__()\n",
    "        PyTorchUtils.__init__(self, seed, gpu)\n",
    "        self.n_features = n_features\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.use_bias = use_bias\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.encoder = nn.LSTM(self.n_features, self.hidden_size, batch_first=True,\n",
    "                               num_layers=self.n_layers[0], bias=self.use_bias[0], dropout=self.dropout[0])\n",
    "        self.to_device(self.encoder)\n",
    "        self.decoder = nn.LSTM(self.n_features, self.hidden_size, batch_first=True,\n",
    "                               num_layers=self.n_layers[1], bias=self.use_bias[1], dropout=self.dropout[1])\n",
    "        self.to_device(self.decoder)\n",
    "        self.hidden2output = nn.Linear(self.hidden_size, self.n_features)\n",
    "        self.to_device(self.hidden2output)\n",
    "\n",
    "    def _init_hidden(self, batch_size):\n",
    "        return (self.to_var(torch.Tensor(self.n_layers[0], batch_size, self.hidden_size).zero_()),\n",
    "                self.to_var(torch.Tensor(self.n_layers[0], batch_size, self.hidden_size).zero_()))\n",
    "\n",
    "    def forward(self, ts_batch, return_latent: bool=False):\n",
    "        batch_size = ts_batch.shape[0]\n",
    "\n",
    "        # 1. Encode the timeseries to make use of the last hidden state.\n",
    "        enc_hidden = self._init_hidden(batch_size)  # initialization with zero\n",
    "        _, enc_hidden = self.encoder(ts_batch.float(), enc_hidden)  # .float() here or .double() for the model\n",
    "\n",
    "        # 2. Use hidden state as initialization for our Decoder-LSTM\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        # 3. Also, use this hidden state to get the first output aka the last point of the reconstructed timeseries\n",
    "        # 4. Reconstruct timeseries backwards\n",
    "        #    * Use true data for training decoder\n",
    "        #    * Use hidden2output for prediction\n",
    "        output = self.to_var(torch.Tensor(ts_batch.size()).zero_())\n",
    "        for i in reversed(range(ts_batch.shape[1])):\n",
    "            output[:, i, :] = self.hidden2output(dec_hidden[0][0, :])\n",
    "\n",
    "            if self.training:\n",
    "                _, dec_hidden = self.decoder(ts_batch[:, i].unsqueeze(1).float(), dec_hidden)\n",
    "            else:\n",
    "                _, dec_hidden = self.decoder(output[:, i].unsqueeze(1), dec_hidden)\n",
    "\n",
    "        return (output, enc_hidden[1][-1]) if return_latent else output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Adapted from Daniel Stanley Tan (https://github.com/danieltan07/dagmm)\"\"\"\n",
    "import logging\n",
    "import sys\n",
    "sys.path.append(\"../DeepADoTS/src/algorithms/\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import trange\n",
    "\n",
    "# from .algorithm_utils import Algorithm, PyTorchUtils\n",
    "# from .autoencoder import AutoEncoderModule\n",
    "#from lstm_enc_dec_axl import LSTMEDModule\n",
    "\n",
    "\n",
    "class DAGMM(Algorithm, PyTorchUtils):\n",
    "    class AutoEncoder:\n",
    "        NN = AutoEncoderModule\n",
    "        LSTM = LSTMEDModule\n",
    "\n",
    "    def __init__(self, num_epochs=10, lambda_energy=0.1, lambda_cov_diag=0.005, lr=1e-3, batch_size=50, gmm_k=3,\n",
    "                 normal_percentile=80, sequence_length=30, autoencoder_type=AutoEncoderModule, autoencoder_args=None,\n",
    "                 hidden_size: int=5, seed: int=None, gpu: int=None, details=True):\n",
    "        _name = 'LSTM-DAGMM' if autoencoder_type == LSTMEDModule else 'DAGMM'\n",
    "        Algorithm.__init__(self, __name__, _name, seed, details=details)\n",
    "        PyTorchUtils.__init__(self, seed, gpu)\n",
    "        self.num_epochs = num_epochs\n",
    "        self.lambda_energy = lambda_energy\n",
    "        self.lambda_cov_diag = lambda_cov_diag\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.gmm_k = gmm_k  # Number of Gaussian mixtures\n",
    "        self.normal_percentile = normal_percentile  # Up to which percentile data should be considered normal\n",
    "        self.autoencoder_type = autoencoder_type\n",
    "        if autoencoder_type == AutoEncoderModule:\n",
    "            self.autoencoder_args = ({'sequence_length': self.sequence_length})\n",
    "        elif autoencoder_type == LSTMEDModule:\n",
    "            self.autoencoder_args = ({'n_layers': (1, 1), 'use_bias': (True, True), 'dropout': (0.0, 0.0)})\n",
    "        self.autoencoder_args.update({'seed': seed, 'gpu': gpu})\n",
    "        if autoencoder_args is not None:\n",
    "            self.autoencoder_args.update(autoencoder_args)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.dagmm, self.optimizer, self.train_energy, self._threshold = None, None, None, None\n",
    "\n",
    "    def reset_grad(self):\n",
    "        self.dagmm.zero_grad()\n",
    "\n",
    "    def dagmm_step(self, input_data):\n",
    "        self.dagmm.train()\n",
    "        enc, dec, z, gamma = self.dagmm(input_data)\n",
    "        total_loss, sample_energy, recon_error, cov_diag = self.dagmm.loss_function(input_data, dec, z, gamma,\n",
    "                                                                                    self.lambda_energy,\n",
    "                                                                                    self.lambda_cov_diag)\n",
    "        self.reset_grad()\n",
    "        total_loss = torch.clamp(total_loss, max=1e7)  # Extremely high loss can cause NaN gradients\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.dagmm.parameters(), 5)\n",
    "        # if np.array([np.isnan(p.grad.detach().numpy()).any() for p in self.dagmm.parameters()]).any():\n",
    "        #     import IPython; IPython.embed()\n",
    "        self.optimizer.step()\n",
    "        return total_loss, sample_energy, recon_error, cov_diag\n",
    "\n",
    "    def fit(self, X: pd.DataFrame):\n",
    "        \"\"\"Learn the mixture probability, mean and covariance for each component k.\n",
    "        Store the computed energy based on the training data and the aforementioned parameters.\"\"\"\n",
    "        #X.interpolate(inplace=True)\n",
    "        X.bfill(inplace=True)\n",
    "        data = X.values\n",
    "        sequences = [data[i:i + self.sequence_length] for i in range(X.shape[0] - self.sequence_length + 1)]\n",
    "        data_loader = DataLoader(dataset=sequences, batch_size=self.batch_size, shuffle=True, drop_last=True)\n",
    "        self.hidden_size = 5 + int(X.shape[1] / 20)\n",
    "        autoencoder = self.autoencoder_type(X.shape[1], hidden_size=self.hidden_size, **self.autoencoder_args)\n",
    "        self.dagmm = DAGMMModule(autoencoder, n_gmm=self.gmm_k, latent_dim=self.hidden_size + 2,\n",
    "                                 seed=self.seed, gpu=self.gpu)\n",
    "        self.to_device(self.dagmm)\n",
    "        self.optimizer = torch.optim.Adam(self.dagmm.parameters(), lr=self.lr)\n",
    "\n",
    "        for _ in trange(self.num_epochs):\n",
    "            for input_data in data_loader:\n",
    "                input_data = self.to_var(input_data)\n",
    "                self.dagmm_step(input_data.float())\n",
    "\n",
    "        self.dagmm.eval()\n",
    "        n = 0\n",
    "        mu_sum = 0\n",
    "        cov_sum = 0\n",
    "        gamma_sum = 0\n",
    "        for input_data in data_loader:\n",
    "            input_data = self.to_var(input_data)\n",
    "            _, _, z, gamma = self.dagmm(input_data.float())\n",
    "            phi, mu, cov = self.dagmm.compute_gmm_params(z, gamma)\n",
    "\n",
    "            batch_gamma_sum = torch.sum(gamma, dim=0)\n",
    "\n",
    "            gamma_sum += batch_gamma_sum\n",
    "            mu_sum += mu * batch_gamma_sum.unsqueeze(-1)  # keep sums of the numerator only\n",
    "            cov_sum += cov * batch_gamma_sum.unsqueeze(-1).unsqueeze(-1)  # keep sums of the numerator only\n",
    "\n",
    "            n += input_data.size(0)\n",
    "\n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        \"\"\"Using the learned mixture probability, mean and covariance for each component k, compute the energy on the\n",
    "        given data.\"\"\"\n",
    "        self.dagmm.eval()\n",
    "        #X.interpolate(inplace=True)\n",
    "        X.bfill(inplace=True)\n",
    "        data = X.values\n",
    "        sequences = [data[i:i + self.sequence_length] for i in range(len(data) - self.sequence_length + 1)]\n",
    "        data_loader = DataLoader(dataset=sequences, batch_size=1, shuffle=False)\n",
    "        test_energy = np.full((self.sequence_length, X.shape[0]), np.nan)\n",
    "\n",
    "        encodings = np.full((self.sequence_length, X.shape[0], self.hidden_size), np.nan)\n",
    "        decodings = np.full((self.sequence_length, X.shape[0], X.shape[1]), np.nan)\n",
    "        euc_errors = np.full((self.sequence_length, X.shape[0]), np.nan)\n",
    "        csn_errors = np.full((self.sequence_length, X.shape[0]), np.nan)\n",
    "\n",
    "        for i, sequence in enumerate(data_loader):\n",
    "            print (\"shape of sequence\",self.to_var(sequence).float().shape)\n",
    "            enc, dec, z, gamma = self.dagmm(self.to_var(sequence).float())\n",
    "            sample_energy, _ = self.dagmm.compute_energy(z, size_average=False)\n",
    "            idx = (i % self.sequence_length, np.arange(i, i + self.sequence_length))\n",
    "            test_energy[idx] = sample_energy.data.numpy()\n",
    "\n",
    "            if self.details:\n",
    "                encodings[idx] = enc.data.numpy()\n",
    "                decodings[idx] = dec.data.numpy()\n",
    "                euc_errors[idx] = z[:, 1].data.numpy()\n",
    "                csn_errors[idx] = z[:, 2].data.numpy()\n",
    "\n",
    "        test_energy = np.nanmean(test_energy, axis=0)\n",
    "\n",
    "        if self.details:\n",
    "            self.prediction_details.update({'latent_representations': np.nanmean(encodings, axis=0).T})\n",
    "            self.prediction_details.update({'reconstructions_mean': np.nanmean(decodings, axis=0).T})\n",
    "            self.prediction_details.update({'euclidean_errors_mean': np.nanmean(euc_errors, axis=0)})\n",
    "            self.prediction_details.update({'cosine_errors_mean': np.nanmean(csn_errors, axis=0)})\n",
    "\n",
    "        return test_energy\n",
    "\n",
    "\n",
    "class DAGMMModule(nn.Module, PyTorchUtils):\n",
    "    \"\"\"Residual Block.\"\"\"\n",
    "\n",
    "    def __init__(self, autoencoder, n_gmm, latent_dim, seed: int, gpu: int):\n",
    "        super(DAGMMModule, self).__init__()\n",
    "        PyTorchUtils.__init__(self, seed, gpu)\n",
    "\n",
    "        self.add_module('autoencoder', autoencoder)\n",
    "\n",
    "        layers = [\n",
    "            nn.Linear(latent_dim, 10),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(10, n_gmm),\n",
    "            nn.Softmax(dim=1)\n",
    "        ]\n",
    "        self.estimation = nn.Sequential(*layers)\n",
    "        self.to_device(self.estimation)\n",
    "\n",
    "        self.register_buffer('phi', self.to_var(torch.zeros(n_gmm)))\n",
    "        self.register_buffer('mu', self.to_var(torch.zeros(n_gmm, latent_dim)))\n",
    "        self.register_buffer('cov', self.to_var(torch.zeros(n_gmm, latent_dim, latent_dim)))\n",
    "\n",
    "    def relative_euclidean_distance(self, a, b, dim=1):\n",
    "        return (a - b).norm(2, dim=dim) / torch.clamp(a.norm(2, dim=dim), min=1e-10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        dec, enc = self.autoencoder(x, return_latent=True)\n",
    "\n",
    "        rec_cosine = F.cosine_similarity(x.view(x.shape[0], -1), dec.view(dec.shape[0], -1), dim=1)\n",
    "        rec_euclidean = self.relative_euclidean_distance(x.view(x.shape[0], -1), dec.view(dec.shape[0], -1), dim=1)\n",
    "\n",
    "        # Concatenate latent representation, cosine similarity and relative Euclidean distance between x and dec(enc(x))\n",
    "        z = torch.cat([enc, rec_euclidean.unsqueeze(-1), rec_cosine.unsqueeze(-1)], dim=1)\n",
    "        gamma = self.estimation(z)\n",
    "\n",
    "        return enc, dec, z, gamma\n",
    "\n",
    "    def compute_gmm_params(self, z, gamma):\n",
    "        N = gamma.size(0)\n",
    "        # K\n",
    "        sum_gamma = torch.sum(gamma, dim=0)\n",
    "\n",
    "        # K\n",
    "        phi = (sum_gamma / N)\n",
    "\n",
    "        self.phi = phi.data\n",
    "\n",
    "        # K x D\n",
    "        mu = torch.sum(gamma.unsqueeze(-1) * z.unsqueeze(1), dim=0) / sum_gamma.unsqueeze(-1)\n",
    "        self.mu = mu.data\n",
    "        # z = N x D\n",
    "        # mu = K x D\n",
    "        # gamma N x K\n",
    "\n",
    "        # z_mu = N x K x D\n",
    "        z_mu = (z.unsqueeze(1) - mu.unsqueeze(0))\n",
    "\n",
    "        # z_mu_outer = N x K x D x D\n",
    "        z_mu_outer = z_mu.unsqueeze(-1) * z_mu.unsqueeze(-2)\n",
    "\n",
    "        # K x D x D\n",
    "        cov = torch.sum(gamma.unsqueeze(-1).unsqueeze(-1) * z_mu_outer, dim=0) / sum_gamma.unsqueeze(-1).unsqueeze(-1)\n",
    "        self.cov = cov.data\n",
    "\n",
    "        return phi, mu, cov\n",
    "\n",
    "    def compute_energy(self, z, phi=None, mu=None, cov=None, size_average=True):\n",
    "        if phi is None:\n",
    "            phi = Variable(self.phi)\n",
    "        if mu is None:\n",
    "            mu = Variable(self.mu)\n",
    "        if cov is None:\n",
    "            cov = Variable(self.cov)\n",
    "\n",
    "        k, d, _ = cov.size()\n",
    "\n",
    "        z_mu = (z.unsqueeze(1) - mu.unsqueeze(0))\n",
    "\n",
    "        cov_inverse = []\n",
    "        det_cov = []\n",
    "        cov_diag = 0\n",
    "        eps = 1e-12\n",
    "        for i in range(k):\n",
    "            # K x D x D\n",
    "            cov_k = cov[i] + self.to_var(torch.eye(d) * eps)\n",
    "            pinv = np.linalg.pinv(cov_k.data.numpy())\n",
    "            cov_inverse.append(Variable(torch.from_numpy(pinv)).unsqueeze(0))\n",
    "\n",
    "            eigvals = np.linalg.eigvals(cov_k.data.cpu().numpy() * (2 * np.pi))\n",
    "            if np.min(eigvals) < 0:\n",
    "                logging.warning(f'Determinant was negative! Clipping Eigenvalues to 0+epsilon from {np.min(eigvals)}')\n",
    "            determinant = np.prod(np.clip(eigvals, a_min=sys.float_info.epsilon, a_max=None))\n",
    "            det_cov.append(determinant)\n",
    "\n",
    "            cov_diag = cov_diag + torch.sum(1 / cov_k.diag())\n",
    "\n",
    "        # K x D x D\n",
    "        cov_inverse = torch.cat(cov_inverse, dim=0)\n",
    "        # K\n",
    "        det_cov = Variable(torch.from_numpy(np.float32(np.array(det_cov))))\n",
    "\n",
    "        # N x K\n",
    "        exp_term_tmp = -0.5 * torch.sum(torch.sum(z_mu.unsqueeze(-1) * cov_inverse.unsqueeze(0), dim=-2) * z_mu, dim=-1)\n",
    "        # for stability (logsumexp)\n",
    "        max_val = torch.max((exp_term_tmp).clamp(min=0), dim=1, keepdim=True)[0]\n",
    "\n",
    "        exp_term = torch.exp(exp_term_tmp - max_val)\n",
    "\n",
    "        sample_energy = -max_val.squeeze() - torch.log(\n",
    "            torch.sum(self.to_var(phi.unsqueeze(0)) * exp_term / (torch.sqrt(self.to_var(det_cov)) + eps).unsqueeze(0),\n",
    "                      dim=1) + eps)\n",
    "\n",
    "        if size_average:\n",
    "            sample_energy = torch.mean(sample_energy)\n",
    "\n",
    "        return sample_energy, cov_diag\n",
    "\n",
    "    def loss_function(self, x, x_hat, z, gamma, lambda_energy, lambda_cov_diag):\n",
    "        recon_error = torch.mean((x.view(*x_hat.shape) - x_hat) ** 2)\n",
    "        phi, mu, cov = self.compute_gmm_params(z, gamma)\n",
    "        sample_energy, cov_diag = self.compute_energy(z, phi, mu, cov)\n",
    "        loss = recon_error + lambda_energy * sample_energy + lambda_cov_diag * cov_diag\n",
    "        return loss, sample_energy, recon_error, cov_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "detectors = DAGMM(num_epochs=10, sequence_length=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 19.05it/s]\n"
     ]
    }
   ],
   "source": [
    "detectors.fit(X_train.iloc[:,:-1].copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 23)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.iloc[:,:-1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare feature for predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n",
      "shape of sequence torch.Size([1, 3, 23])\n"
     ]
    }
   ],
   "source": [
    "score = detectors.predict(X_train.iloc[:,:-1].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 2)\n"
     ]
    }
   ],
   "source": [
    "output = pd.DataFrame({\"cano_locdt_index\":X_train.iloc[:,-1]})\n",
    "output[\"score\"] = score\n",
    "print (output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 23)\n",
      "(300, 6)\n",
      "(300, 62)\n"
     ]
    }
   ],
   "source": [
    "output[\"cosine_errors_mean\"] = detectors.prediction_details[\"cosine_errors_mean\"]\n",
    "output[\"euclidean_errors_mean\"]  = detectors.prediction_details[\"euclidean_errors_mean\"]\n",
    "data = detectors.prediction_details[\"reconstructions_mean\"]\n",
    "reconstructions_mean = pd.DataFrame(data.T,\n",
    "             columns = [\"reconstructions_mean_latent_features_{}\".format(i) for i in range(data.shape[0])]\n",
    "            )\n",
    "print (reconstructions_mean.shape)\n",
    "data = detectors.prediction_details[\"latent_representations\"]\n",
    "latent_representations = pd.DataFrame(data.T,\n",
    "             columns = [\"latent_representations_latent_features_{}\".format(i) for i in range(data.shape[0])]\n",
    "            )\n",
    "print (latent_representations.shape)\n",
    "output = pd.concat([output,reconstructions_mean,latent_representations], axis = 1)\n",
    "print (output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cano_locdt_index</th>\n",
       "      <th>score</th>\n",
       "      <th>cosine_errors_mean</th>\n",
       "      <th>euclidean_errors_mean</th>\n",
       "      <th>reconstructions_mean_latent_features_0</th>\n",
       "      <th>reconstructions_mean_latent_features_1</th>\n",
       "      <th>reconstructions_mean_latent_features_2</th>\n",
       "      <th>reconstructions_mean_latent_features_3</th>\n",
       "      <th>reconstructions_mean_latent_features_4</th>\n",
       "      <th>reconstructions_mean_latent_features_5</th>\n",
       "      <th>reconstructions_mean_latent_features_6</th>\n",
       "      <th>reconstructions_mean_latent_features_7</th>\n",
       "      <th>reconstructions_mean_latent_features_8</th>\n",
       "      <th>reconstructions_mean_latent_features_9</th>\n",
       "      <th>reconstructions_mean_latent_features_10</th>\n",
       "      <th>reconstructions_mean_latent_features_11</th>\n",
       "      <th>reconstructions_mean_latent_features_12</th>\n",
       "      <th>reconstructions_mean_latent_features_13</th>\n",
       "      <th>reconstructions_mean_latent_features_14</th>\n",
       "      <th>reconstructions_mean_latent_features_15</th>\n",
       "      <th>reconstructions_mean_latent_features_16</th>\n",
       "      <th>reconstructions_mean_latent_features_17</th>\n",
       "      <th>reconstructions_mean_latent_features_18</th>\n",
       "      <th>reconstructions_mean_latent_features_19</th>\n",
       "      <th>reconstructions_mean_latent_features_20</th>\n",
       "      <th>reconstructions_mean_latent_features_21</th>\n",
       "      <th>reconstructions_mean_latent_features_22</th>\n",
       "      <th>latent_representations_latent_features_0</th>\n",
       "      <th>latent_representations_latent_features_1</th>\n",
       "      <th>latent_representations_latent_features_2</th>\n",
       "      <th>latent_representations_latent_features_3</th>\n",
       "      <th>latent_representations_latent_features_4</th>\n",
       "      <th>latent_representations_latent_features_5</th>\n",
       "      <th>reconstructions_mean_latent_features_0</th>\n",
       "      <th>reconstructions_mean_latent_features_1</th>\n",
       "      <th>reconstructions_mean_latent_features_2</th>\n",
       "      <th>reconstructions_mean_latent_features_3</th>\n",
       "      <th>reconstructions_mean_latent_features_4</th>\n",
       "      <th>reconstructions_mean_latent_features_5</th>\n",
       "      <th>reconstructions_mean_latent_features_6</th>\n",
       "      <th>reconstructions_mean_latent_features_7</th>\n",
       "      <th>reconstructions_mean_latent_features_8</th>\n",
       "      <th>reconstructions_mean_latent_features_9</th>\n",
       "      <th>reconstructions_mean_latent_features_10</th>\n",
       "      <th>reconstructions_mean_latent_features_11</th>\n",
       "      <th>reconstructions_mean_latent_features_12</th>\n",
       "      <th>reconstructions_mean_latent_features_13</th>\n",
       "      <th>reconstructions_mean_latent_features_14</th>\n",
       "      <th>reconstructions_mean_latent_features_15</th>\n",
       "      <th>reconstructions_mean_latent_features_16</th>\n",
       "      <th>reconstructions_mean_latent_features_17</th>\n",
       "      <th>reconstructions_mean_latent_features_18</th>\n",
       "      <th>reconstructions_mean_latent_features_19</th>\n",
       "      <th>reconstructions_mean_latent_features_20</th>\n",
       "      <th>reconstructions_mean_latent_features_21</th>\n",
       "      <th>reconstructions_mean_latent_features_22</th>\n",
       "      <th>latent_representations_latent_features_0</th>\n",
       "      <th>latent_representations_latent_features_1</th>\n",
       "      <th>latent_representations_latent_features_2</th>\n",
       "      <th>latent_representations_latent_features_3</th>\n",
       "      <th>latent_representations_latent_features_4</th>\n",
       "      <th>latent_representations_latent_features_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.631021</td>\n",
       "      <td>-0.295345</td>\n",
       "      <td>-0.059350</td>\n",
       "      <td>0.924081</td>\n",
       "      <td>-0.135590</td>\n",
       "      <td>0.499637</td>\n",
       "      <td>0.241374</td>\n",
       "      <td>-0.017618</td>\n",
       "      <td>0.693070</td>\n",
       "      <td>-0.296043</td>\n",
       "      <td>0.333671</td>\n",
       "      <td>0.215816</td>\n",
       "      <td>-0.632744</td>\n",
       "      <td>-0.082227</td>\n",
       "      <td>0.352146</td>\n",
       "      <td>0.786789</td>\n",
       "      <td>0.181245</td>\n",
       "      <td>-0.725199</td>\n",
       "      <td>-0.668880</td>\n",
       "      <td>0.253682</td>\n",
       "      <td>0.315962</td>\n",
       "      <td>0.589892</td>\n",
       "      <td>0.212861</td>\n",
       "      <td>0.284910</td>\n",
       "      <td>0.750360</td>\n",
       "      <td>0.456047</td>\n",
       "      <td>0.025644</td>\n",
       "      <td>-0.059350</td>\n",
       "      <td>-0.295345</td>\n",
       "      <td>-0.156371</td>\n",
       "      <td>0.089071</td>\n",
       "      <td>0.074201</td>\n",
       "      <td>0.924081</td>\n",
       "      <td>-0.135590</td>\n",
       "      <td>0.499637</td>\n",
       "      <td>0.241374</td>\n",
       "      <td>-0.017618</td>\n",
       "      <td>0.693070</td>\n",
       "      <td>-0.296043</td>\n",
       "      <td>0.333671</td>\n",
       "      <td>0.215816</td>\n",
       "      <td>-0.632744</td>\n",
       "      <td>-0.082227</td>\n",
       "      <td>0.352146</td>\n",
       "      <td>0.786789</td>\n",
       "      <td>0.181245</td>\n",
       "      <td>-0.725199</td>\n",
       "      <td>-0.668880</td>\n",
       "      <td>0.253682</td>\n",
       "      <td>0.315962</td>\n",
       "      <td>0.589892</td>\n",
       "      <td>0.212861</td>\n",
       "      <td>0.284910</td>\n",
       "      <td>0.750360</td>\n",
       "      <td>0.456047</td>\n",
       "      <td>0.025644</td>\n",
       "      <td>-0.059350</td>\n",
       "      <td>-0.295345</td>\n",
       "      <td>-0.156371</td>\n",
       "      <td>0.089071</td>\n",
       "      <td>0.074201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27.631021</td>\n",
       "      <td>0.382906</td>\n",
       "      <td>0.271174</td>\n",
       "      <td>0.727449</td>\n",
       "      <td>0.151623</td>\n",
       "      <td>-0.271612</td>\n",
       "      <td>-0.256591</td>\n",
       "      <td>0.490845</td>\n",
       "      <td>1.015347</td>\n",
       "      <td>0.207021</td>\n",
       "      <td>0.011632</td>\n",
       "      <td>0.392299</td>\n",
       "      <td>-0.386213</td>\n",
       "      <td>0.593477</td>\n",
       "      <td>0.432118</td>\n",
       "      <td>0.357133</td>\n",
       "      <td>0.118143</td>\n",
       "      <td>-0.396401</td>\n",
       "      <td>-0.241968</td>\n",
       "      <td>0.561575</td>\n",
       "      <td>0.709550</td>\n",
       "      <td>0.691831</td>\n",
       "      <td>0.556354</td>\n",
       "      <td>0.085471</td>\n",
       "      <td>0.667678</td>\n",
       "      <td>0.544193</td>\n",
       "      <td>-0.466960</td>\n",
       "      <td>0.271174</td>\n",
       "      <td>0.382906</td>\n",
       "      <td>-0.666963</td>\n",
       "      <td>0.448975</td>\n",
       "      <td>-0.493218</td>\n",
       "      <td>0.727449</td>\n",
       "      <td>0.151623</td>\n",
       "      <td>-0.271612</td>\n",
       "      <td>-0.256591</td>\n",
       "      <td>0.490845</td>\n",
       "      <td>1.015347</td>\n",
       "      <td>0.207021</td>\n",
       "      <td>0.011632</td>\n",
       "      <td>0.392299</td>\n",
       "      <td>-0.386213</td>\n",
       "      <td>0.593477</td>\n",
       "      <td>0.432118</td>\n",
       "      <td>0.357133</td>\n",
       "      <td>0.118143</td>\n",
       "      <td>-0.396401</td>\n",
       "      <td>-0.241968</td>\n",
       "      <td>0.561575</td>\n",
       "      <td>0.709550</td>\n",
       "      <td>0.691831</td>\n",
       "      <td>0.556354</td>\n",
       "      <td>0.085471</td>\n",
       "      <td>0.667678</td>\n",
       "      <td>0.544193</td>\n",
       "      <td>-0.466960</td>\n",
       "      <td>0.271174</td>\n",
       "      <td>0.382906</td>\n",
       "      <td>-0.666963</td>\n",
       "      <td>0.448975</td>\n",
       "      <td>-0.493218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3749_1</td>\n",
       "      <td>25.149796</td>\n",
       "      <td>0.375645</td>\n",
       "      <td>0.293028</td>\n",
       "      <td>0.615858</td>\n",
       "      <td>-0.015094</td>\n",
       "      <td>0.043652</td>\n",
       "      <td>0.213557</td>\n",
       "      <td>0.288134</td>\n",
       "      <td>0.883730</td>\n",
       "      <td>-0.065764</td>\n",
       "      <td>0.124521</td>\n",
       "      <td>0.471429</td>\n",
       "      <td>-0.453050</td>\n",
       "      <td>0.440064</td>\n",
       "      <td>0.413630</td>\n",
       "      <td>0.400986</td>\n",
       "      <td>0.135690</td>\n",
       "      <td>-0.418351</td>\n",
       "      <td>-0.236989</td>\n",
       "      <td>0.402316</td>\n",
       "      <td>0.329611</td>\n",
       "      <td>0.657517</td>\n",
       "      <td>0.392622</td>\n",
       "      <td>0.233974</td>\n",
       "      <td>0.523237</td>\n",
       "      <td>0.546441</td>\n",
       "      <td>-0.470165</td>\n",
       "      <td>0.293028</td>\n",
       "      <td>0.375645</td>\n",
       "      <td>-0.673999</td>\n",
       "      <td>0.433263</td>\n",
       "      <td>-0.466943</td>\n",
       "      <td>0.615858</td>\n",
       "      <td>-0.015094</td>\n",
       "      <td>0.043652</td>\n",
       "      <td>0.213557</td>\n",
       "      <td>0.288134</td>\n",
       "      <td>0.883730</td>\n",
       "      <td>-0.065764</td>\n",
       "      <td>0.124521</td>\n",
       "      <td>0.471429</td>\n",
       "      <td>-0.453050</td>\n",
       "      <td>0.440064</td>\n",
       "      <td>0.413630</td>\n",
       "      <td>0.400986</td>\n",
       "      <td>0.135690</td>\n",
       "      <td>-0.418351</td>\n",
       "      <td>-0.236989</td>\n",
       "      <td>0.402316</td>\n",
       "      <td>0.329611</td>\n",
       "      <td>0.657517</td>\n",
       "      <td>0.392622</td>\n",
       "      <td>0.233974</td>\n",
       "      <td>0.523237</td>\n",
       "      <td>0.546441</td>\n",
       "      <td>-0.470165</td>\n",
       "      <td>0.293028</td>\n",
       "      <td>0.375645</td>\n",
       "      <td>-0.673999</td>\n",
       "      <td>0.433263</td>\n",
       "      <td>-0.466943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>14.849816</td>\n",
       "      <td>0.751846</td>\n",
       "      <td>0.524848</td>\n",
       "      <td>0.527854</td>\n",
       "      <td>0.061934</td>\n",
       "      <td>0.025055</td>\n",
       "      <td>-0.068244</td>\n",
       "      <td>0.514123</td>\n",
       "      <td>0.804582</td>\n",
       "      <td>0.102690</td>\n",
       "      <td>0.296015</td>\n",
       "      <td>0.670417</td>\n",
       "      <td>-0.188862</td>\n",
       "      <td>0.711714</td>\n",
       "      <td>0.656565</td>\n",
       "      <td>0.538123</td>\n",
       "      <td>-0.054643</td>\n",
       "      <td>-0.196332</td>\n",
       "      <td>-0.076617</td>\n",
       "      <td>0.599034</td>\n",
       "      <td>0.445602</td>\n",
       "      <td>0.639521</td>\n",
       "      <td>0.643435</td>\n",
       "      <td>0.317581</td>\n",
       "      <td>0.574855</td>\n",
       "      <td>0.386058</td>\n",
       "      <td>-0.706799</td>\n",
       "      <td>0.524848</td>\n",
       "      <td>0.751846</td>\n",
       "      <td>-0.971888</td>\n",
       "      <td>0.631818</td>\n",
       "      <td>-0.753402</td>\n",
       "      <td>0.527854</td>\n",
       "      <td>0.061934</td>\n",
       "      <td>0.025055</td>\n",
       "      <td>-0.068244</td>\n",
       "      <td>0.514123</td>\n",
       "      <td>0.804582</td>\n",
       "      <td>0.102690</td>\n",
       "      <td>0.296015</td>\n",
       "      <td>0.670417</td>\n",
       "      <td>-0.188862</td>\n",
       "      <td>0.711714</td>\n",
       "      <td>0.656565</td>\n",
       "      <td>0.538123</td>\n",
       "      <td>-0.054643</td>\n",
       "      <td>-0.196332</td>\n",
       "      <td>-0.076617</td>\n",
       "      <td>0.599034</td>\n",
       "      <td>0.445602</td>\n",
       "      <td>0.639521</td>\n",
       "      <td>0.643435</td>\n",
       "      <td>0.317581</td>\n",
       "      <td>0.574855</td>\n",
       "      <td>0.386058</td>\n",
       "      <td>-0.706799</td>\n",
       "      <td>0.524848</td>\n",
       "      <td>0.751846</td>\n",
       "      <td>-0.971888</td>\n",
       "      <td>0.631818</td>\n",
       "      <td>-0.753402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3749_1</td>\n",
       "      <td>-0.102822</td>\n",
       "      <td>0.774593</td>\n",
       "      <td>0.580416</td>\n",
       "      <td>0.463161</td>\n",
       "      <td>0.047286</td>\n",
       "      <td>0.021042</td>\n",
       "      <td>0.056516</td>\n",
       "      <td>0.490920</td>\n",
       "      <td>0.795669</td>\n",
       "      <td>0.052190</td>\n",
       "      <td>0.263364</td>\n",
       "      <td>0.665853</td>\n",
       "      <td>-0.185761</td>\n",
       "      <td>0.698043</td>\n",
       "      <td>0.608211</td>\n",
       "      <td>0.469882</td>\n",
       "      <td>-0.016209</td>\n",
       "      <td>-0.160862</td>\n",
       "      <td>-0.016941</td>\n",
       "      <td>0.559579</td>\n",
       "      <td>0.351703</td>\n",
       "      <td>0.597816</td>\n",
       "      <td>0.595374</td>\n",
       "      <td>0.312439</td>\n",
       "      <td>0.511031</td>\n",
       "      <td>0.411958</td>\n",
       "      <td>-0.700036</td>\n",
       "      <td>0.580416</td>\n",
       "      <td>0.774593</td>\n",
       "      <td>-1.004546</td>\n",
       "      <td>0.639794</td>\n",
       "      <td>-0.728963</td>\n",
       "      <td>0.463161</td>\n",
       "      <td>0.047286</td>\n",
       "      <td>0.021042</td>\n",
       "      <td>0.056516</td>\n",
       "      <td>0.490920</td>\n",
       "      <td>0.795669</td>\n",
       "      <td>0.052190</td>\n",
       "      <td>0.263364</td>\n",
       "      <td>0.665853</td>\n",
       "      <td>-0.185761</td>\n",
       "      <td>0.698043</td>\n",
       "      <td>0.608211</td>\n",
       "      <td>0.469882</td>\n",
       "      <td>-0.016209</td>\n",
       "      <td>-0.160862</td>\n",
       "      <td>-0.016941</td>\n",
       "      <td>0.559579</td>\n",
       "      <td>0.351703</td>\n",
       "      <td>0.597816</td>\n",
       "      <td>0.595374</td>\n",
       "      <td>0.312439</td>\n",
       "      <td>0.511031</td>\n",
       "      <td>0.411958</td>\n",
       "      <td>-0.700036</td>\n",
       "      <td>0.580416</td>\n",
       "      <td>0.774593</td>\n",
       "      <td>-1.004546</td>\n",
       "      <td>0.639794</td>\n",
       "      <td>-0.728963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>114592_7</td>\n",
       "      <td>-6.041363</td>\n",
       "      <td>0.166159</td>\n",
       "      <td>0.052324</td>\n",
       "      <td>0.799040</td>\n",
       "      <td>0.059592</td>\n",
       "      <td>-0.156794</td>\n",
       "      <td>0.022642</td>\n",
       "      <td>0.224824</td>\n",
       "      <td>1.048931</td>\n",
       "      <td>-0.006963</td>\n",
       "      <td>-0.072168</td>\n",
       "      <td>0.292208</td>\n",
       "      <td>-0.620655</td>\n",
       "      <td>0.373231</td>\n",
       "      <td>0.281610</td>\n",
       "      <td>0.287987</td>\n",
       "      <td>0.239766</td>\n",
       "      <td>-0.599871</td>\n",
       "      <td>-0.393992</td>\n",
       "      <td>0.363112</td>\n",
       "      <td>0.473857</td>\n",
       "      <td>0.775260</td>\n",
       "      <td>0.342774</td>\n",
       "      <td>0.081585</td>\n",
       "      <td>0.606319</td>\n",
       "      <td>0.626860</td>\n",
       "      <td>-0.231496</td>\n",
       "      <td>0.052324</td>\n",
       "      <td>0.166159</td>\n",
       "      <td>-0.328720</td>\n",
       "      <td>0.132949</td>\n",
       "      <td>-0.314295</td>\n",
       "      <td>0.799040</td>\n",
       "      <td>0.059592</td>\n",
       "      <td>-0.156794</td>\n",
       "      <td>0.022642</td>\n",
       "      <td>0.224824</td>\n",
       "      <td>1.048931</td>\n",
       "      <td>-0.006963</td>\n",
       "      <td>-0.072168</td>\n",
       "      <td>0.292208</td>\n",
       "      <td>-0.620655</td>\n",
       "      <td>0.373231</td>\n",
       "      <td>0.281610</td>\n",
       "      <td>0.287987</td>\n",
       "      <td>0.239766</td>\n",
       "      <td>-0.599871</td>\n",
       "      <td>-0.393992</td>\n",
       "      <td>0.363112</td>\n",
       "      <td>0.473857</td>\n",
       "      <td>0.775260</td>\n",
       "      <td>0.342774</td>\n",
       "      <td>0.081585</td>\n",
       "      <td>0.606319</td>\n",
       "      <td>0.626860</td>\n",
       "      <td>-0.231496</td>\n",
       "      <td>0.052324</td>\n",
       "      <td>0.166159</td>\n",
       "      <td>-0.328720</td>\n",
       "      <td>0.132949</td>\n",
       "      <td>-0.314295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>63818_7</td>\n",
       "      <td>-13.553216</td>\n",
       "      <td>0.364161</td>\n",
       "      <td>0.149798</td>\n",
       "      <td>0.718950</td>\n",
       "      <td>0.008564</td>\n",
       "      <td>0.028018</td>\n",
       "      <td>0.129240</td>\n",
       "      <td>0.285359</td>\n",
       "      <td>0.953450</td>\n",
       "      <td>-0.041098</td>\n",
       "      <td>0.102429</td>\n",
       "      <td>0.455793</td>\n",
       "      <td>-0.516891</td>\n",
       "      <td>0.444984</td>\n",
       "      <td>0.437084</td>\n",
       "      <td>0.443345</td>\n",
       "      <td>0.144968</td>\n",
       "      <td>-0.482219</td>\n",
       "      <td>-0.303066</td>\n",
       "      <td>0.420591</td>\n",
       "      <td>0.399034</td>\n",
       "      <td>0.710684</td>\n",
       "      <td>0.413561</td>\n",
       "      <td>0.223634</td>\n",
       "      <td>0.578219</td>\n",
       "      <td>0.562926</td>\n",
       "      <td>-0.367954</td>\n",
       "      <td>0.149798</td>\n",
       "      <td>0.364161</td>\n",
       "      <td>-0.473505</td>\n",
       "      <td>0.219472</td>\n",
       "      <td>-0.484520</td>\n",
       "      <td>0.718950</td>\n",
       "      <td>0.008564</td>\n",
       "      <td>0.028018</td>\n",
       "      <td>0.129240</td>\n",
       "      <td>0.285359</td>\n",
       "      <td>0.953450</td>\n",
       "      <td>-0.041098</td>\n",
       "      <td>0.102429</td>\n",
       "      <td>0.455793</td>\n",
       "      <td>-0.516891</td>\n",
       "      <td>0.444984</td>\n",
       "      <td>0.437084</td>\n",
       "      <td>0.443345</td>\n",
       "      <td>0.144968</td>\n",
       "      <td>-0.482219</td>\n",
       "      <td>-0.303066</td>\n",
       "      <td>0.420591</td>\n",
       "      <td>0.399034</td>\n",
       "      <td>0.710684</td>\n",
       "      <td>0.413561</td>\n",
       "      <td>0.223634</td>\n",
       "      <td>0.578219</td>\n",
       "      <td>0.562926</td>\n",
       "      <td>-0.367954</td>\n",
       "      <td>0.149798</td>\n",
       "      <td>0.364161</td>\n",
       "      <td>-0.473505</td>\n",
       "      <td>0.219472</td>\n",
       "      <td>-0.484520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297</td>\n",
       "      <td>114592_7</td>\n",
       "      <td>-14.528060</td>\n",
       "      <td>0.532978</td>\n",
       "      <td>0.369651</td>\n",
       "      <td>0.624850</td>\n",
       "      <td>0.056137</td>\n",
       "      <td>-0.011949</td>\n",
       "      <td>-0.026962</td>\n",
       "      <td>0.415428</td>\n",
       "      <td>0.877308</td>\n",
       "      <td>0.064062</td>\n",
       "      <td>0.184376</td>\n",
       "      <td>0.549605</td>\n",
       "      <td>-0.335890</td>\n",
       "      <td>0.588437</td>\n",
       "      <td>0.533188</td>\n",
       "      <td>0.476712</td>\n",
       "      <td>0.038410</td>\n",
       "      <td>-0.324567</td>\n",
       "      <td>-0.181855</td>\n",
       "      <td>0.501890</td>\n",
       "      <td>0.447872</td>\n",
       "      <td>0.685958</td>\n",
       "      <td>0.522313</td>\n",
       "      <td>0.240465</td>\n",
       "      <td>0.582773</td>\n",
       "      <td>0.456203</td>\n",
       "      <td>-0.452632</td>\n",
       "      <td>0.369651</td>\n",
       "      <td>0.532978</td>\n",
       "      <td>-0.663555</td>\n",
       "      <td>0.314323</td>\n",
       "      <td>-0.562679</td>\n",
       "      <td>0.624850</td>\n",
       "      <td>0.056137</td>\n",
       "      <td>-0.011949</td>\n",
       "      <td>-0.026962</td>\n",
       "      <td>0.415428</td>\n",
       "      <td>0.877308</td>\n",
       "      <td>0.064062</td>\n",
       "      <td>0.184376</td>\n",
       "      <td>0.549605</td>\n",
       "      <td>-0.335890</td>\n",
       "      <td>0.588437</td>\n",
       "      <td>0.533188</td>\n",
       "      <td>0.476712</td>\n",
       "      <td>0.038410</td>\n",
       "      <td>-0.324567</td>\n",
       "      <td>-0.181855</td>\n",
       "      <td>0.501890</td>\n",
       "      <td>0.447872</td>\n",
       "      <td>0.685958</td>\n",
       "      <td>0.522313</td>\n",
       "      <td>0.240465</td>\n",
       "      <td>0.582773</td>\n",
       "      <td>0.456203</td>\n",
       "      <td>-0.452632</td>\n",
       "      <td>0.369651</td>\n",
       "      <td>0.532978</td>\n",
       "      <td>-0.663555</td>\n",
       "      <td>0.314323</td>\n",
       "      <td>-0.562679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>63818_7</td>\n",
       "      <td>-14.508166</td>\n",
       "      <td>0.535894</td>\n",
       "      <td>0.328998</td>\n",
       "      <td>0.620974</td>\n",
       "      <td>0.003399</td>\n",
       "      <td>0.024119</td>\n",
       "      <td>0.116367</td>\n",
       "      <td>0.349870</td>\n",
       "      <td>0.908796</td>\n",
       "      <td>0.039865</td>\n",
       "      <td>0.124301</td>\n",
       "      <td>0.585564</td>\n",
       "      <td>-0.473440</td>\n",
       "      <td>0.545585</td>\n",
       "      <td>0.495606</td>\n",
       "      <td>0.447903</td>\n",
       "      <td>0.052618</td>\n",
       "      <td>-0.399251</td>\n",
       "      <td>-0.198518</td>\n",
       "      <td>0.431417</td>\n",
       "      <td>0.386331</td>\n",
       "      <td>0.756053</td>\n",
       "      <td>0.415958</td>\n",
       "      <td>0.234272</td>\n",
       "      <td>0.486071</td>\n",
       "      <td>0.537504</td>\n",
       "      <td>-0.468846</td>\n",
       "      <td>0.328998</td>\n",
       "      <td>0.535894</td>\n",
       "      <td>-0.655420</td>\n",
       "      <td>0.336354</td>\n",
       "      <td>-0.584057</td>\n",
       "      <td>0.620974</td>\n",
       "      <td>0.003399</td>\n",
       "      <td>0.024119</td>\n",
       "      <td>0.116367</td>\n",
       "      <td>0.349870</td>\n",
       "      <td>0.908796</td>\n",
       "      <td>0.039865</td>\n",
       "      <td>0.124301</td>\n",
       "      <td>0.585564</td>\n",
       "      <td>-0.473440</td>\n",
       "      <td>0.545585</td>\n",
       "      <td>0.495606</td>\n",
       "      <td>0.447903</td>\n",
       "      <td>0.052618</td>\n",
       "      <td>-0.399251</td>\n",
       "      <td>-0.198518</td>\n",
       "      <td>0.431417</td>\n",
       "      <td>0.386331</td>\n",
       "      <td>0.756053</td>\n",
       "      <td>0.415958</td>\n",
       "      <td>0.234272</td>\n",
       "      <td>0.486071</td>\n",
       "      <td>0.537504</td>\n",
       "      <td>-0.468846</td>\n",
       "      <td>0.328998</td>\n",
       "      <td>0.535894</td>\n",
       "      <td>-0.655420</td>\n",
       "      <td>0.336354</td>\n",
       "      <td>-0.584057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>82829_7</td>\n",
       "      <td>-15.592033</td>\n",
       "      <td>0.643885</td>\n",
       "      <td>0.484361</td>\n",
       "      <td>0.561604</td>\n",
       "      <td>0.054388</td>\n",
       "      <td>0.056957</td>\n",
       "      <td>-0.025368</td>\n",
       "      <td>0.228774</td>\n",
       "      <td>0.717402</td>\n",
       "      <td>-0.082200</td>\n",
       "      <td>0.208895</td>\n",
       "      <td>0.641069</td>\n",
       "      <td>-0.329810</td>\n",
       "      <td>0.612947</td>\n",
       "      <td>0.565084</td>\n",
       "      <td>0.466685</td>\n",
       "      <td>-0.027274</td>\n",
       "      <td>-0.283880</td>\n",
       "      <td>-0.074148</td>\n",
       "      <td>0.427189</td>\n",
       "      <td>0.084883</td>\n",
       "      <td>0.687125</td>\n",
       "      <td>0.500243</td>\n",
       "      <td>0.316556</td>\n",
       "      <td>0.512957</td>\n",
       "      <td>0.278132</td>\n",
       "      <td>-0.537599</td>\n",
       "      <td>0.484361</td>\n",
       "      <td>0.643885</td>\n",
       "      <td>-0.818383</td>\n",
       "      <td>0.460280</td>\n",
       "      <td>-0.628634</td>\n",
       "      <td>0.561604</td>\n",
       "      <td>0.054388</td>\n",
       "      <td>0.056957</td>\n",
       "      <td>-0.025368</td>\n",
       "      <td>0.228774</td>\n",
       "      <td>0.717402</td>\n",
       "      <td>-0.082200</td>\n",
       "      <td>0.208895</td>\n",
       "      <td>0.641069</td>\n",
       "      <td>-0.329810</td>\n",
       "      <td>0.612947</td>\n",
       "      <td>0.565084</td>\n",
       "      <td>0.466685</td>\n",
       "      <td>-0.027274</td>\n",
       "      <td>-0.283880</td>\n",
       "      <td>-0.074148</td>\n",
       "      <td>0.427189</td>\n",
       "      <td>0.084883</td>\n",
       "      <td>0.687125</td>\n",
       "      <td>0.500243</td>\n",
       "      <td>0.316556</td>\n",
       "      <td>0.512957</td>\n",
       "      <td>0.278132</td>\n",
       "      <td>-0.537599</td>\n",
       "      <td>0.484361</td>\n",
       "      <td>0.643885</td>\n",
       "      <td>-0.818383</td>\n",
       "      <td>0.460280</td>\n",
       "      <td>-0.628634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    cano_locdt_index      score  cosine_errors_mean  euclidean_errors_mean  \\\n",
       "0                  0  27.631021           -0.295345              -0.059350   \n",
       "1                  0  27.631021            0.382906               0.271174   \n",
       "2             3749_1  25.149796            0.375645               0.293028   \n",
       "3                  0  14.849816            0.751846               0.524848   \n",
       "4             3749_1  -0.102822            0.774593               0.580416   \n",
       "..               ...        ...                 ...                    ...   \n",
       "295         114592_7  -6.041363            0.166159               0.052324   \n",
       "296          63818_7 -13.553216            0.364161               0.149798   \n",
       "297         114592_7 -14.528060            0.532978               0.369651   \n",
       "298          63818_7 -14.508166            0.535894               0.328998   \n",
       "299          82829_7 -15.592033            0.643885               0.484361   \n",
       "\n",
       "     reconstructions_mean_latent_features_0  \\\n",
       "0                                  0.924081   \n",
       "1                                  0.727449   \n",
       "2                                  0.615858   \n",
       "3                                  0.527854   \n",
       "4                                  0.463161   \n",
       "..                                      ...   \n",
       "295                                0.799040   \n",
       "296                                0.718950   \n",
       "297                                0.624850   \n",
       "298                                0.620974   \n",
       "299                                0.561604   \n",
       "\n",
       "     reconstructions_mean_latent_features_1  \\\n",
       "0                                 -0.135590   \n",
       "1                                  0.151623   \n",
       "2                                 -0.015094   \n",
       "3                                  0.061934   \n",
       "4                                  0.047286   \n",
       "..                                      ...   \n",
       "295                                0.059592   \n",
       "296                                0.008564   \n",
       "297                                0.056137   \n",
       "298                                0.003399   \n",
       "299                                0.054388   \n",
       "\n",
       "     reconstructions_mean_latent_features_2  \\\n",
       "0                                  0.499637   \n",
       "1                                 -0.271612   \n",
       "2                                  0.043652   \n",
       "3                                  0.025055   \n",
       "4                                  0.021042   \n",
       "..                                      ...   \n",
       "295                               -0.156794   \n",
       "296                                0.028018   \n",
       "297                               -0.011949   \n",
       "298                                0.024119   \n",
       "299                                0.056957   \n",
       "\n",
       "     reconstructions_mean_latent_features_3  \\\n",
       "0                                  0.241374   \n",
       "1                                 -0.256591   \n",
       "2                                  0.213557   \n",
       "3                                 -0.068244   \n",
       "4                                  0.056516   \n",
       "..                                      ...   \n",
       "295                                0.022642   \n",
       "296                                0.129240   \n",
       "297                               -0.026962   \n",
       "298                                0.116367   \n",
       "299                               -0.025368   \n",
       "\n",
       "     reconstructions_mean_latent_features_4  \\\n",
       "0                                 -0.017618   \n",
       "1                                  0.490845   \n",
       "2                                  0.288134   \n",
       "3                                  0.514123   \n",
       "4                                  0.490920   \n",
       "..                                      ...   \n",
       "295                                0.224824   \n",
       "296                                0.285359   \n",
       "297                                0.415428   \n",
       "298                                0.349870   \n",
       "299                                0.228774   \n",
       "\n",
       "     reconstructions_mean_latent_features_5  \\\n",
       "0                                  0.693070   \n",
       "1                                  1.015347   \n",
       "2                                  0.883730   \n",
       "3                                  0.804582   \n",
       "4                                  0.795669   \n",
       "..                                      ...   \n",
       "295                                1.048931   \n",
       "296                                0.953450   \n",
       "297                                0.877308   \n",
       "298                                0.908796   \n",
       "299                                0.717402   \n",
       "\n",
       "     reconstructions_mean_latent_features_6  \\\n",
       "0                                 -0.296043   \n",
       "1                                  0.207021   \n",
       "2                                 -0.065764   \n",
       "3                                  0.102690   \n",
       "4                                  0.052190   \n",
       "..                                      ...   \n",
       "295                               -0.006963   \n",
       "296                               -0.041098   \n",
       "297                                0.064062   \n",
       "298                                0.039865   \n",
       "299                               -0.082200   \n",
       "\n",
       "     reconstructions_mean_latent_features_7  \\\n",
       "0                                  0.333671   \n",
       "1                                  0.011632   \n",
       "2                                  0.124521   \n",
       "3                                  0.296015   \n",
       "4                                  0.263364   \n",
       "..                                      ...   \n",
       "295                               -0.072168   \n",
       "296                                0.102429   \n",
       "297                                0.184376   \n",
       "298                                0.124301   \n",
       "299                                0.208895   \n",
       "\n",
       "     reconstructions_mean_latent_features_8  \\\n",
       "0                                  0.215816   \n",
       "1                                  0.392299   \n",
       "2                                  0.471429   \n",
       "3                                  0.670417   \n",
       "4                                  0.665853   \n",
       "..                                      ...   \n",
       "295                                0.292208   \n",
       "296                                0.455793   \n",
       "297                                0.549605   \n",
       "298                                0.585564   \n",
       "299                                0.641069   \n",
       "\n",
       "     reconstructions_mean_latent_features_9  \\\n",
       "0                                 -0.632744   \n",
       "1                                 -0.386213   \n",
       "2                                 -0.453050   \n",
       "3                                 -0.188862   \n",
       "4                                 -0.185761   \n",
       "..                                      ...   \n",
       "295                               -0.620655   \n",
       "296                               -0.516891   \n",
       "297                               -0.335890   \n",
       "298                               -0.473440   \n",
       "299                               -0.329810   \n",
       "\n",
       "     reconstructions_mean_latent_features_10  \\\n",
       "0                                  -0.082227   \n",
       "1                                   0.593477   \n",
       "2                                   0.440064   \n",
       "3                                   0.711714   \n",
       "4                                   0.698043   \n",
       "..                                       ...   \n",
       "295                                 0.373231   \n",
       "296                                 0.444984   \n",
       "297                                 0.588437   \n",
       "298                                 0.545585   \n",
       "299                                 0.612947   \n",
       "\n",
       "     reconstructions_mean_latent_features_11  \\\n",
       "0                                   0.352146   \n",
       "1                                   0.432118   \n",
       "2                                   0.413630   \n",
       "3                                   0.656565   \n",
       "4                                   0.608211   \n",
       "..                                       ...   \n",
       "295                                 0.281610   \n",
       "296                                 0.437084   \n",
       "297                                 0.533188   \n",
       "298                                 0.495606   \n",
       "299                                 0.565084   \n",
       "\n",
       "     reconstructions_mean_latent_features_12  \\\n",
       "0                                   0.786789   \n",
       "1                                   0.357133   \n",
       "2                                   0.400986   \n",
       "3                                   0.538123   \n",
       "4                                   0.469882   \n",
       "..                                       ...   \n",
       "295                                 0.287987   \n",
       "296                                 0.443345   \n",
       "297                                 0.476712   \n",
       "298                                 0.447903   \n",
       "299                                 0.466685   \n",
       "\n",
       "     reconstructions_mean_latent_features_13  \\\n",
       "0                                   0.181245   \n",
       "1                                   0.118143   \n",
       "2                                   0.135690   \n",
       "3                                  -0.054643   \n",
       "4                                  -0.016209   \n",
       "..                                       ...   \n",
       "295                                 0.239766   \n",
       "296                                 0.144968   \n",
       "297                                 0.038410   \n",
       "298                                 0.052618   \n",
       "299                                -0.027274   \n",
       "\n",
       "     reconstructions_mean_latent_features_14  \\\n",
       "0                                  -0.725199   \n",
       "1                                  -0.396401   \n",
       "2                                  -0.418351   \n",
       "3                                  -0.196332   \n",
       "4                                  -0.160862   \n",
       "..                                       ...   \n",
       "295                                -0.599871   \n",
       "296                                -0.482219   \n",
       "297                                -0.324567   \n",
       "298                                -0.399251   \n",
       "299                                -0.283880   \n",
       "\n",
       "     reconstructions_mean_latent_features_15  \\\n",
       "0                                  -0.668880   \n",
       "1                                  -0.241968   \n",
       "2                                  -0.236989   \n",
       "3                                  -0.076617   \n",
       "4                                  -0.016941   \n",
       "..                                       ...   \n",
       "295                                -0.393992   \n",
       "296                                -0.303066   \n",
       "297                                -0.181855   \n",
       "298                                -0.198518   \n",
       "299                                -0.074148   \n",
       "\n",
       "     reconstructions_mean_latent_features_16  \\\n",
       "0                                   0.253682   \n",
       "1                                   0.561575   \n",
       "2                                   0.402316   \n",
       "3                                   0.599034   \n",
       "4                                   0.559579   \n",
       "..                                       ...   \n",
       "295                                 0.363112   \n",
       "296                                 0.420591   \n",
       "297                                 0.501890   \n",
       "298                                 0.431417   \n",
       "299                                 0.427189   \n",
       "\n",
       "     reconstructions_mean_latent_features_17  \\\n",
       "0                                   0.315962   \n",
       "1                                   0.709550   \n",
       "2                                   0.329611   \n",
       "3                                   0.445602   \n",
       "4                                   0.351703   \n",
       "..                                       ...   \n",
       "295                                 0.473857   \n",
       "296                                 0.399034   \n",
       "297                                 0.447872   \n",
       "298                                 0.386331   \n",
       "299                                 0.084883   \n",
       "\n",
       "     reconstructions_mean_latent_features_18  \\\n",
       "0                                   0.589892   \n",
       "1                                   0.691831   \n",
       "2                                   0.657517   \n",
       "3                                   0.639521   \n",
       "4                                   0.597816   \n",
       "..                                       ...   \n",
       "295                                 0.775260   \n",
       "296                                 0.710684   \n",
       "297                                 0.685958   \n",
       "298                                 0.756053   \n",
       "299                                 0.687125   \n",
       "\n",
       "     reconstructions_mean_latent_features_19  \\\n",
       "0                                   0.212861   \n",
       "1                                   0.556354   \n",
       "2                                   0.392622   \n",
       "3                                   0.643435   \n",
       "4                                   0.595374   \n",
       "..                                       ...   \n",
       "295                                 0.342774   \n",
       "296                                 0.413561   \n",
       "297                                 0.522313   \n",
       "298                                 0.415958   \n",
       "299                                 0.500243   \n",
       "\n",
       "     reconstructions_mean_latent_features_20  \\\n",
       "0                                   0.284910   \n",
       "1                                   0.085471   \n",
       "2                                   0.233974   \n",
       "3                                   0.317581   \n",
       "4                                   0.312439   \n",
       "..                                       ...   \n",
       "295                                 0.081585   \n",
       "296                                 0.223634   \n",
       "297                                 0.240465   \n",
       "298                                 0.234272   \n",
       "299                                 0.316556   \n",
       "\n",
       "     reconstructions_mean_latent_features_21  \\\n",
       "0                                   0.750360   \n",
       "1                                   0.667678   \n",
       "2                                   0.523237   \n",
       "3                                   0.574855   \n",
       "4                                   0.511031   \n",
       "..                                       ...   \n",
       "295                                 0.606319   \n",
       "296                                 0.578219   \n",
       "297                                 0.582773   \n",
       "298                                 0.486071   \n",
       "299                                 0.512957   \n",
       "\n",
       "     reconstructions_mean_latent_features_22  \\\n",
       "0                                   0.456047   \n",
       "1                                   0.544193   \n",
       "2                                   0.546441   \n",
       "3                                   0.386058   \n",
       "4                                   0.411958   \n",
       "..                                       ...   \n",
       "295                                 0.626860   \n",
       "296                                 0.562926   \n",
       "297                                 0.456203   \n",
       "298                                 0.537504   \n",
       "299                                 0.278132   \n",
       "\n",
       "     latent_representations_latent_features_0  \\\n",
       "0                                    0.025644   \n",
       "1                                   -0.466960   \n",
       "2                                   -0.470165   \n",
       "3                                   -0.706799   \n",
       "4                                   -0.700036   \n",
       "..                                        ...   \n",
       "295                                 -0.231496   \n",
       "296                                 -0.367954   \n",
       "297                                 -0.452632   \n",
       "298                                 -0.468846   \n",
       "299                                 -0.537599   \n",
       "\n",
       "     latent_representations_latent_features_1  \\\n",
       "0                                   -0.059350   \n",
       "1                                    0.271174   \n",
       "2                                    0.293028   \n",
       "3                                    0.524848   \n",
       "4                                    0.580416   \n",
       "..                                        ...   \n",
       "295                                  0.052324   \n",
       "296                                  0.149798   \n",
       "297                                  0.369651   \n",
       "298                                  0.328998   \n",
       "299                                  0.484361   \n",
       "\n",
       "     latent_representations_latent_features_2  \\\n",
       "0                                   -0.295345   \n",
       "1                                    0.382906   \n",
       "2                                    0.375645   \n",
       "3                                    0.751846   \n",
       "4                                    0.774593   \n",
       "..                                        ...   \n",
       "295                                  0.166159   \n",
       "296                                  0.364161   \n",
       "297                                  0.532978   \n",
       "298                                  0.535894   \n",
       "299                                  0.643885   \n",
       "\n",
       "     latent_representations_latent_features_3  \\\n",
       "0                                   -0.156371   \n",
       "1                                   -0.666963   \n",
       "2                                   -0.673999   \n",
       "3                                   -0.971888   \n",
       "4                                   -1.004546   \n",
       "..                                        ...   \n",
       "295                                 -0.328720   \n",
       "296                                 -0.473505   \n",
       "297                                 -0.663555   \n",
       "298                                 -0.655420   \n",
       "299                                 -0.818383   \n",
       "\n",
       "     latent_representations_latent_features_4  \\\n",
       "0                                    0.089071   \n",
       "1                                    0.448975   \n",
       "2                                    0.433263   \n",
       "3                                    0.631818   \n",
       "4                                    0.639794   \n",
       "..                                        ...   \n",
       "295                                  0.132949   \n",
       "296                                  0.219472   \n",
       "297                                  0.314323   \n",
       "298                                  0.336354   \n",
       "299                                  0.460280   \n",
       "\n",
       "     latent_representations_latent_features_5  \\\n",
       "0                                    0.074201   \n",
       "1                                   -0.493218   \n",
       "2                                   -0.466943   \n",
       "3                                   -0.753402   \n",
       "4                                   -0.728963   \n",
       "..                                        ...   \n",
       "295                                 -0.314295   \n",
       "296                                 -0.484520   \n",
       "297                                 -0.562679   \n",
       "298                                 -0.584057   \n",
       "299                                 -0.628634   \n",
       "\n",
       "     reconstructions_mean_latent_features_0  \\\n",
       "0                                  0.924081   \n",
       "1                                  0.727449   \n",
       "2                                  0.615858   \n",
       "3                                  0.527854   \n",
       "4                                  0.463161   \n",
       "..                                      ...   \n",
       "295                                0.799040   \n",
       "296                                0.718950   \n",
       "297                                0.624850   \n",
       "298                                0.620974   \n",
       "299                                0.561604   \n",
       "\n",
       "     reconstructions_mean_latent_features_1  \\\n",
       "0                                 -0.135590   \n",
       "1                                  0.151623   \n",
       "2                                 -0.015094   \n",
       "3                                  0.061934   \n",
       "4                                  0.047286   \n",
       "..                                      ...   \n",
       "295                                0.059592   \n",
       "296                                0.008564   \n",
       "297                                0.056137   \n",
       "298                                0.003399   \n",
       "299                                0.054388   \n",
       "\n",
       "     reconstructions_mean_latent_features_2  \\\n",
       "0                                  0.499637   \n",
       "1                                 -0.271612   \n",
       "2                                  0.043652   \n",
       "3                                  0.025055   \n",
       "4                                  0.021042   \n",
       "..                                      ...   \n",
       "295                               -0.156794   \n",
       "296                                0.028018   \n",
       "297                               -0.011949   \n",
       "298                                0.024119   \n",
       "299                                0.056957   \n",
       "\n",
       "     reconstructions_mean_latent_features_3  \\\n",
       "0                                  0.241374   \n",
       "1                                 -0.256591   \n",
       "2                                  0.213557   \n",
       "3                                 -0.068244   \n",
       "4                                  0.056516   \n",
       "..                                      ...   \n",
       "295                                0.022642   \n",
       "296                                0.129240   \n",
       "297                               -0.026962   \n",
       "298                                0.116367   \n",
       "299                               -0.025368   \n",
       "\n",
       "     reconstructions_mean_latent_features_4  \\\n",
       "0                                 -0.017618   \n",
       "1                                  0.490845   \n",
       "2                                  0.288134   \n",
       "3                                  0.514123   \n",
       "4                                  0.490920   \n",
       "..                                      ...   \n",
       "295                                0.224824   \n",
       "296                                0.285359   \n",
       "297                                0.415428   \n",
       "298                                0.349870   \n",
       "299                                0.228774   \n",
       "\n",
       "     reconstructions_mean_latent_features_5  \\\n",
       "0                                  0.693070   \n",
       "1                                  1.015347   \n",
       "2                                  0.883730   \n",
       "3                                  0.804582   \n",
       "4                                  0.795669   \n",
       "..                                      ...   \n",
       "295                                1.048931   \n",
       "296                                0.953450   \n",
       "297                                0.877308   \n",
       "298                                0.908796   \n",
       "299                                0.717402   \n",
       "\n",
       "     reconstructions_mean_latent_features_6  \\\n",
       "0                                 -0.296043   \n",
       "1                                  0.207021   \n",
       "2                                 -0.065764   \n",
       "3                                  0.102690   \n",
       "4                                  0.052190   \n",
       "..                                      ...   \n",
       "295                               -0.006963   \n",
       "296                               -0.041098   \n",
       "297                                0.064062   \n",
       "298                                0.039865   \n",
       "299                               -0.082200   \n",
       "\n",
       "     reconstructions_mean_latent_features_7  \\\n",
       "0                                  0.333671   \n",
       "1                                  0.011632   \n",
       "2                                  0.124521   \n",
       "3                                  0.296015   \n",
       "4                                  0.263364   \n",
       "..                                      ...   \n",
       "295                               -0.072168   \n",
       "296                                0.102429   \n",
       "297                                0.184376   \n",
       "298                                0.124301   \n",
       "299                                0.208895   \n",
       "\n",
       "     reconstructions_mean_latent_features_8  \\\n",
       "0                                  0.215816   \n",
       "1                                  0.392299   \n",
       "2                                  0.471429   \n",
       "3                                  0.670417   \n",
       "4                                  0.665853   \n",
       "..                                      ...   \n",
       "295                                0.292208   \n",
       "296                                0.455793   \n",
       "297                                0.549605   \n",
       "298                                0.585564   \n",
       "299                                0.641069   \n",
       "\n",
       "     reconstructions_mean_latent_features_9  \\\n",
       "0                                 -0.632744   \n",
       "1                                 -0.386213   \n",
       "2                                 -0.453050   \n",
       "3                                 -0.188862   \n",
       "4                                 -0.185761   \n",
       "..                                      ...   \n",
       "295                               -0.620655   \n",
       "296                               -0.516891   \n",
       "297                               -0.335890   \n",
       "298                               -0.473440   \n",
       "299                               -0.329810   \n",
       "\n",
       "     reconstructions_mean_latent_features_10  \\\n",
       "0                                  -0.082227   \n",
       "1                                   0.593477   \n",
       "2                                   0.440064   \n",
       "3                                   0.711714   \n",
       "4                                   0.698043   \n",
       "..                                       ...   \n",
       "295                                 0.373231   \n",
       "296                                 0.444984   \n",
       "297                                 0.588437   \n",
       "298                                 0.545585   \n",
       "299                                 0.612947   \n",
       "\n",
       "     reconstructions_mean_latent_features_11  \\\n",
       "0                                   0.352146   \n",
       "1                                   0.432118   \n",
       "2                                   0.413630   \n",
       "3                                   0.656565   \n",
       "4                                   0.608211   \n",
       "..                                       ...   \n",
       "295                                 0.281610   \n",
       "296                                 0.437084   \n",
       "297                                 0.533188   \n",
       "298                                 0.495606   \n",
       "299                                 0.565084   \n",
       "\n",
       "     reconstructions_mean_latent_features_12  \\\n",
       "0                                   0.786789   \n",
       "1                                   0.357133   \n",
       "2                                   0.400986   \n",
       "3                                   0.538123   \n",
       "4                                   0.469882   \n",
       "..                                       ...   \n",
       "295                                 0.287987   \n",
       "296                                 0.443345   \n",
       "297                                 0.476712   \n",
       "298                                 0.447903   \n",
       "299                                 0.466685   \n",
       "\n",
       "     reconstructions_mean_latent_features_13  \\\n",
       "0                                   0.181245   \n",
       "1                                   0.118143   \n",
       "2                                   0.135690   \n",
       "3                                  -0.054643   \n",
       "4                                  -0.016209   \n",
       "..                                       ...   \n",
       "295                                 0.239766   \n",
       "296                                 0.144968   \n",
       "297                                 0.038410   \n",
       "298                                 0.052618   \n",
       "299                                -0.027274   \n",
       "\n",
       "     reconstructions_mean_latent_features_14  \\\n",
       "0                                  -0.725199   \n",
       "1                                  -0.396401   \n",
       "2                                  -0.418351   \n",
       "3                                  -0.196332   \n",
       "4                                  -0.160862   \n",
       "..                                       ...   \n",
       "295                                -0.599871   \n",
       "296                                -0.482219   \n",
       "297                                -0.324567   \n",
       "298                                -0.399251   \n",
       "299                                -0.283880   \n",
       "\n",
       "     reconstructions_mean_latent_features_15  \\\n",
       "0                                  -0.668880   \n",
       "1                                  -0.241968   \n",
       "2                                  -0.236989   \n",
       "3                                  -0.076617   \n",
       "4                                  -0.016941   \n",
       "..                                       ...   \n",
       "295                                -0.393992   \n",
       "296                                -0.303066   \n",
       "297                                -0.181855   \n",
       "298                                -0.198518   \n",
       "299                                -0.074148   \n",
       "\n",
       "     reconstructions_mean_latent_features_16  \\\n",
       "0                                   0.253682   \n",
       "1                                   0.561575   \n",
       "2                                   0.402316   \n",
       "3                                   0.599034   \n",
       "4                                   0.559579   \n",
       "..                                       ...   \n",
       "295                                 0.363112   \n",
       "296                                 0.420591   \n",
       "297                                 0.501890   \n",
       "298                                 0.431417   \n",
       "299                                 0.427189   \n",
       "\n",
       "     reconstructions_mean_latent_features_17  \\\n",
       "0                                   0.315962   \n",
       "1                                   0.709550   \n",
       "2                                   0.329611   \n",
       "3                                   0.445602   \n",
       "4                                   0.351703   \n",
       "..                                       ...   \n",
       "295                                 0.473857   \n",
       "296                                 0.399034   \n",
       "297                                 0.447872   \n",
       "298                                 0.386331   \n",
       "299                                 0.084883   \n",
       "\n",
       "     reconstructions_mean_latent_features_18  \\\n",
       "0                                   0.589892   \n",
       "1                                   0.691831   \n",
       "2                                   0.657517   \n",
       "3                                   0.639521   \n",
       "4                                   0.597816   \n",
       "..                                       ...   \n",
       "295                                 0.775260   \n",
       "296                                 0.710684   \n",
       "297                                 0.685958   \n",
       "298                                 0.756053   \n",
       "299                                 0.687125   \n",
       "\n",
       "     reconstructions_mean_latent_features_19  \\\n",
       "0                                   0.212861   \n",
       "1                                   0.556354   \n",
       "2                                   0.392622   \n",
       "3                                   0.643435   \n",
       "4                                   0.595374   \n",
       "..                                       ...   \n",
       "295                                 0.342774   \n",
       "296                                 0.413561   \n",
       "297                                 0.522313   \n",
       "298                                 0.415958   \n",
       "299                                 0.500243   \n",
       "\n",
       "     reconstructions_mean_latent_features_20  \\\n",
       "0                                   0.284910   \n",
       "1                                   0.085471   \n",
       "2                                   0.233974   \n",
       "3                                   0.317581   \n",
       "4                                   0.312439   \n",
       "..                                       ...   \n",
       "295                                 0.081585   \n",
       "296                                 0.223634   \n",
       "297                                 0.240465   \n",
       "298                                 0.234272   \n",
       "299                                 0.316556   \n",
       "\n",
       "     reconstructions_mean_latent_features_21  \\\n",
       "0                                   0.750360   \n",
       "1                                   0.667678   \n",
       "2                                   0.523237   \n",
       "3                                   0.574855   \n",
       "4                                   0.511031   \n",
       "..                                       ...   \n",
       "295                                 0.606319   \n",
       "296                                 0.578219   \n",
       "297                                 0.582773   \n",
       "298                                 0.486071   \n",
       "299                                 0.512957   \n",
       "\n",
       "     reconstructions_mean_latent_features_22  \\\n",
       "0                                   0.456047   \n",
       "1                                   0.544193   \n",
       "2                                   0.546441   \n",
       "3                                   0.386058   \n",
       "4                                   0.411958   \n",
       "..                                       ...   \n",
       "295                                 0.626860   \n",
       "296                                 0.562926   \n",
       "297                                 0.456203   \n",
       "298                                 0.537504   \n",
       "299                                 0.278132   \n",
       "\n",
       "     latent_representations_latent_features_0  \\\n",
       "0                                    0.025644   \n",
       "1                                   -0.466960   \n",
       "2                                   -0.470165   \n",
       "3                                   -0.706799   \n",
       "4                                   -0.700036   \n",
       "..                                        ...   \n",
       "295                                 -0.231496   \n",
       "296                                 -0.367954   \n",
       "297                                 -0.452632   \n",
       "298                                 -0.468846   \n",
       "299                                 -0.537599   \n",
       "\n",
       "     latent_representations_latent_features_1  \\\n",
       "0                                   -0.059350   \n",
       "1                                    0.271174   \n",
       "2                                    0.293028   \n",
       "3                                    0.524848   \n",
       "4                                    0.580416   \n",
       "..                                        ...   \n",
       "295                                  0.052324   \n",
       "296                                  0.149798   \n",
       "297                                  0.369651   \n",
       "298                                  0.328998   \n",
       "299                                  0.484361   \n",
       "\n",
       "     latent_representations_latent_features_2  \\\n",
       "0                                   -0.295345   \n",
       "1                                    0.382906   \n",
       "2                                    0.375645   \n",
       "3                                    0.751846   \n",
       "4                                    0.774593   \n",
       "..                                        ...   \n",
       "295                                  0.166159   \n",
       "296                                  0.364161   \n",
       "297                                  0.532978   \n",
       "298                                  0.535894   \n",
       "299                                  0.643885   \n",
       "\n",
       "     latent_representations_latent_features_3  \\\n",
       "0                                   -0.156371   \n",
       "1                                   -0.666963   \n",
       "2                                   -0.673999   \n",
       "3                                   -0.971888   \n",
       "4                                   -1.004546   \n",
       "..                                        ...   \n",
       "295                                 -0.328720   \n",
       "296                                 -0.473505   \n",
       "297                                 -0.663555   \n",
       "298                                 -0.655420   \n",
       "299                                 -0.818383   \n",
       "\n",
       "     latent_representations_latent_features_4  \\\n",
       "0                                    0.089071   \n",
       "1                                    0.448975   \n",
       "2                                    0.433263   \n",
       "3                                    0.631818   \n",
       "4                                    0.639794   \n",
       "..                                        ...   \n",
       "295                                  0.132949   \n",
       "296                                  0.219472   \n",
       "297                                  0.314323   \n",
       "298                                  0.336354   \n",
       "299                                  0.460280   \n",
       "\n",
       "     latent_representations_latent_features_5  \n",
       "0                                    0.074201  \n",
       "1                                   -0.493218  \n",
       "2                                   -0.466943  \n",
       "3                                   -0.753402  \n",
       "4                                   -0.728963  \n",
       "..                                        ...  \n",
       "295                                 -0.314295  \n",
       "296                                 -0.484520  \n",
       "297                                 -0.562679  \n",
       "298                                 -0.584057  \n",
       "299                                 -0.628634  \n",
       "\n",
       "[300 rows x 62 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'latent_representations': array([[ 0.7802254 , -0.10729223,  0.31457698, ...,  0.31983497,\n",
       "          0.27089506,  1.10672879],\n",
       "        [ 0.98883927,  0.09547198,  0.41578778, ..., -0.02283099,\n",
       "          0.01203567,  0.81264138],\n",
       "        [ 1.01293802, -0.41170257,  0.10397224, ..., -0.57779823,\n",
       "         -0.52440584,  0.75147319],\n",
       "        [-1.05199564, -0.26685005, -0.6365966 , ..., -0.62019366,\n",
       "         -0.58028763, -1.31965804],\n",
       "        [-0.9647001 ,  0.4631204 , -0.04750307, ...,  0.66197101,\n",
       "          0.60248911, -0.67978966],\n",
       "        [ 1.112046  ,  0.14413276,  0.56184111, ...,  0.37602215,\n",
       "          0.35842338,  1.25383401]]),\n",
       " 'reconstructions_mean': array([[-0.43439987,  0.55227272,  0.25557916, ...,  0.32192115,\n",
       "          0.61953512,  0.74356216],\n",
       "        [-1.8718406 , -0.63063285, -0.69196452, ..., -0.64851837,\n",
       "         -0.55238891, -0.20118597],\n",
       "        [-2.08274555, -0.72016533,  0.07728161, ..., -0.20354194,\n",
       "         -0.50705667,  2.21474838],\n",
       "        ...,\n",
       "        [-0.22628234,  0.41729324,  0.17113061, ...,  0.36344143,\n",
       "          0.47307926,  0.15760511],\n",
       "        [-0.0852127 ,  0.05987076, -0.20191567, ...,  0.0715631 ,\n",
       "          0.02784714, -0.85424185],\n",
       "        [-0.0083544 , -0.08580844,  0.10708154, ...,  0.26318133,\n",
       "         -0.01424614, -0.07141042]]),\n",
       " 'euclidean_errors_mean': array([ 0.98883927,  0.09547198,  0.41578778, ..., -0.02283099,\n",
       "         0.01203567,  0.81264138]),\n",
       " 'cosine_errors_mean': array([ 1.01293802, -0.41170257,  0.10397224, ..., -0.57779823,\n",
       "        -0.52440584,  0.75147319])}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detectors.prediction_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4818,)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detectors.prediction_details[\"cosine_errors_mean\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4818,)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detectors.prediction_details[\"euclidean_errors_mean\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 4818)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detectors.prediction_details[\"reconstructions_mean\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 4818)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detectors.prediction_details[\"latent_representations\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 23)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = detectors.prediction_details[\"reconstructions_mean\"]\n",
    "reconstructions_mean = pd.DataFrame(data.T,\n",
    "             columns = [\"reconstructions_mean_latent_features_{}\".format(i) for i in range(data.shape[0])]\n",
    "            )\n",
    "reconstructions_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 6)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = detectors.prediction_details[\"latent_representations\"]\n",
    "latent_representations = pd.DataFrame(data.T,\n",
    "             columns = [\"latent_representations_latent_features_{}\".format(i) for i in range(data.shape[0])]\n",
    "            )\n",
    "latent_representations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cano_locdt_index</th>\n",
       "      <th>score</th>\n",
       "      <th>cosine_errors_mean</th>\n",
       "      <th>euclidean_errors_mean</th>\n",
       "      <th>reconstructions_mean_latent_features_0</th>\n",
       "      <th>reconstructions_mean_latent_features_1</th>\n",
       "      <th>reconstructions_mean_latent_features_2</th>\n",
       "      <th>reconstructions_mean_latent_features_3</th>\n",
       "      <th>reconstructions_mean_latent_features_4</th>\n",
       "      <th>reconstructions_mean_latent_features_5</th>\n",
       "      <th>reconstructions_mean_latent_features_6</th>\n",
       "      <th>reconstructions_mean_latent_features_7</th>\n",
       "      <th>reconstructions_mean_latent_features_8</th>\n",
       "      <th>reconstructions_mean_latent_features_9</th>\n",
       "      <th>reconstructions_mean_latent_features_10</th>\n",
       "      <th>reconstructions_mean_latent_features_11</th>\n",
       "      <th>reconstructions_mean_latent_features_12</th>\n",
       "      <th>reconstructions_mean_latent_features_13</th>\n",
       "      <th>reconstructions_mean_latent_features_14</th>\n",
       "      <th>reconstructions_mean_latent_features_15</th>\n",
       "      <th>reconstructions_mean_latent_features_16</th>\n",
       "      <th>reconstructions_mean_latent_features_17</th>\n",
       "      <th>reconstructions_mean_latent_features_18</th>\n",
       "      <th>reconstructions_mean_latent_features_19</th>\n",
       "      <th>reconstructions_mean_latent_features_20</th>\n",
       "      <th>reconstructions_mean_latent_features_21</th>\n",
       "      <th>reconstructions_mean_latent_features_22</th>\n",
       "      <th>latent_representations_latent_features_0</th>\n",
       "      <th>latent_representations_latent_features_1</th>\n",
       "      <th>latent_representations_latent_features_2</th>\n",
       "      <th>latent_representations_latent_features_3</th>\n",
       "      <th>latent_representations_latent_features_4</th>\n",
       "      <th>latent_representations_latent_features_5</th>\n",
       "      <th>reconstructions_mean_latent_features_0</th>\n",
       "      <th>reconstructions_mean_latent_features_1</th>\n",
       "      <th>reconstructions_mean_latent_features_2</th>\n",
       "      <th>reconstructions_mean_latent_features_3</th>\n",
       "      <th>reconstructions_mean_latent_features_4</th>\n",
       "      <th>reconstructions_mean_latent_features_5</th>\n",
       "      <th>reconstructions_mean_latent_features_6</th>\n",
       "      <th>reconstructions_mean_latent_features_7</th>\n",
       "      <th>reconstructions_mean_latent_features_8</th>\n",
       "      <th>reconstructions_mean_latent_features_9</th>\n",
       "      <th>reconstructions_mean_latent_features_10</th>\n",
       "      <th>reconstructions_mean_latent_features_11</th>\n",
       "      <th>reconstructions_mean_latent_features_12</th>\n",
       "      <th>reconstructions_mean_latent_features_13</th>\n",
       "      <th>reconstructions_mean_latent_features_14</th>\n",
       "      <th>reconstructions_mean_latent_features_15</th>\n",
       "      <th>reconstructions_mean_latent_features_16</th>\n",
       "      <th>reconstructions_mean_latent_features_17</th>\n",
       "      <th>reconstructions_mean_latent_features_18</th>\n",
       "      <th>reconstructions_mean_latent_features_19</th>\n",
       "      <th>reconstructions_mean_latent_features_20</th>\n",
       "      <th>reconstructions_mean_latent_features_21</th>\n",
       "      <th>reconstructions_mean_latent_features_22</th>\n",
       "      <th>latent_representations_latent_features_0</th>\n",
       "      <th>latent_representations_latent_features_1</th>\n",
       "      <th>latent_representations_latent_features_2</th>\n",
       "      <th>latent_representations_latent_features_3</th>\n",
       "      <th>latent_representations_latent_features_4</th>\n",
       "      <th>latent_representations_latent_features_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.631021</td>\n",
       "      <td>-0.295345</td>\n",
       "      <td>-0.059350</td>\n",
       "      <td>0.924081</td>\n",
       "      <td>-0.135590</td>\n",
       "      <td>0.499637</td>\n",
       "      <td>0.241374</td>\n",
       "      <td>-0.017618</td>\n",
       "      <td>0.693070</td>\n",
       "      <td>-0.296043</td>\n",
       "      <td>0.333671</td>\n",
       "      <td>0.215816</td>\n",
       "      <td>-0.632744</td>\n",
       "      <td>-0.082227</td>\n",
       "      <td>0.352146</td>\n",
       "      <td>0.786789</td>\n",
       "      <td>0.181245</td>\n",
       "      <td>-0.725199</td>\n",
       "      <td>-0.668880</td>\n",
       "      <td>0.253682</td>\n",
       "      <td>0.315962</td>\n",
       "      <td>0.589892</td>\n",
       "      <td>0.212861</td>\n",
       "      <td>0.284910</td>\n",
       "      <td>0.750360</td>\n",
       "      <td>0.456047</td>\n",
       "      <td>0.025644</td>\n",
       "      <td>-0.059350</td>\n",
       "      <td>-0.295345</td>\n",
       "      <td>-0.156371</td>\n",
       "      <td>0.089071</td>\n",
       "      <td>0.074201</td>\n",
       "      <td>0.924081</td>\n",
       "      <td>-0.135590</td>\n",
       "      <td>0.499637</td>\n",
       "      <td>0.241374</td>\n",
       "      <td>-0.017618</td>\n",
       "      <td>0.693070</td>\n",
       "      <td>-0.296043</td>\n",
       "      <td>0.333671</td>\n",
       "      <td>0.215816</td>\n",
       "      <td>-0.632744</td>\n",
       "      <td>-0.082227</td>\n",
       "      <td>0.352146</td>\n",
       "      <td>0.786789</td>\n",
       "      <td>0.181245</td>\n",
       "      <td>-0.725199</td>\n",
       "      <td>-0.668880</td>\n",
       "      <td>0.253682</td>\n",
       "      <td>0.315962</td>\n",
       "      <td>0.589892</td>\n",
       "      <td>0.212861</td>\n",
       "      <td>0.284910</td>\n",
       "      <td>0.750360</td>\n",
       "      <td>0.456047</td>\n",
       "      <td>0.025644</td>\n",
       "      <td>-0.059350</td>\n",
       "      <td>-0.295345</td>\n",
       "      <td>-0.156371</td>\n",
       "      <td>0.089071</td>\n",
       "      <td>0.074201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27.631021</td>\n",
       "      <td>0.382906</td>\n",
       "      <td>0.271174</td>\n",
       "      <td>0.727449</td>\n",
       "      <td>0.151623</td>\n",
       "      <td>-0.271612</td>\n",
       "      <td>-0.256591</td>\n",
       "      <td>0.490845</td>\n",
       "      <td>1.015347</td>\n",
       "      <td>0.207021</td>\n",
       "      <td>0.011632</td>\n",
       "      <td>0.392299</td>\n",
       "      <td>-0.386213</td>\n",
       "      <td>0.593477</td>\n",
       "      <td>0.432118</td>\n",
       "      <td>0.357133</td>\n",
       "      <td>0.118143</td>\n",
       "      <td>-0.396401</td>\n",
       "      <td>-0.241968</td>\n",
       "      <td>0.561575</td>\n",
       "      <td>0.709550</td>\n",
       "      <td>0.691831</td>\n",
       "      <td>0.556354</td>\n",
       "      <td>0.085471</td>\n",
       "      <td>0.667678</td>\n",
       "      <td>0.544193</td>\n",
       "      <td>-0.466960</td>\n",
       "      <td>0.271174</td>\n",
       "      <td>0.382906</td>\n",
       "      <td>-0.666963</td>\n",
       "      <td>0.448975</td>\n",
       "      <td>-0.493218</td>\n",
       "      <td>0.727449</td>\n",
       "      <td>0.151623</td>\n",
       "      <td>-0.271612</td>\n",
       "      <td>-0.256591</td>\n",
       "      <td>0.490845</td>\n",
       "      <td>1.015347</td>\n",
       "      <td>0.207021</td>\n",
       "      <td>0.011632</td>\n",
       "      <td>0.392299</td>\n",
       "      <td>-0.386213</td>\n",
       "      <td>0.593477</td>\n",
       "      <td>0.432118</td>\n",
       "      <td>0.357133</td>\n",
       "      <td>0.118143</td>\n",
       "      <td>-0.396401</td>\n",
       "      <td>-0.241968</td>\n",
       "      <td>0.561575</td>\n",
       "      <td>0.709550</td>\n",
       "      <td>0.691831</td>\n",
       "      <td>0.556354</td>\n",
       "      <td>0.085471</td>\n",
       "      <td>0.667678</td>\n",
       "      <td>0.544193</td>\n",
       "      <td>-0.466960</td>\n",
       "      <td>0.271174</td>\n",
       "      <td>0.382906</td>\n",
       "      <td>-0.666963</td>\n",
       "      <td>0.448975</td>\n",
       "      <td>-0.493218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3749_1</td>\n",
       "      <td>25.149796</td>\n",
       "      <td>0.375645</td>\n",
       "      <td>0.293028</td>\n",
       "      <td>0.615858</td>\n",
       "      <td>-0.015094</td>\n",
       "      <td>0.043652</td>\n",
       "      <td>0.213557</td>\n",
       "      <td>0.288134</td>\n",
       "      <td>0.883730</td>\n",
       "      <td>-0.065764</td>\n",
       "      <td>0.124521</td>\n",
       "      <td>0.471429</td>\n",
       "      <td>-0.453050</td>\n",
       "      <td>0.440064</td>\n",
       "      <td>0.413630</td>\n",
       "      <td>0.400986</td>\n",
       "      <td>0.135690</td>\n",
       "      <td>-0.418351</td>\n",
       "      <td>-0.236989</td>\n",
       "      <td>0.402316</td>\n",
       "      <td>0.329611</td>\n",
       "      <td>0.657517</td>\n",
       "      <td>0.392622</td>\n",
       "      <td>0.233974</td>\n",
       "      <td>0.523237</td>\n",
       "      <td>0.546441</td>\n",
       "      <td>-0.470165</td>\n",
       "      <td>0.293028</td>\n",
       "      <td>0.375645</td>\n",
       "      <td>-0.673999</td>\n",
       "      <td>0.433263</td>\n",
       "      <td>-0.466943</td>\n",
       "      <td>0.615858</td>\n",
       "      <td>-0.015094</td>\n",
       "      <td>0.043652</td>\n",
       "      <td>0.213557</td>\n",
       "      <td>0.288134</td>\n",
       "      <td>0.883730</td>\n",
       "      <td>-0.065764</td>\n",
       "      <td>0.124521</td>\n",
       "      <td>0.471429</td>\n",
       "      <td>-0.453050</td>\n",
       "      <td>0.440064</td>\n",
       "      <td>0.413630</td>\n",
       "      <td>0.400986</td>\n",
       "      <td>0.135690</td>\n",
       "      <td>-0.418351</td>\n",
       "      <td>-0.236989</td>\n",
       "      <td>0.402316</td>\n",
       "      <td>0.329611</td>\n",
       "      <td>0.657517</td>\n",
       "      <td>0.392622</td>\n",
       "      <td>0.233974</td>\n",
       "      <td>0.523237</td>\n",
       "      <td>0.546441</td>\n",
       "      <td>-0.470165</td>\n",
       "      <td>0.293028</td>\n",
       "      <td>0.375645</td>\n",
       "      <td>-0.673999</td>\n",
       "      <td>0.433263</td>\n",
       "      <td>-0.466943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>14.849816</td>\n",
       "      <td>0.751846</td>\n",
       "      <td>0.524848</td>\n",
       "      <td>0.527854</td>\n",
       "      <td>0.061934</td>\n",
       "      <td>0.025055</td>\n",
       "      <td>-0.068244</td>\n",
       "      <td>0.514123</td>\n",
       "      <td>0.804582</td>\n",
       "      <td>0.102690</td>\n",
       "      <td>0.296015</td>\n",
       "      <td>0.670417</td>\n",
       "      <td>-0.188862</td>\n",
       "      <td>0.711714</td>\n",
       "      <td>0.656565</td>\n",
       "      <td>0.538123</td>\n",
       "      <td>-0.054643</td>\n",
       "      <td>-0.196332</td>\n",
       "      <td>-0.076617</td>\n",
       "      <td>0.599034</td>\n",
       "      <td>0.445602</td>\n",
       "      <td>0.639521</td>\n",
       "      <td>0.643435</td>\n",
       "      <td>0.317581</td>\n",
       "      <td>0.574855</td>\n",
       "      <td>0.386058</td>\n",
       "      <td>-0.706799</td>\n",
       "      <td>0.524848</td>\n",
       "      <td>0.751846</td>\n",
       "      <td>-0.971888</td>\n",
       "      <td>0.631818</td>\n",
       "      <td>-0.753402</td>\n",
       "      <td>0.527854</td>\n",
       "      <td>0.061934</td>\n",
       "      <td>0.025055</td>\n",
       "      <td>-0.068244</td>\n",
       "      <td>0.514123</td>\n",
       "      <td>0.804582</td>\n",
       "      <td>0.102690</td>\n",
       "      <td>0.296015</td>\n",
       "      <td>0.670417</td>\n",
       "      <td>-0.188862</td>\n",
       "      <td>0.711714</td>\n",
       "      <td>0.656565</td>\n",
       "      <td>0.538123</td>\n",
       "      <td>-0.054643</td>\n",
       "      <td>-0.196332</td>\n",
       "      <td>-0.076617</td>\n",
       "      <td>0.599034</td>\n",
       "      <td>0.445602</td>\n",
       "      <td>0.639521</td>\n",
       "      <td>0.643435</td>\n",
       "      <td>0.317581</td>\n",
       "      <td>0.574855</td>\n",
       "      <td>0.386058</td>\n",
       "      <td>-0.706799</td>\n",
       "      <td>0.524848</td>\n",
       "      <td>0.751846</td>\n",
       "      <td>-0.971888</td>\n",
       "      <td>0.631818</td>\n",
       "      <td>-0.753402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3749_1</td>\n",
       "      <td>-0.102822</td>\n",
       "      <td>0.774593</td>\n",
       "      <td>0.580416</td>\n",
       "      <td>0.463161</td>\n",
       "      <td>0.047286</td>\n",
       "      <td>0.021042</td>\n",
       "      <td>0.056516</td>\n",
       "      <td>0.490920</td>\n",
       "      <td>0.795669</td>\n",
       "      <td>0.052190</td>\n",
       "      <td>0.263364</td>\n",
       "      <td>0.665853</td>\n",
       "      <td>-0.185761</td>\n",
       "      <td>0.698043</td>\n",
       "      <td>0.608211</td>\n",
       "      <td>0.469882</td>\n",
       "      <td>-0.016209</td>\n",
       "      <td>-0.160862</td>\n",
       "      <td>-0.016941</td>\n",
       "      <td>0.559579</td>\n",
       "      <td>0.351703</td>\n",
       "      <td>0.597816</td>\n",
       "      <td>0.595374</td>\n",
       "      <td>0.312439</td>\n",
       "      <td>0.511031</td>\n",
       "      <td>0.411958</td>\n",
       "      <td>-0.700036</td>\n",
       "      <td>0.580416</td>\n",
       "      <td>0.774593</td>\n",
       "      <td>-1.004546</td>\n",
       "      <td>0.639794</td>\n",
       "      <td>-0.728963</td>\n",
       "      <td>0.463161</td>\n",
       "      <td>0.047286</td>\n",
       "      <td>0.021042</td>\n",
       "      <td>0.056516</td>\n",
       "      <td>0.490920</td>\n",
       "      <td>0.795669</td>\n",
       "      <td>0.052190</td>\n",
       "      <td>0.263364</td>\n",
       "      <td>0.665853</td>\n",
       "      <td>-0.185761</td>\n",
       "      <td>0.698043</td>\n",
       "      <td>0.608211</td>\n",
       "      <td>0.469882</td>\n",
       "      <td>-0.016209</td>\n",
       "      <td>-0.160862</td>\n",
       "      <td>-0.016941</td>\n",
       "      <td>0.559579</td>\n",
       "      <td>0.351703</td>\n",
       "      <td>0.597816</td>\n",
       "      <td>0.595374</td>\n",
       "      <td>0.312439</td>\n",
       "      <td>0.511031</td>\n",
       "      <td>0.411958</td>\n",
       "      <td>-0.700036</td>\n",
       "      <td>0.580416</td>\n",
       "      <td>0.774593</td>\n",
       "      <td>-1.004546</td>\n",
       "      <td>0.639794</td>\n",
       "      <td>-0.728963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>210775_1</td>\n",
       "      <td>-12.619732</td>\n",
       "      <td>1.012949</td>\n",
       "      <td>0.709775</td>\n",
       "      <td>0.387155</td>\n",
       "      <td>0.037030</td>\n",
       "      <td>0.104426</td>\n",
       "      <td>0.030774</td>\n",
       "      <td>0.569312</td>\n",
       "      <td>0.702525</td>\n",
       "      <td>0.081885</td>\n",
       "      <td>0.390739</td>\n",
       "      <td>0.766583</td>\n",
       "      <td>-0.060772</td>\n",
       "      <td>0.770531</td>\n",
       "      <td>0.714979</td>\n",
       "      <td>0.558446</td>\n",
       "      <td>-0.109012</td>\n",
       "      <td>-0.049638</td>\n",
       "      <td>0.053448</td>\n",
       "      <td>0.619988</td>\n",
       "      <td>0.351526</td>\n",
       "      <td>0.558075</td>\n",
       "      <td>0.666303</td>\n",
       "      <td>0.389158</td>\n",
       "      <td>0.504092</td>\n",
       "      <td>0.331673</td>\n",
       "      <td>-0.834877</td>\n",
       "      <td>0.709775</td>\n",
       "      <td>1.012949</td>\n",
       "      <td>-1.173817</td>\n",
       "      <td>0.756216</td>\n",
       "      <td>-0.911121</td>\n",
       "      <td>0.387155</td>\n",
       "      <td>0.037030</td>\n",
       "      <td>0.104426</td>\n",
       "      <td>0.030774</td>\n",
       "      <td>0.569312</td>\n",
       "      <td>0.702525</td>\n",
       "      <td>0.081885</td>\n",
       "      <td>0.390739</td>\n",
       "      <td>0.766583</td>\n",
       "      <td>-0.060772</td>\n",
       "      <td>0.770531</td>\n",
       "      <td>0.714979</td>\n",
       "      <td>0.558446</td>\n",
       "      <td>-0.109012</td>\n",
       "      <td>-0.049638</td>\n",
       "      <td>0.053448</td>\n",
       "      <td>0.619988</td>\n",
       "      <td>0.351526</td>\n",
       "      <td>0.558075</td>\n",
       "      <td>0.666303</td>\n",
       "      <td>0.389158</td>\n",
       "      <td>0.504092</td>\n",
       "      <td>0.331673</td>\n",
       "      <td>-0.834877</td>\n",
       "      <td>0.709775</td>\n",
       "      <td>1.012949</td>\n",
       "      <td>-1.173817</td>\n",
       "      <td>0.756216</td>\n",
       "      <td>-0.911121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3749_1</td>\n",
       "      <td>-16.940465</td>\n",
       "      <td>1.096660</td>\n",
       "      <td>0.705653</td>\n",
       "      <td>0.394544</td>\n",
       "      <td>0.037204</td>\n",
       "      <td>0.120336</td>\n",
       "      <td>0.007432</td>\n",
       "      <td>0.593737</td>\n",
       "      <td>0.699279</td>\n",
       "      <td>0.095778</td>\n",
       "      <td>0.415976</td>\n",
       "      <td>0.793834</td>\n",
       "      <td>-0.044724</td>\n",
       "      <td>0.795758</td>\n",
       "      <td>0.746082</td>\n",
       "      <td>0.590015</td>\n",
       "      <td>-0.133887</td>\n",
       "      <td>-0.036226</td>\n",
       "      <td>0.058087</td>\n",
       "      <td>0.640044</td>\n",
       "      <td>0.374000</td>\n",
       "      <td>0.564055</td>\n",
       "      <td>0.687906</td>\n",
       "      <td>0.405685</td>\n",
       "      <td>0.519862</td>\n",
       "      <td>0.318479</td>\n",
       "      <td>-0.927088</td>\n",
       "      <td>0.705653</td>\n",
       "      <td>1.096660</td>\n",
       "      <td>-1.221680</td>\n",
       "      <td>0.798013</td>\n",
       "      <td>-0.989326</td>\n",
       "      <td>0.394544</td>\n",
       "      <td>0.037204</td>\n",
       "      <td>0.120336</td>\n",
       "      <td>0.007432</td>\n",
       "      <td>0.593737</td>\n",
       "      <td>0.699279</td>\n",
       "      <td>0.095778</td>\n",
       "      <td>0.415976</td>\n",
       "      <td>0.793834</td>\n",
       "      <td>-0.044724</td>\n",
       "      <td>0.795758</td>\n",
       "      <td>0.746082</td>\n",
       "      <td>0.590015</td>\n",
       "      <td>-0.133887</td>\n",
       "      <td>-0.036226</td>\n",
       "      <td>0.058087</td>\n",
       "      <td>0.640044</td>\n",
       "      <td>0.374000</td>\n",
       "      <td>0.564055</td>\n",
       "      <td>0.687906</td>\n",
       "      <td>0.405685</td>\n",
       "      <td>0.519862</td>\n",
       "      <td>0.318479</td>\n",
       "      <td>-0.927088</td>\n",
       "      <td>0.705653</td>\n",
       "      <td>1.096660</td>\n",
       "      <td>-1.221680</td>\n",
       "      <td>0.798013</td>\n",
       "      <td>-0.989326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>210775_1</td>\n",
       "      <td>-16.401832</td>\n",
       "      <td>0.898658</td>\n",
       "      <td>0.557853</td>\n",
       "      <td>0.486657</td>\n",
       "      <td>0.007380</td>\n",
       "      <td>0.175337</td>\n",
       "      <td>0.041739</td>\n",
       "      <td>0.523104</td>\n",
       "      <td>0.728095</td>\n",
       "      <td>0.060682</td>\n",
       "      <td>0.405824</td>\n",
       "      <td>0.741287</td>\n",
       "      <td>-0.155162</td>\n",
       "      <td>0.694126</td>\n",
       "      <td>0.710677</td>\n",
       "      <td>0.628721</td>\n",
       "      <td>-0.103223</td>\n",
       "      <td>-0.150420</td>\n",
       "      <td>-0.051760</td>\n",
       "      <td>0.586947</td>\n",
       "      <td>0.391034</td>\n",
       "      <td>0.610740</td>\n",
       "      <td>0.625927</td>\n",
       "      <td>0.398710</td>\n",
       "      <td>0.551563</td>\n",
       "      <td>0.353640</td>\n",
       "      <td>-0.786630</td>\n",
       "      <td>0.557853</td>\n",
       "      <td>0.898658</td>\n",
       "      <td>-1.040015</td>\n",
       "      <td>0.678517</td>\n",
       "      <td>-0.851363</td>\n",
       "      <td>0.486657</td>\n",
       "      <td>0.007380</td>\n",
       "      <td>0.175337</td>\n",
       "      <td>0.041739</td>\n",
       "      <td>0.523104</td>\n",
       "      <td>0.728095</td>\n",
       "      <td>0.060682</td>\n",
       "      <td>0.405824</td>\n",
       "      <td>0.741287</td>\n",
       "      <td>-0.155162</td>\n",
       "      <td>0.694126</td>\n",
       "      <td>0.710677</td>\n",
       "      <td>0.628721</td>\n",
       "      <td>-0.103223</td>\n",
       "      <td>-0.150420</td>\n",
       "      <td>-0.051760</td>\n",
       "      <td>0.586947</td>\n",
       "      <td>0.391034</td>\n",
       "      <td>0.610740</td>\n",
       "      <td>0.625927</td>\n",
       "      <td>0.398710</td>\n",
       "      <td>0.551563</td>\n",
       "      <td>0.353640</td>\n",
       "      <td>-0.786630</td>\n",
       "      <td>0.557853</td>\n",
       "      <td>0.898658</td>\n",
       "      <td>-1.040015</td>\n",
       "      <td>0.678517</td>\n",
       "      <td>-0.851363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>75818_1</td>\n",
       "      <td>-15.136129</td>\n",
       "      <td>0.741843</td>\n",
       "      <td>0.363658</td>\n",
       "      <td>0.605523</td>\n",
       "      <td>0.027722</td>\n",
       "      <td>0.107808</td>\n",
       "      <td>-0.026271</td>\n",
       "      <td>0.488546</td>\n",
       "      <td>0.828642</td>\n",
       "      <td>0.069423</td>\n",
       "      <td>0.319989</td>\n",
       "      <td>0.676679</td>\n",
       "      <td>-0.263114</td>\n",
       "      <td>0.662748</td>\n",
       "      <td>0.664831</td>\n",
       "      <td>0.609078</td>\n",
       "      <td>-0.052587</td>\n",
       "      <td>-0.264513</td>\n",
       "      <td>-0.141555</td>\n",
       "      <td>0.570657</td>\n",
       "      <td>0.464796</td>\n",
       "      <td>0.675333</td>\n",
       "      <td>0.608619</td>\n",
       "      <td>0.344979</td>\n",
       "      <td>0.608943</td>\n",
       "      <td>0.405020</td>\n",
       "      <td>-0.700171</td>\n",
       "      <td>0.363658</td>\n",
       "      <td>0.741843</td>\n",
       "      <td>-0.871878</td>\n",
       "      <td>0.607275</td>\n",
       "      <td>-0.779586</td>\n",
       "      <td>0.605523</td>\n",
       "      <td>0.027722</td>\n",
       "      <td>0.107808</td>\n",
       "      <td>-0.026271</td>\n",
       "      <td>0.488546</td>\n",
       "      <td>0.828642</td>\n",
       "      <td>0.069423</td>\n",
       "      <td>0.319989</td>\n",
       "      <td>0.676679</td>\n",
       "      <td>-0.263114</td>\n",
       "      <td>0.662748</td>\n",
       "      <td>0.664831</td>\n",
       "      <td>0.609078</td>\n",
       "      <td>-0.052587</td>\n",
       "      <td>-0.264513</td>\n",
       "      <td>-0.141555</td>\n",
       "      <td>0.570657</td>\n",
       "      <td>0.464796</td>\n",
       "      <td>0.675333</td>\n",
       "      <td>0.608619</td>\n",
       "      <td>0.344979</td>\n",
       "      <td>0.608943</td>\n",
       "      <td>0.405020</td>\n",
       "      <td>-0.700171</td>\n",
       "      <td>0.363658</td>\n",
       "      <td>0.741843</td>\n",
       "      <td>-0.871878</td>\n",
       "      <td>0.607275</td>\n",
       "      <td>-0.779586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>210775_1</td>\n",
       "      <td>-15.213317</td>\n",
       "      <td>0.600765</td>\n",
       "      <td>0.296333</td>\n",
       "      <td>0.631184</td>\n",
       "      <td>0.037168</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>-0.001885</td>\n",
       "      <td>0.448543</td>\n",
       "      <td>0.901886</td>\n",
       "      <td>0.055512</td>\n",
       "      <td>0.223482</td>\n",
       "      <td>0.607258</td>\n",
       "      <td>-0.329878</td>\n",
       "      <td>0.633247</td>\n",
       "      <td>0.582016</td>\n",
       "      <td>0.513401</td>\n",
       "      <td>0.018786</td>\n",
       "      <td>-0.327859</td>\n",
       "      <td>-0.175906</td>\n",
       "      <td>0.531355</td>\n",
       "      <td>0.452803</td>\n",
       "      <td>0.713578</td>\n",
       "      <td>0.570956</td>\n",
       "      <td>0.283335</td>\n",
       "      <td>0.602211</td>\n",
       "      <td>0.470376</td>\n",
       "      <td>-0.590139</td>\n",
       "      <td>0.296333</td>\n",
       "      <td>0.600765</td>\n",
       "      <td>-0.764500</td>\n",
       "      <td>0.529132</td>\n",
       "      <td>-0.665459</td>\n",
       "      <td>0.631184</td>\n",
       "      <td>0.037168</td>\n",
       "      <td>0.023529</td>\n",
       "      <td>-0.001885</td>\n",
       "      <td>0.448543</td>\n",
       "      <td>0.901886</td>\n",
       "      <td>0.055512</td>\n",
       "      <td>0.223482</td>\n",
       "      <td>0.607258</td>\n",
       "      <td>-0.329878</td>\n",
       "      <td>0.633247</td>\n",
       "      <td>0.582016</td>\n",
       "      <td>0.513401</td>\n",
       "      <td>0.018786</td>\n",
       "      <td>-0.327859</td>\n",
       "      <td>-0.175906</td>\n",
       "      <td>0.531355</td>\n",
       "      <td>0.452803</td>\n",
       "      <td>0.713578</td>\n",
       "      <td>0.570956</td>\n",
       "      <td>0.283335</td>\n",
       "      <td>0.602211</td>\n",
       "      <td>0.470376</td>\n",
       "      <td>-0.590139</td>\n",
       "      <td>0.296333</td>\n",
       "      <td>0.600765</td>\n",
       "      <td>-0.764500</td>\n",
       "      <td>0.529132</td>\n",
       "      <td>-0.665459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cano_locdt_index      score  cosine_errors_mean  euclidean_errors_mean  \\\n",
       "0                0  27.631021           -0.295345              -0.059350   \n",
       "1                0  27.631021            0.382906               0.271174   \n",
       "2           3749_1  25.149796            0.375645               0.293028   \n",
       "3                0  14.849816            0.751846               0.524848   \n",
       "4           3749_1  -0.102822            0.774593               0.580416   \n",
       "5         210775_1 -12.619732            1.012949               0.709775   \n",
       "6           3749_1 -16.940465            1.096660               0.705653   \n",
       "7         210775_1 -16.401832            0.898658               0.557853   \n",
       "8          75818_1 -15.136129            0.741843               0.363658   \n",
       "9         210775_1 -15.213317            0.600765               0.296333   \n",
       "\n",
       "   reconstructions_mean_latent_features_0  \\\n",
       "0                                0.924081   \n",
       "1                                0.727449   \n",
       "2                                0.615858   \n",
       "3                                0.527854   \n",
       "4                                0.463161   \n",
       "5                                0.387155   \n",
       "6                                0.394544   \n",
       "7                                0.486657   \n",
       "8                                0.605523   \n",
       "9                                0.631184   \n",
       "\n",
       "   reconstructions_mean_latent_features_1  \\\n",
       "0                               -0.135590   \n",
       "1                                0.151623   \n",
       "2                               -0.015094   \n",
       "3                                0.061934   \n",
       "4                                0.047286   \n",
       "5                                0.037030   \n",
       "6                                0.037204   \n",
       "7                                0.007380   \n",
       "8                                0.027722   \n",
       "9                                0.037168   \n",
       "\n",
       "   reconstructions_mean_latent_features_2  \\\n",
       "0                                0.499637   \n",
       "1                               -0.271612   \n",
       "2                                0.043652   \n",
       "3                                0.025055   \n",
       "4                                0.021042   \n",
       "5                                0.104426   \n",
       "6                                0.120336   \n",
       "7                                0.175337   \n",
       "8                                0.107808   \n",
       "9                                0.023529   \n",
       "\n",
       "   reconstructions_mean_latent_features_3  \\\n",
       "0                                0.241374   \n",
       "1                               -0.256591   \n",
       "2                                0.213557   \n",
       "3                               -0.068244   \n",
       "4                                0.056516   \n",
       "5                                0.030774   \n",
       "6                                0.007432   \n",
       "7                                0.041739   \n",
       "8                               -0.026271   \n",
       "9                               -0.001885   \n",
       "\n",
       "   reconstructions_mean_latent_features_4  \\\n",
       "0                               -0.017618   \n",
       "1                                0.490845   \n",
       "2                                0.288134   \n",
       "3                                0.514123   \n",
       "4                                0.490920   \n",
       "5                                0.569312   \n",
       "6                                0.593737   \n",
       "7                                0.523104   \n",
       "8                                0.488546   \n",
       "9                                0.448543   \n",
       "\n",
       "   reconstructions_mean_latent_features_5  \\\n",
       "0                                0.693070   \n",
       "1                                1.015347   \n",
       "2                                0.883730   \n",
       "3                                0.804582   \n",
       "4                                0.795669   \n",
       "5                                0.702525   \n",
       "6                                0.699279   \n",
       "7                                0.728095   \n",
       "8                                0.828642   \n",
       "9                                0.901886   \n",
       "\n",
       "   reconstructions_mean_latent_features_6  \\\n",
       "0                               -0.296043   \n",
       "1                                0.207021   \n",
       "2                               -0.065764   \n",
       "3                                0.102690   \n",
       "4                                0.052190   \n",
       "5                                0.081885   \n",
       "6                                0.095778   \n",
       "7                                0.060682   \n",
       "8                                0.069423   \n",
       "9                                0.055512   \n",
       "\n",
       "   reconstructions_mean_latent_features_7  \\\n",
       "0                                0.333671   \n",
       "1                                0.011632   \n",
       "2                                0.124521   \n",
       "3                                0.296015   \n",
       "4                                0.263364   \n",
       "5                                0.390739   \n",
       "6                                0.415976   \n",
       "7                                0.405824   \n",
       "8                                0.319989   \n",
       "9                                0.223482   \n",
       "\n",
       "   reconstructions_mean_latent_features_8  \\\n",
       "0                                0.215816   \n",
       "1                                0.392299   \n",
       "2                                0.471429   \n",
       "3                                0.670417   \n",
       "4                                0.665853   \n",
       "5                                0.766583   \n",
       "6                                0.793834   \n",
       "7                                0.741287   \n",
       "8                                0.676679   \n",
       "9                                0.607258   \n",
       "\n",
       "   reconstructions_mean_latent_features_9  \\\n",
       "0                               -0.632744   \n",
       "1                               -0.386213   \n",
       "2                               -0.453050   \n",
       "3                               -0.188862   \n",
       "4                               -0.185761   \n",
       "5                               -0.060772   \n",
       "6                               -0.044724   \n",
       "7                               -0.155162   \n",
       "8                               -0.263114   \n",
       "9                               -0.329878   \n",
       "\n",
       "   reconstructions_mean_latent_features_10  \\\n",
       "0                                -0.082227   \n",
       "1                                 0.593477   \n",
       "2                                 0.440064   \n",
       "3                                 0.711714   \n",
       "4                                 0.698043   \n",
       "5                                 0.770531   \n",
       "6                                 0.795758   \n",
       "7                                 0.694126   \n",
       "8                                 0.662748   \n",
       "9                                 0.633247   \n",
       "\n",
       "   reconstructions_mean_latent_features_11  \\\n",
       "0                                 0.352146   \n",
       "1                                 0.432118   \n",
       "2                                 0.413630   \n",
       "3                                 0.656565   \n",
       "4                                 0.608211   \n",
       "5                                 0.714979   \n",
       "6                                 0.746082   \n",
       "7                                 0.710677   \n",
       "8                                 0.664831   \n",
       "9                                 0.582016   \n",
       "\n",
       "   reconstructions_mean_latent_features_12  \\\n",
       "0                                 0.786789   \n",
       "1                                 0.357133   \n",
       "2                                 0.400986   \n",
       "3                                 0.538123   \n",
       "4                                 0.469882   \n",
       "5                                 0.558446   \n",
       "6                                 0.590015   \n",
       "7                                 0.628721   \n",
       "8                                 0.609078   \n",
       "9                                 0.513401   \n",
       "\n",
       "   reconstructions_mean_latent_features_13  \\\n",
       "0                                 0.181245   \n",
       "1                                 0.118143   \n",
       "2                                 0.135690   \n",
       "3                                -0.054643   \n",
       "4                                -0.016209   \n",
       "5                                -0.109012   \n",
       "6                                -0.133887   \n",
       "7                                -0.103223   \n",
       "8                                -0.052587   \n",
       "9                                 0.018786   \n",
       "\n",
       "   reconstructions_mean_latent_features_14  \\\n",
       "0                                -0.725199   \n",
       "1                                -0.396401   \n",
       "2                                -0.418351   \n",
       "3                                -0.196332   \n",
       "4                                -0.160862   \n",
       "5                                -0.049638   \n",
       "6                                -0.036226   \n",
       "7                                -0.150420   \n",
       "8                                -0.264513   \n",
       "9                                -0.327859   \n",
       "\n",
       "   reconstructions_mean_latent_features_15  \\\n",
       "0                                -0.668880   \n",
       "1                                -0.241968   \n",
       "2                                -0.236989   \n",
       "3                                -0.076617   \n",
       "4                                -0.016941   \n",
       "5                                 0.053448   \n",
       "6                                 0.058087   \n",
       "7                                -0.051760   \n",
       "8                                -0.141555   \n",
       "9                                -0.175906   \n",
       "\n",
       "   reconstructions_mean_latent_features_16  \\\n",
       "0                                 0.253682   \n",
       "1                                 0.561575   \n",
       "2                                 0.402316   \n",
       "3                                 0.599034   \n",
       "4                                 0.559579   \n",
       "5                                 0.619988   \n",
       "6                                 0.640044   \n",
       "7                                 0.586947   \n",
       "8                                 0.570657   \n",
       "9                                 0.531355   \n",
       "\n",
       "   reconstructions_mean_latent_features_17  \\\n",
       "0                                 0.315962   \n",
       "1                                 0.709550   \n",
       "2                                 0.329611   \n",
       "3                                 0.445602   \n",
       "4                                 0.351703   \n",
       "5                                 0.351526   \n",
       "6                                 0.374000   \n",
       "7                                 0.391034   \n",
       "8                                 0.464796   \n",
       "9                                 0.452803   \n",
       "\n",
       "   reconstructions_mean_latent_features_18  \\\n",
       "0                                 0.589892   \n",
       "1                                 0.691831   \n",
       "2                                 0.657517   \n",
       "3                                 0.639521   \n",
       "4                                 0.597816   \n",
       "5                                 0.558075   \n",
       "6                                 0.564055   \n",
       "7                                 0.610740   \n",
       "8                                 0.675333   \n",
       "9                                 0.713578   \n",
       "\n",
       "   reconstructions_mean_latent_features_19  \\\n",
       "0                                 0.212861   \n",
       "1                                 0.556354   \n",
       "2                                 0.392622   \n",
       "3                                 0.643435   \n",
       "4                                 0.595374   \n",
       "5                                 0.666303   \n",
       "6                                 0.687906   \n",
       "7                                 0.625927   \n",
       "8                                 0.608619   \n",
       "9                                 0.570956   \n",
       "\n",
       "   reconstructions_mean_latent_features_20  \\\n",
       "0                                 0.284910   \n",
       "1                                 0.085471   \n",
       "2                                 0.233974   \n",
       "3                                 0.317581   \n",
       "4                                 0.312439   \n",
       "5                                 0.389158   \n",
       "6                                 0.405685   \n",
       "7                                 0.398710   \n",
       "8                                 0.344979   \n",
       "9                                 0.283335   \n",
       "\n",
       "   reconstructions_mean_latent_features_21  \\\n",
       "0                                 0.750360   \n",
       "1                                 0.667678   \n",
       "2                                 0.523237   \n",
       "3                                 0.574855   \n",
       "4                                 0.511031   \n",
       "5                                 0.504092   \n",
       "6                                 0.519862   \n",
       "7                                 0.551563   \n",
       "8                                 0.608943   \n",
       "9                                 0.602211   \n",
       "\n",
       "   reconstructions_mean_latent_features_22  \\\n",
       "0                                 0.456047   \n",
       "1                                 0.544193   \n",
       "2                                 0.546441   \n",
       "3                                 0.386058   \n",
       "4                                 0.411958   \n",
       "5                                 0.331673   \n",
       "6                                 0.318479   \n",
       "7                                 0.353640   \n",
       "8                                 0.405020   \n",
       "9                                 0.470376   \n",
       "\n",
       "   latent_representations_latent_features_0  \\\n",
       "0                                  0.025644   \n",
       "1                                 -0.466960   \n",
       "2                                 -0.470165   \n",
       "3                                 -0.706799   \n",
       "4                                 -0.700036   \n",
       "5                                 -0.834877   \n",
       "6                                 -0.927088   \n",
       "7                                 -0.786630   \n",
       "8                                 -0.700171   \n",
       "9                                 -0.590139   \n",
       "\n",
       "   latent_representations_latent_features_1  \\\n",
       "0                                 -0.059350   \n",
       "1                                  0.271174   \n",
       "2                                  0.293028   \n",
       "3                                  0.524848   \n",
       "4                                  0.580416   \n",
       "5                                  0.709775   \n",
       "6                                  0.705653   \n",
       "7                                  0.557853   \n",
       "8                                  0.363658   \n",
       "9                                  0.296333   \n",
       "\n",
       "   latent_representations_latent_features_2  \\\n",
       "0                                 -0.295345   \n",
       "1                                  0.382906   \n",
       "2                                  0.375645   \n",
       "3                                  0.751846   \n",
       "4                                  0.774593   \n",
       "5                                  1.012949   \n",
       "6                                  1.096660   \n",
       "7                                  0.898658   \n",
       "8                                  0.741843   \n",
       "9                                  0.600765   \n",
       "\n",
       "   latent_representations_latent_features_3  \\\n",
       "0                                 -0.156371   \n",
       "1                                 -0.666963   \n",
       "2                                 -0.673999   \n",
       "3                                 -0.971888   \n",
       "4                                 -1.004546   \n",
       "5                                 -1.173817   \n",
       "6                                 -1.221680   \n",
       "7                                 -1.040015   \n",
       "8                                 -0.871878   \n",
       "9                                 -0.764500   \n",
       "\n",
       "   latent_representations_latent_features_4  \\\n",
       "0                                  0.089071   \n",
       "1                                  0.448975   \n",
       "2                                  0.433263   \n",
       "3                                  0.631818   \n",
       "4                                  0.639794   \n",
       "5                                  0.756216   \n",
       "6                                  0.798013   \n",
       "7                                  0.678517   \n",
       "8                                  0.607275   \n",
       "9                                  0.529132   \n",
       "\n",
       "   latent_representations_latent_features_5  \\\n",
       "0                                  0.074201   \n",
       "1                                 -0.493218   \n",
       "2                                 -0.466943   \n",
       "3                                 -0.753402   \n",
       "4                                 -0.728963   \n",
       "5                                 -0.911121   \n",
       "6                                 -0.989326   \n",
       "7                                 -0.851363   \n",
       "8                                 -0.779586   \n",
       "9                                 -0.665459   \n",
       "\n",
       "   reconstructions_mean_latent_features_0  \\\n",
       "0                                0.924081   \n",
       "1                                0.727449   \n",
       "2                                0.615858   \n",
       "3                                0.527854   \n",
       "4                                0.463161   \n",
       "5                                0.387155   \n",
       "6                                0.394544   \n",
       "7                                0.486657   \n",
       "8                                0.605523   \n",
       "9                                0.631184   \n",
       "\n",
       "   reconstructions_mean_latent_features_1  \\\n",
       "0                               -0.135590   \n",
       "1                                0.151623   \n",
       "2                               -0.015094   \n",
       "3                                0.061934   \n",
       "4                                0.047286   \n",
       "5                                0.037030   \n",
       "6                                0.037204   \n",
       "7                                0.007380   \n",
       "8                                0.027722   \n",
       "9                                0.037168   \n",
       "\n",
       "   reconstructions_mean_latent_features_2  \\\n",
       "0                                0.499637   \n",
       "1                               -0.271612   \n",
       "2                                0.043652   \n",
       "3                                0.025055   \n",
       "4                                0.021042   \n",
       "5                                0.104426   \n",
       "6                                0.120336   \n",
       "7                                0.175337   \n",
       "8                                0.107808   \n",
       "9                                0.023529   \n",
       "\n",
       "   reconstructions_mean_latent_features_3  \\\n",
       "0                                0.241374   \n",
       "1                               -0.256591   \n",
       "2                                0.213557   \n",
       "3                               -0.068244   \n",
       "4                                0.056516   \n",
       "5                                0.030774   \n",
       "6                                0.007432   \n",
       "7                                0.041739   \n",
       "8                               -0.026271   \n",
       "9                               -0.001885   \n",
       "\n",
       "   reconstructions_mean_latent_features_4  \\\n",
       "0                               -0.017618   \n",
       "1                                0.490845   \n",
       "2                                0.288134   \n",
       "3                                0.514123   \n",
       "4                                0.490920   \n",
       "5                                0.569312   \n",
       "6                                0.593737   \n",
       "7                                0.523104   \n",
       "8                                0.488546   \n",
       "9                                0.448543   \n",
       "\n",
       "   reconstructions_mean_latent_features_5  \\\n",
       "0                                0.693070   \n",
       "1                                1.015347   \n",
       "2                                0.883730   \n",
       "3                                0.804582   \n",
       "4                                0.795669   \n",
       "5                                0.702525   \n",
       "6                                0.699279   \n",
       "7                                0.728095   \n",
       "8                                0.828642   \n",
       "9                                0.901886   \n",
       "\n",
       "   reconstructions_mean_latent_features_6  \\\n",
       "0                               -0.296043   \n",
       "1                                0.207021   \n",
       "2                               -0.065764   \n",
       "3                                0.102690   \n",
       "4                                0.052190   \n",
       "5                                0.081885   \n",
       "6                                0.095778   \n",
       "7                                0.060682   \n",
       "8                                0.069423   \n",
       "9                                0.055512   \n",
       "\n",
       "   reconstructions_mean_latent_features_7  \\\n",
       "0                                0.333671   \n",
       "1                                0.011632   \n",
       "2                                0.124521   \n",
       "3                                0.296015   \n",
       "4                                0.263364   \n",
       "5                                0.390739   \n",
       "6                                0.415976   \n",
       "7                                0.405824   \n",
       "8                                0.319989   \n",
       "9                                0.223482   \n",
       "\n",
       "   reconstructions_mean_latent_features_8  \\\n",
       "0                                0.215816   \n",
       "1                                0.392299   \n",
       "2                                0.471429   \n",
       "3                                0.670417   \n",
       "4                                0.665853   \n",
       "5                                0.766583   \n",
       "6                                0.793834   \n",
       "7                                0.741287   \n",
       "8                                0.676679   \n",
       "9                                0.607258   \n",
       "\n",
       "   reconstructions_mean_latent_features_9  \\\n",
       "0                               -0.632744   \n",
       "1                               -0.386213   \n",
       "2                               -0.453050   \n",
       "3                               -0.188862   \n",
       "4                               -0.185761   \n",
       "5                               -0.060772   \n",
       "6                               -0.044724   \n",
       "7                               -0.155162   \n",
       "8                               -0.263114   \n",
       "9                               -0.329878   \n",
       "\n",
       "   reconstructions_mean_latent_features_10  \\\n",
       "0                                -0.082227   \n",
       "1                                 0.593477   \n",
       "2                                 0.440064   \n",
       "3                                 0.711714   \n",
       "4                                 0.698043   \n",
       "5                                 0.770531   \n",
       "6                                 0.795758   \n",
       "7                                 0.694126   \n",
       "8                                 0.662748   \n",
       "9                                 0.633247   \n",
       "\n",
       "   reconstructions_mean_latent_features_11  \\\n",
       "0                                 0.352146   \n",
       "1                                 0.432118   \n",
       "2                                 0.413630   \n",
       "3                                 0.656565   \n",
       "4                                 0.608211   \n",
       "5                                 0.714979   \n",
       "6                                 0.746082   \n",
       "7                                 0.710677   \n",
       "8                                 0.664831   \n",
       "9                                 0.582016   \n",
       "\n",
       "   reconstructions_mean_latent_features_12  \\\n",
       "0                                 0.786789   \n",
       "1                                 0.357133   \n",
       "2                                 0.400986   \n",
       "3                                 0.538123   \n",
       "4                                 0.469882   \n",
       "5                                 0.558446   \n",
       "6                                 0.590015   \n",
       "7                                 0.628721   \n",
       "8                                 0.609078   \n",
       "9                                 0.513401   \n",
       "\n",
       "   reconstructions_mean_latent_features_13  \\\n",
       "0                                 0.181245   \n",
       "1                                 0.118143   \n",
       "2                                 0.135690   \n",
       "3                                -0.054643   \n",
       "4                                -0.016209   \n",
       "5                                -0.109012   \n",
       "6                                -0.133887   \n",
       "7                                -0.103223   \n",
       "8                                -0.052587   \n",
       "9                                 0.018786   \n",
       "\n",
       "   reconstructions_mean_latent_features_14  \\\n",
       "0                                -0.725199   \n",
       "1                                -0.396401   \n",
       "2                                -0.418351   \n",
       "3                                -0.196332   \n",
       "4                                -0.160862   \n",
       "5                                -0.049638   \n",
       "6                                -0.036226   \n",
       "7                                -0.150420   \n",
       "8                                -0.264513   \n",
       "9                                -0.327859   \n",
       "\n",
       "   reconstructions_mean_latent_features_15  \\\n",
       "0                                -0.668880   \n",
       "1                                -0.241968   \n",
       "2                                -0.236989   \n",
       "3                                -0.076617   \n",
       "4                                -0.016941   \n",
       "5                                 0.053448   \n",
       "6                                 0.058087   \n",
       "7                                -0.051760   \n",
       "8                                -0.141555   \n",
       "9                                -0.175906   \n",
       "\n",
       "   reconstructions_mean_latent_features_16  \\\n",
       "0                                 0.253682   \n",
       "1                                 0.561575   \n",
       "2                                 0.402316   \n",
       "3                                 0.599034   \n",
       "4                                 0.559579   \n",
       "5                                 0.619988   \n",
       "6                                 0.640044   \n",
       "7                                 0.586947   \n",
       "8                                 0.570657   \n",
       "9                                 0.531355   \n",
       "\n",
       "   reconstructions_mean_latent_features_17  \\\n",
       "0                                 0.315962   \n",
       "1                                 0.709550   \n",
       "2                                 0.329611   \n",
       "3                                 0.445602   \n",
       "4                                 0.351703   \n",
       "5                                 0.351526   \n",
       "6                                 0.374000   \n",
       "7                                 0.391034   \n",
       "8                                 0.464796   \n",
       "9                                 0.452803   \n",
       "\n",
       "   reconstructions_mean_latent_features_18  \\\n",
       "0                                 0.589892   \n",
       "1                                 0.691831   \n",
       "2                                 0.657517   \n",
       "3                                 0.639521   \n",
       "4                                 0.597816   \n",
       "5                                 0.558075   \n",
       "6                                 0.564055   \n",
       "7                                 0.610740   \n",
       "8                                 0.675333   \n",
       "9                                 0.713578   \n",
       "\n",
       "   reconstructions_mean_latent_features_19  \\\n",
       "0                                 0.212861   \n",
       "1                                 0.556354   \n",
       "2                                 0.392622   \n",
       "3                                 0.643435   \n",
       "4                                 0.595374   \n",
       "5                                 0.666303   \n",
       "6                                 0.687906   \n",
       "7                                 0.625927   \n",
       "8                                 0.608619   \n",
       "9                                 0.570956   \n",
       "\n",
       "   reconstructions_mean_latent_features_20  \\\n",
       "0                                 0.284910   \n",
       "1                                 0.085471   \n",
       "2                                 0.233974   \n",
       "3                                 0.317581   \n",
       "4                                 0.312439   \n",
       "5                                 0.389158   \n",
       "6                                 0.405685   \n",
       "7                                 0.398710   \n",
       "8                                 0.344979   \n",
       "9                                 0.283335   \n",
       "\n",
       "   reconstructions_mean_latent_features_21  \\\n",
       "0                                 0.750360   \n",
       "1                                 0.667678   \n",
       "2                                 0.523237   \n",
       "3                                 0.574855   \n",
       "4                                 0.511031   \n",
       "5                                 0.504092   \n",
       "6                                 0.519862   \n",
       "7                                 0.551563   \n",
       "8                                 0.608943   \n",
       "9                                 0.602211   \n",
       "\n",
       "   reconstructions_mean_latent_features_22  \\\n",
       "0                                 0.456047   \n",
       "1                                 0.544193   \n",
       "2                                 0.546441   \n",
       "3                                 0.386058   \n",
       "4                                 0.411958   \n",
       "5                                 0.331673   \n",
       "6                                 0.318479   \n",
       "7                                 0.353640   \n",
       "8                                 0.405020   \n",
       "9                                 0.470376   \n",
       "\n",
       "   latent_representations_latent_features_0  \\\n",
       "0                                  0.025644   \n",
       "1                                 -0.466960   \n",
       "2                                 -0.470165   \n",
       "3                                 -0.706799   \n",
       "4                                 -0.700036   \n",
       "5                                 -0.834877   \n",
       "6                                 -0.927088   \n",
       "7                                 -0.786630   \n",
       "8                                 -0.700171   \n",
       "9                                 -0.590139   \n",
       "\n",
       "   latent_representations_latent_features_1  \\\n",
       "0                                 -0.059350   \n",
       "1                                  0.271174   \n",
       "2                                  0.293028   \n",
       "3                                  0.524848   \n",
       "4                                  0.580416   \n",
       "5                                  0.709775   \n",
       "6                                  0.705653   \n",
       "7                                  0.557853   \n",
       "8                                  0.363658   \n",
       "9                                  0.296333   \n",
       "\n",
       "   latent_representations_latent_features_2  \\\n",
       "0                                 -0.295345   \n",
       "1                                  0.382906   \n",
       "2                                  0.375645   \n",
       "3                                  0.751846   \n",
       "4                                  0.774593   \n",
       "5                                  1.012949   \n",
       "6                                  1.096660   \n",
       "7                                  0.898658   \n",
       "8                                  0.741843   \n",
       "9                                  0.600765   \n",
       "\n",
       "   latent_representations_latent_features_3  \\\n",
       "0                                 -0.156371   \n",
       "1                                 -0.666963   \n",
       "2                                 -0.673999   \n",
       "3                                 -0.971888   \n",
       "4                                 -1.004546   \n",
       "5                                 -1.173817   \n",
       "6                                 -1.221680   \n",
       "7                                 -1.040015   \n",
       "8                                 -0.871878   \n",
       "9                                 -0.764500   \n",
       "\n",
       "   latent_representations_latent_features_4  \\\n",
       "0                                  0.089071   \n",
       "1                                  0.448975   \n",
       "2                                  0.433263   \n",
       "3                                  0.631818   \n",
       "4                                  0.639794   \n",
       "5                                  0.756216   \n",
       "6                                  0.798013   \n",
       "7                                  0.678517   \n",
       "8                                  0.607275   \n",
       "9                                  0.529132   \n",
       "\n",
       "   latent_representations_latent_features_5  \n",
       "0                                  0.074201  \n",
       "1                                 -0.493218  \n",
       "2                                 -0.466943  \n",
       "3                                 -0.753402  \n",
       "4                                 -0.728963  \n",
       "5                                 -0.911121  \n",
       "6                                 -0.989326  \n",
       "7                                 -0.851363  \n",
       "8                                 -0.779586  \n",
       "9                                 -0.665459  "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = []\n",
    "for i in range(len(output)):\n",
    "    if i%3 == 2:\n",
    "        feature.append(output.iloc[i:i+1])\n",
    "feature = pd.concat(feature,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature.to_csv(\"/data/yunrui_li/fraud/fraud_detection/features/DAGMM_features.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
