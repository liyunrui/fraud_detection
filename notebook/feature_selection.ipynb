{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "python3 main.py ../../dataset/train.csv ../../dataset/test.csv ../result/cv_results.csv ../result/submission.csv > ../result/logs.txt\n",
    "\n",
    "make train\n",
    "\n",
    "\"\"\"\n",
    "import sys\n",
    "sys.path.append(\"../fraud_detection/src/\")\n",
    "import time\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import numpy as np\n",
    "from contextlib import contextmanager\n",
    "import gc \n",
    "from util import s_to_time_format, string_to_datetime, hour_to_range, kfold_lightgbm, kfold_xgb\n",
    "from util import rolling_stats_target_by_cols\n",
    "#from util import _time_elapsed_between_last_transactions,time_elapsed_between_last_transactions\n",
    "#from util import num_transaction_in_past_n_days\n",
    "#from util import add_auto_encoder_feature\n",
    "#from util import group_target_by_cols_split_by_users\n",
    "from time import strftime, localtime\n",
    "import logging\n",
    "import sys\n",
    "from config import Configs\n",
    "\n",
    "# logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "#log_file = '{}-{}-{}.log'.format(opt.model_name, opt.dataset, strftime(\"%y%m%d-%H%M\", localtime()))\n",
    "log_file = '../fraud_detection/result/fs_{}.log'.format(strftime(\"%y%m%d-%H%M\", localtime()))\n",
    "logger.addHandler(logging.FileHandler(log_file))\n",
    "\n",
    "def group_target_by_cols(df_train, df_test, recipe):\n",
    "    df = pd.concat([df_train, df_test], axis = 0)\n",
    "    for m in range(len(recipe)):\n",
    "        cols = recipe[m][0]\n",
    "        for n in range(len(recipe[m][1])):\n",
    "            target = recipe[m][1][n][0]\n",
    "            method = recipe[m][1][n][1]\n",
    "            name_grouped_target = method+\"_\"+target+'_BY_'+'_'.join(cols)\n",
    "            tmp = df[cols + [target]].groupby(cols).agg(method)\n",
    "            tmp = tmp.reset_index().rename(index=str, columns={target: name_grouped_target})\n",
    "            df_train = df_train.merge(tmp, how='left', on=cols)\n",
    "            df_test = df_test.merge(tmp, how='left', on=cols)\n",
    "\n",
    "        # reduced memory    \n",
    "        del tmp\n",
    "        gc.collect()\n",
    "    \n",
    "    return df_train, df_test\n",
    "\n",
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    logger.info(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n",
    "    \n",
    "def main(args):\n",
    "    with timer(\"Process train/test application\"):\n",
    "        #-------------------------\n",
    "        # load dataset\n",
    "        #-------------------------\n",
    "        df_train = pd.read_csv(args.train_file)\n",
    "        df_test = pd.read_csv(args.test_file)\n",
    "        #-------------------------\n",
    "        # pre-processing\n",
    "        #-------------------------\n",
    "\n",
    "        for cat in Configs.CATEGORY:\n",
    "            df_train[cat] = df_train[cat].astype('category') #.cat.codes\n",
    "            df_test[cat] = df_test[cat].astype('category')\n",
    "            \n",
    "        for df in [df_train, df_test]:\n",
    "            # pre-processing\n",
    "            df[\"loctm_\"] = df.loctm.astype(int).astype(str)\n",
    "            df.loctm_ = df.loctm_.apply(s_to_time_format).apply(string_to_datetime)\n",
    "            # # time-related feature\n",
    "            df[\"loctm_hour_of_day\"] = df.loctm_.apply(lambda x: x.hour).astype('category')\n",
    "            df[\"loctm_minute_of_hour\"] = df.loctm_.apply(lambda x: x.minute)\n",
    "            df[\"loctm_second_of_min\"] = df.loctm_.apply(lambda x: x.second)\n",
    "            # df[\"loctm_absolute_time\"] = [h*60+m for h,m in zip(df.loctm_hour_of_day,df.loctm_minute_of_hour)]\n",
    "            df[\"hour_range\"] = df.loctm_.apply(lambda x: hour_to_range(x.hour)).astype(\"category\")\n",
    "            # removed the columns no need\n",
    "            df.drop(columns = [\"loctm_\"], axis = 1, inplace = True)\n",
    "            # auxiliary fields\n",
    "            df[\"day_hr_min\"] = [\"{}:{}:{}\".format(i,j,k) for i,j,k in zip(df.locdt,df.loctm_hour_of_day,df.loctm_minute_of_hour)]\n",
    "            df[\"day_hr_min_sec\"] = [\"{}:{}:{}:{}\".format(i,j,k,z) for i,j,k,z in zip(df.locdt,df.loctm_hour_of_day,df.loctm_minute_of_hour,df.loctm_second_of_min)]\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add bacno/cano feature\"):\n",
    "        df_train, df_test = group_target_by_cols(df_train, df_test, Configs.CONAM_AGG_RECIPE_1)\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add iterm-related feature\"):\n",
    "        df_train, df_test = group_target_by_cols(df_train, df_test, Configs.ITERM_AGG_RECIPE)\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add conam-related feature\"):\n",
    "        df_train, df_test = group_target_by_cols(df_train, df_test, Configs.CONAM_AGG_RECIPE_2)\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add hour-related feature\"):\n",
    "        df_train, df_test = group_target_by_cols(df_train, df_test, Configs.HOUR_AGG_RECIPE)\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add cano/conam feature\"):\n",
    "        df_train, df_test = group_target_by_cols(df_train, df_test, Configs.CANO_CONAM_COUNT_RECIPE)\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add cano/bacno latent feature\"):\n",
    "        df = pd.read_csv(\"../fraud_detection/features/bacno_latent_features_w_cano.csv\")\n",
    "        df_train = df_train.merge(df, on = \"bacno\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"bacno\", how = \"left\")\n",
    "        df = pd.read_csv(\"../fraud_detection/features/bacno_cano_latent_features.csv\")\n",
    "        df_train = df_train.merge(df, on = \"cano\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"cano\", how = \"left\")\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add locdt-related feature\"):\n",
    "        df_train, df_test = group_target_by_cols(df_train, df_test, Configs.LOCDT_CONAM_RECIPE)\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add mchno-related feature\"):\n",
    "        df_train, df_test = group_target_by_cols(df_train, df_test, Configs.MCHNO_CONAM_RECIPE)\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add scity-related feature\"):\n",
    "        df_train, df_test = group_target_by_cols(df_train, df_test, Configs.SCITY_CONAM_RECIPE)\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add stocn-related feature\"):\n",
    "        df_train, df_test = group_target_by_cols(df_train, df_test, Configs.STOCN_CONAM_RECIPE)\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add mchno/bacno latent feature\"):\n",
    "        df = pd.read_csv(\"../fraud_detection/features/bacno_latent_features_w_mchno.csv\")\n",
    "        df_train = df_train.merge(df, on = \"bacno\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"bacno\", how = \"left\")\n",
    "        df = pd.read_csv(\"../fraud_detection/features/bacno_mchno_latent_features.csv\")\n",
    "        df_train = df_train.merge(df, on = \"mchno\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"mchno\", how = \"left\")\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add time second-level feature on bacno\"):\n",
    "        df_train, df_test = group_target_by_cols(\n",
    "            df_train, \n",
    "            df_test, \n",
    "            Configs.HOUR_AGG_SEC_LEVEL_RECIPE_BACNO,\n",
    "            )\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add time second-level feature on cano\"):\n",
    "        df_train, df_test = group_target_by_cols(\n",
    "            df_train, \n",
    "            df_test, \n",
    "            Configs.HOUR_AGG_SEC_LEVEL_RECIPE_CANO,\n",
    "            )\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add time second-level feature on mchno\"):\n",
    "        df_train, df_test = group_target_by_cols(\n",
    "            df_train, \n",
    "            df_test, \n",
    "            Configs.HOUR_AGG_SEC_LEVEL_RECIPE_MCHNO,\n",
    "            )\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add time second-level feature on csmcu/stocn/scity\"):\n",
    "        df_train, df_test = group_target_by_cols(\n",
    "            df_train, \n",
    "            df_test, \n",
    "            Configs.HOUR_AGG_SEC_LEVEL_RECIPE,\n",
    "            )\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add time second-level feature on acqic/csmcu/stocn/scity\"):\n",
    "        df_train, df_test = group_target_by_cols(\n",
    "            df_train, \n",
    "            df_test, \n",
    "            Configs.HOUR_AGG_SEC_LEVEL_RECIPE_2,\n",
    "            )\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add conam-related feature v3\"):\n",
    "        df_train, df_test = group_target_by_cols(\n",
    "            df_train, \n",
    "            df_test, \n",
    "            Configs.CONAM_AGG_RECIPE_3,\n",
    "            )\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add locdt-related feature v2\"):\n",
    "        df_train, df_test = group_target_by_cols(df_train, df_test, Configs.LOCDT_CONAM_RECIPE_2)\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add conam-related feature v4\"):\n",
    "        df_train, df_test = group_target_by_cols(\n",
    "            df_train, \n",
    "            df_test, \n",
    "            Configs.CONAM_AGG_RECIPE_4,\n",
    "            )\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "    with timer(\"Add cano/mchno latent feature\"):\n",
    "        df = pd.read_csv(\"../fraud_detection/features/cano_latent_features_w_mchno.csv\")\n",
    "        df_train = df_train.merge(df, on = \"cano\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"cano\", how = \"left\")\n",
    "        df = pd.read_csv(\"../fraud_detection/features/cano_mchno_latent_features.csv\")\n",
    "        df_train = df_train.merge(df, on = \"mchno\", how = \"left\")\n",
    "        df_test = df_test.merge(df, on = \"mchno\", how = \"left\")\n",
    "\n",
    "        logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "        logger.info(\"Test application df shape: {}\".format(df_test.shape))   \n",
    "        \n",
    "    #return df_train, df_test\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    with timer(\"Run LightGBM with kfold\"):\n",
    "#         if args.feature_selection:\n",
    "#             logger.info(\"==============Feature Selection==============\")\n",
    "#             for df in [df_train, df_test]:\n",
    "#                 # drop random features (by null hypothesis)\n",
    "#                 df.drop(Configs.FEATURE_GRAVEYARD, axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "#                 # drop unused features features_with_no_imp_at_least_twice\n",
    "#                 df.drop(Configs.FEATURE_USELESSNESS, axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "#                 gc.collect()   \n",
    "#             logger.info(\"Train application df shape: {}\".format(df_train.shape))\n",
    "#             logger.info(\"Test application df shape: {}\".format(df_test.shape))\n",
    "\n",
    "        for df in [df_train, df_test]:\n",
    "            df.drop(columns = [\"loctm_hour_of_day\",\n",
    "                               \"loctm_minute_of_hour\", \n",
    "                               \"loctm_second_of_min\",\n",
    "                               \"day_hr_min\",\n",
    "                               \"day_hr_min_sec\",\n",
    "                               ], axis = 1, inplace = True)\n",
    "   \n",
    "    return df_train, df_test\n",
    "\n",
    "#         ITERATION = (5 if args.TEST_NULL_HYPO else 1)\n",
    "#         feature_importance_df = pd.DataFrame()\n",
    "#         over_iterations_val_auc = np.zeros(ITERATION)\n",
    "#         for i in range(ITERATION):\n",
    "#             logger.info('Iteration %i' %i)\n",
    "#             if args.model == \"lgb\":    \n",
    "#                 iter_feat_imp, over_folds_val_auc = kfold_lightgbm(df_train, df_test, num_folds = args.NUM_FOLDS, args = args, stratified = args.STRATIFIED, seed = args.SEED, logger = logger)\n",
    "#             elif args.model == \"xgb\":\n",
    "#                 iter_feat_imp, over_folds_val_auc = kfold_xgb(df_train, df_test, num_folds = args.NUM_FOLDS, args = args, stratified = args.STRATIFIED, seed = args.SEED, logger = logger)\n",
    "#             else:\n",
    "#                 print(\"Now we only support LightGBM or Xgboost model!\")           \n",
    "#             feature_importance_df = pd.concat([feature_importance_df, iter_feat_imp], axis=0)\n",
    "#             over_iterations_val_auc[i] = over_folds_val_auc\n",
    "\n",
    "#         logger.info('============================================\\nOver-iterations val f1 score %.6f' %over_iterations_val_auc.mean())\n",
    "#         logger.info('Standard deviation %.6f\\n============================================' %over_iterations_val_auc.std())\n",
    "    \n",
    "#     if args.feature_importance_plot == True:\n",
    "#         from util import display_importances\n",
    "#         display_importances(feature_importance_df, args.model)\n",
    "        \n",
    "#     feature_importance_df_median = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").median().sort_values(by=\"importance\", ascending=False)\n",
    "#     useless_features_df = feature_importance_df_median.loc[feature_importance_df_median['importance'] == 0]\n",
    "#     feature_importance_df_mean = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "#     if args.TEST_NULL_HYPO:\n",
    "#         feature_importance_df_mean.to_csv(\"../fraud_detection/result/feature_importance-null_hypo.csv\", index = True)\n",
    "#     else:\n",
    "#         feature_importance_df_mean.to_csv(\"../fraud_detection/result/feature_importance.csv\", index = True)\n",
    "#         useless_features_list = useless_features_df.index.tolist()\n",
    "#         logger.info('Useless features: \\'' + '\\', \\''.join(useless_features_list) + '\\'')\n",
    "#     return feature_importance_df_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_file': '/data/yunrui_li/fraud/dataset/train.csv',\n",
       " 'test_file': '/data/yunrui_li/fraud/dataset/test.csv',\n",
       " 'result_path': '/data/yunrui_li/fraud/fraud_detection/result/submission.csv',\n",
       " 'feature_selection': True,\n",
       " 'feature_importance_plot': False,\n",
       " 'SEED': 1030,\n",
       " 'NUM_FOLDS': 5,\n",
       " 'CPU_USE_RATE': 1.0,\n",
       " 'STRATIFIED': True,\n",
       " 'NUM_LEAVES': 31,\n",
       " 'COLSAMPLE_BYTREE': 1.0,\n",
       " 'SUBSAMPLE': 1.0,\n",
       " 'SUBSAMPLE_FREQ': 0,\n",
       " 'MAX_DEPTH': -1,\n",
       " 'REG_ALPHA': 0.0,\n",
       " 'REG_LAMBDA': 0.0,\n",
       " 'MIN_SPLIT_GAIN': 0.0,\n",
       " 'MIN_CHILD_WEIGHT': 0.001,\n",
       " 'MAX_BIN': 255,\n",
       " 'SCALE_POS_WEIGHT': 3,\n",
       " 'TEST_NULL_HYPO': False,\n",
       " 'model': 'lgb',\n",
       " 'ensemble': False,\n",
       " 'seed': 1030}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = {\n",
    " \"train_file\":\"/data/yunrui_li/fraud/dataset/train.csv\",\n",
    " \"test_file\":\"/data/yunrui_li/fraud/dataset/test.csv\",\n",
    " \"result_path\":\"/data/yunrui_li/fraud/fraud_detection/result/submission.csv\",\n",
    " \"feature_selection\":True,\n",
    " \"feature_importance_plot\": False,\n",
    " \"SEED\": 1030,\n",
    " \"NUM_FOLDS\": 5, # 5\n",
    " \"CPU_USE_RATE\":1.0,\n",
    " \"STRATIFIED\": True,\n",
    " \"NUM_LEAVES\":31,\n",
    " \"COLSAMPLE_BYTREE\":1.0,\n",
    " \"SUBSAMPLE\": 1.0,\n",
    " \"SUBSAMPLE_FREQ\": 0,\n",
    " \"MAX_DEPTH\": -1,\n",
    " \"REG_ALPHA\": 0.0,\n",
    " \"REG_LAMBDA\": 0.0,\n",
    " \"MIN_SPLIT_GAIN\": 0.0,\n",
    " \"MIN_CHILD_WEIGHT\": 0.001,\n",
    " \"MAX_BIN\": 255,\n",
    " \"SCALE_POS_WEIGHT\": 3,\n",
    " \"TEST_NULL_HYPO\":False,\n",
    " \"model\": \"lgb\",\n",
    " \"ensemble\":False,\n",
    " \"seed\":1030,\n",
    "}\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "args = AttrDict(args)\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train application df shape: (1521787, 29)\n",
      "Test application df shape: (421665, 28)\n",
      "Process train/test application - done in 67s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ldap_home/yunrui.li/.pyenv/versions/3.6.5/envs/fraud/lib/python3.6/site-packages/ipykernel_launcher.py:36: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train application df shape: (1521787, 43)\n",
      "Test application df shape: (421665, 42)\n",
      "Add bacno/cano feature - done in 17s\n",
      "Train application df shape: (1521787, 115)\n",
      "Test application df shape: (421665, 114)\n",
      "Add iterm-related feature - done in 107s\n",
      "Train application df shape: (1521787, 185)\n",
      "Test application df shape: (421665, 184)\n",
      "Add conam-related feature - done in 131s\n",
      "Train application df shape: (1521787, 209)\n",
      "Test application df shape: (421665, 208)\n",
      "Add hour-related feature - done in 452s\n",
      "Train application df shape: (1521787, 210)\n",
      "Test application df shape: (421665, 209)\n",
      "Add cano/conam feature - done in 48s\n",
      "Train application df shape: (1521787, 230)\n",
      "Test application df shape: (421665, 229)\n",
      "Add cano/bacno latent feature - done in 5s\n",
      "Train application df shape: (1521787, 265)\n",
      "Test application df shape: (421665, 264)\n",
      "Add locdt-related feature - done in 256s\n",
      "Train application df shape: (1521787, 272)\n",
      "Test application df shape: (421665, 271)\n",
      "Add mchno-related feature - done in 114s\n",
      "Train application df shape: (1521787, 279)\n",
      "Test application df shape: (421665, 278)\n",
      "Add scity-related feature - done in 116s\n",
      "Train application df shape: (1521787, 286)\n",
      "Test application df shape: (421665, 285)\n",
      "Add stocn-related feature - done in 120s\n",
      "Train application df shape: (1521787, 306)\n",
      "Test application df shape: (421665, 305)\n",
      "Add mchno/bacno latent feature - done in 5s\n",
      "Train application df shape: (1521787, 320)\n",
      "Test application df shape: (421665, 319)\n",
      "Add time second-level feature on bacno - done in 196s\n",
      "Train application df shape: (1521787, 334)\n",
      "Test application df shape: (421665, 333)\n",
      "Add time second-level feature on cano - done in 255s\n",
      "Train application df shape: (1521787, 348)\n",
      "Test application df shape: (421665, 347)\n",
      "Add time second-level feature on mchno - done in 265s\n",
      "Train application df shape: (1521787, 390)\n",
      "Test application df shape: (421665, 389)\n",
      "Add time second-level feature on csmcu/stocn/scity - done in 428s\n",
      "Train application df shape: (1521787, 446)\n",
      "Test application df shape: (421665, 445)\n",
      "Add time second-level feature on acqic/csmcu/stocn/scity - done in 662s\n",
      "Train application df shape: (1521787, 488)\n",
      "Test application df shape: (421665, 487)\n",
      "Add conam-related feature v3 - done in 439s\n",
      "Train application df shape: (1521787, 572)\n",
      "Test application df shape: (421665, 571)\n",
      "Add locdt-related feature v2 - done in 574s\n",
      "Train application df shape: (1521787, 641)\n",
      "Test application df shape: (421665, 640)\n",
      "Add conam-related feature v4 - done in 741s\n",
      "Train application df shape: (1521787, 661)\n",
      "Test application df shape: (421665, 660)\n",
      "Add cano/mchno latent feature - done in 11s\n",
      "Run LightGBM with kfold - done in 16s\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ldap_home/yunrui.li/.pyenv/versions/3.6.5/envs/fraud/lib/python3.6/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([df_train,df_test], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(\"../fraud_detection/features/features.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x1400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# csmcu & scity positive correlation (幣別與消費城市)\n",
    "plt.figure(figsize = (14,14))\n",
    "plt.title('Credit Card Transactions features correlation plot (Pearson)')\n",
    "sns.heatmap(corr,xticklabels=corr.columns,yticklabels=corr.columns,linewidths=.1,cmap=\"Reds\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a8784ddd4e15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'corr' is not defined"
     ]
    }
   ],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "th = 0.990\n",
    "f = []\n",
    "feature_seen = []\n",
    "for feat1 in corr.columns.tolist():\n",
    "    for feat2, value in corr[feat1].iteritems():\n",
    "        if feat1!=feat2:\n",
    "            if value > th or value < (-1)* th:\n",
    "                if feat2 not in feature_seen:\n",
    "                    f.append((feat1,feat2,value))\n",
    "                    feature_seen.append(feat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_GRAVEYARD = []\n",
    "for fea_pair in f:\n",
    "    if fea_pair[0]!=fea_pair[1]:\n",
    "        if (fea_pair[0]) not in FEATURE_GRAVEYARD and (fea_pair[1]) not in FEATURE_GRAVEYARD:\n",
    "                FEATURE_GRAVEYARD.append(fea_pair[0])\n",
    "            \n",
    "FEATURE_GRAVEYARD = list(set(FEATURE_GRAVEYARD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(FEATURE_GRAVEYARD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sum_conam_BY_bacno_flbmk',\n",
       " 'max_conam_BY_acqic_day_hr_min_sec',\n",
       " 'max_iterm_BY_csmcu',\n",
       " 'var_conam_BY_bacno',\n",
       " 'count_conam_BY_cano_locdt_scity',\n",
       " 'mean_conam_BY_bacno_scity_day_hr_min',\n",
       " 'median_conam_BY_bacno_scity_day_hr_min',\n",
       " 'min_conam_BY_bacno_scity_day_hr_min',\n",
       " 'mean_conam_BY_cano_locdt_scity',\n",
       " 'mean_conam_BY_bacno_stocn_day_hr_min',\n",
       " 'mean_iterm_BY_stocn',\n",
       " 'sum_conam_BY_bacno_locdt_mchno',\n",
       " 'max_conam_BY_bacno_day_hr_min_sec',\n",
       " 'min_conam_BY_bacno_cano',\n",
       " 'max_conam_BY_bacno_day_hr_min',\n",
       " 'max_conam_BY_bacno_locdt_stocn_scity',\n",
       " 'max_conam_BY_bacno_stocn_day_hr_min_sec',\n",
       " 'conam',\n",
       " 'max_conam_BY_bacno',\n",
       " 'median_conam_BY_cano',\n",
       " 'count_conam_BY_csmcu',\n",
       " 'min_conam_BY_bacno_acqic_day_hr_min_sec',\n",
       " 'count_conam_BY_scity_locdt',\n",
       " 'mean_conam_BY_bacno_locdt_stocn',\n",
       " 'min_iterm_BY_bacno_cano',\n",
       " 'sum_conam_BY_stscd',\n",
       " 'mean_conam_BY_bacno',\n",
       " 'count_conam_BY_bacno_cano',\n",
       " 'count_conam_BY_bacno_stocn_day_hr_min_sec',\n",
       " 'var_conam_BY_bacno_locdt_mchno',\n",
       " 'stscd',\n",
       " 'count_conam_BY_bacno_acqic_day_hr_min_sec',\n",
       " 'sum_conam_BY_bacno_stocn_day_hr_min_sec',\n",
       " 'count_conam_BY_bacno_day_hr_min_sec',\n",
       " 'mean_iterm_BY_acqic',\n",
       " 'count_conam_BY_bacno_locdt_mchno',\n",
       " 'mean_iterm_BY_bacno_cano',\n",
       " 'sum_conam_BY_bacno_cano',\n",
       " 'sum_conam_BY_bacno_acqic_day_hr_min_sec',\n",
       " 'sum_conam_BY_bacno_acqic_day_hr_min',\n",
       " 'median_conam_BY_bacno_day_hr_min_sec',\n",
       " 'mean_conam_BY_mchno_day_hr_min',\n",
       " 'sum_conam_BY_bacno',\n",
       " 'sum_conam_BY_bacno_scity_day_hr_min_sec',\n",
       " 'sum_conam_BY_contp',\n",
       " 'mean_conam_BY_bacno_mchno',\n",
       " 'median_conam_BY_cano_day_hr_min_sec',\n",
       " 'median_conam_BY_ovrlt',\n",
       " 'max_conam_BY_cano',\n",
       " 'max_conam_BY_bacno_acqic_day_hr_min',\n",
       " 'min_conam_BY_bacno_acqic_day_hr_min',\n",
       " 'count_conam_BY_bacno_day_hr_min',\n",
       " 'mean_conam_BY_flbmk',\n",
       " 'median_conam_BY_cano_day_hr_min',\n",
       " 'sum_iterm_BY_bacno_cano',\n",
       " 'sum_conam_BY_bacno_locdt_scity',\n",
       " 'sum_conam_BY_loctm_hour_of_day_bacno',\n",
       " 'count_conam_BY_bacno',\n",
       " 'max_conam_BY_flbmk',\n",
       " 'sum_conam_BY_ovrlt',\n",
       " 'var_conam_BY_bacno_locdt_scity',\n",
       " 'count_conam_BY_bacno_locdt_scity',\n",
       " 'median_conam_BY_bacno_acqic_day_hr_min_sec',\n",
       " 'max_conam_BY_bacno_locdt_scity',\n",
       " 'mean_conam_BY_stocn_day_hr_min_sec',\n",
       " 'count_conam_BY_hcefg',\n",
       " 'mean_conam_BY_cano',\n",
       " 'min_conam_BY_bacno_scity_day_hr_min_sec',\n",
       " 'sum_conam_BY_stocn',\n",
       " 'max_conam_BY_bacno_acqic_day_hr_min_sec',\n",
       " 'mean_conam_BY_cano_locdt_stocn_scity',\n",
       " 'median_conam_BY_bacno_acqic_day_hr_min',\n",
       " 'median_conam_BY_bacno_locdt_scity',\n",
       " 'var_conam_BY_bacno_cano',\n",
       " 'max_conam_BY_bacno_cano',\n",
       " 'mean_conam_BY_scity_day_hr_min_sec',\n",
       " 'count_conam_BY_flbmk',\n",
       " 'mean_conam_BY_cano_locdt_stocn',\n",
       " 'min_conam_BY_bacno_day_hr_min_sec',\n",
       " 'max_conam_BY_bacno_locdt_mchno',\n",
       " 'mean_conam_BY_mchno_day_hr_min_sec',\n",
       " 'count_conam_BY_stocn_locdt',\n",
       " 'median_conam_BY_bacno_locdt_stocn_scity',\n",
       " 'count_conam_BY_contp_locdt',\n",
       " 'count_conam_BY_scity',\n",
       " 'mean_conam_BY_bacno_locdt_stocn_scity',\n",
       " 'mean_conam_BY_bacno_acqic_day_hr_min',\n",
       " 'median_conam_BY_mchno_day_hr_min_sec',\n",
       " 'mean_conam_BY_bacno_stocn_day_hr_min_sec',\n",
       " 'mean_conam_BY_acqic_day_hr_min_sec',\n",
       " 'count_conam_BY_stocn',\n",
       " 'max_conam_BY_ovrlt',\n",
       " 'min_conam_BY_cano_day_hr_min_sec',\n",
       " 'max_conam_BY_mchno_day_hr_min_sec',\n",
       " 'min_conam_BY_bacno_stocn_day_hr_min_sec',\n",
       " 'mean_conam_BY_cano_day_hr_min',\n",
       " 'mean_conam_BY_bacno_day_hr_min',\n",
       " 'var_conam_BY_cano_locdt_scity',\n",
       " 'sum_conam_BY_cano_locdt_scity',\n",
       " 'count_conam_BY_bacno_locdt_stocn_scity',\n",
       " 'max_conam_BY_cano_locdt_scity',\n",
       " 'mean_conam_BY_csmcu_day_hr_min_sec',\n",
       " 'min_conam_BY_bacno_stocn_day_hr_min',\n",
       " 'var_conam_BY_bacno_scity_day_hr_min_sec',\n",
       " 'sum_conam_BY_cano',\n",
       " 'var_iterm_BY_bacno_cano',\n",
       " 'min_conam_BY_bacno_locdt_stocn_scity',\n",
       " 'count_conam_BY_bacno_locdt',\n",
       " 'count_conam_BY_contp',\n",
       " 'var_conam_BY_cano',\n",
       " 'median_conam_BY_cano_locdt_scity',\n",
       " 'mean_conam_BY_cano_day_hr_min_sec',\n",
       " 'min_conam_BY_bacno_day_hr_min',\n",
       " 'var_conam_BY_bacno_day_hr_min_sec',\n",
       " 'median_conam_BY_acqic_day_hr_min_sec',\n",
       " 'var_conam_BY_bacno_acqic_day_hr_min_sec',\n",
       " 'median_conam_BY_bacno_cano',\n",
       " 'min_conam_BY_bacno_locdt_mchno',\n",
       " 'count_conam_BY_bacno_locdt_stocn',\n",
       " 'median_conam_BY_bacno_scity_day_hr_min_sec',\n",
       " 'min_conam_BY_bacno_locdt_scity',\n",
       " 'mean_conam_BY_cano_locdt_mchno',\n",
       " 'median_iterm_BY_bacno_cano',\n",
       " 'var_conam_BY_stscd',\n",
       " 'mean_conam_BY_bacno_day_hr_min_sec',\n",
       " 'max_conam_BY_bacno_scity_day_hr_min',\n",
       " 'count_conam_BY_stscd',\n",
       " 'min_conam_BY_acqic_day_hr_min_sec',\n",
       " 'max_conam_BY_bacno_scity_day_hr_min_sec',\n",
       " 'median_conam_BY_flbmk',\n",
       " 'sum_iterm_BY_csmcu',\n",
       " 'sum_iterm_BY_stocn',\n",
       " 'mean_iterm_BY_scity',\n",
       " 'min_conam_BY_cano_locdt_scity',\n",
       " 'max_iterm_BY_bacno_cano',\n",
       " 'mean_conam_BY_ovrlt',\n",
       " 'median_conam_BY_bacno_day_hr_min',\n",
       " 'mean_iterm_BY_stscd',\n",
       " 'var_conam_BY_bacno_day_hr_min',\n",
       " 'sum_conam_BY_bacno_day_hr_min_sec',\n",
       " 'mean_conam_BY_bacno_acqic_day_hr_min_sec',\n",
       " 'median_conam_BY_bacno_stocn_day_hr_min',\n",
       " 'mean_conam_BY_bacno_cano',\n",
       " 'mean_conam_BY_bacno_locdt_mchno',\n",
       " 'count_conam_BY_ovrlt',\n",
       " 'mean_conam_BY_bacno_locdt_scity',\n",
       " 'mean_conam_BY_bacno_scity_day_hr_min_sec',\n",
       " 'median_conam_BY_bacno_locdt_mchno',\n",
       " 'max_conam_BY_stscd',\n",
       " 'sum_conam_BY_bacno_locdt_stocn_scity',\n",
       " 'mean_iterm_BY_csmcu',\n",
       " 'var_conam_BY_bacno_stocn_day_hr_min_sec',\n",
       " 'count_conam_BY_bacno_scity_day_hr_min_sec',\n",
       " 'max_iterm_BY_stocn',\n",
       " 'max_conam_BY_bacno_stocn_day_hr_min',\n",
       " 'max_conam_BY_cano_day_hr_min_sec',\n",
       " 'median_conam_BY_bacno',\n",
       " 'sum_conam_BY_flbmk',\n",
       " 'sum_conam_BY_csmcu',\n",
       " 'sum_conam_BY_bacno_locdt_stocn',\n",
       " 'sum_conam_BY_bacno_day_hr_min',\n",
       " 'median_conam_BY_bacno_stocn_day_hr_min_sec',\n",
       " 'max_conam_BY_stocn']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FEATURE_GRAVEYARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('max_conam_BY_bacno', 'max_conam_BY_bacno_flbmk', 0.9947683007357537)\n"
     ]
    }
   ],
   "source": [
    "for pair in f:\n",
    "    if pair[0]==\"max_conam_BY_bacno\":\n",
    "        print (pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
