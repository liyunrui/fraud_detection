{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ldap_home/yunrui.li/.pyenv/versions/3.6.5/envs/deepts/lib/python3.6/site-packages/ipykernel_launcher.py:105: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training data (1390382, 27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/18 [00:00<?, ?it/s]/ldap_home/yunrui.li/.pyenv/versions/3.6.5/envs/deepts/lib/python3.6/site-packages/ipykernel_launcher.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 18/18 [00:33<00:00,  1.84s/it]\n",
      "  0%|          | 0/23 [00:00<?, ?it/s]/ldap_home/yunrui.li/.pyenv/versions/3.6.5/envs/deepts/lib/python3.6/site-packages/ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 23/23 [00:51<00:00,  2.26s/it]\n",
      "/ldap_home/yunrui.li/.pyenv/versions/3.6.5/envs/deepts/lib/python3.6/site-packages/pandas/core/frame.py:4102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append(\"../fraud_detection/src/\")\n",
    "from util import s_to_time_format, string_to_datetime,hour_to_range\n",
    "from tqdm import tqdm\n",
    "\n",
    "def value_to_count(df_train, df_test, df_train_normal_cano_id, df_):\n",
    "    # separate continuous feature and categorial features\n",
    "    feats = ['acqic', 'bacno', 'cano', 'conam', 'contp', 'csmcu', 'ecfg', 'etymd',\n",
    "       'flbmk', 'flg_3dsmk', 'hcefg', 'insfg', 'iterm', 'locdt',\n",
    "       'mcc', 'mchno', 'ovrlt', 'scity', 'stocn', 'stscd', 'loctm_hour_of_day',\n",
    "       'loctm_minute_of_hour', 'loctm_second_of_min'] \n",
    "    cont_feats = ['iterm', \n",
    "                  'locdt',\n",
    "                  'loctm_hour_of_day',\n",
    "                  'loctm_minute_of_hour', \n",
    "                  'loctm_second_of_min']\n",
    "    feats = [f for f in feats if f not in cont_feats]\n",
    "    # we only coner categorial features\n",
    "    \n",
    "    df = pd.concat([df_train[feats], df_test[feats]], axis = 0)\n",
    "    for f in tqdm(feats):\n",
    "        count_dict = df[f].value_counts(dropna = False).to_dict() \n",
    "        df_train_normal_cano_id[f] = df_train_normal_cano_id[f].apply(lambda v: count_dict[v])\n",
    "        df_train[f] = df_train[f].apply(lambda v: count_dict[v])\n",
    "        df_test[f] = df_test[f].apply(lambda v: count_dict[v])\n",
    "        df_[f] = df_[f].apply(lambda v: count_dict[v])\n",
    "    return df_train,df_test,df_train_normal_cano_id, df_\n",
    "\n",
    "def feature_normalization_auto(df_train, df_test, df_train_normal_cano_id,df_):\n",
    "    \"\"\"\n",
    "    return two inputs of autoencoder, one is for train and another one is for test\n",
    "    \"\"\"\n",
    "    #from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n",
    "    feats = ['acqic', 'bacno', 'cano', 'conam', 'contp', 'csmcu', 'ecfg', 'etymd',\n",
    "       'flbmk', 'flg_3dsmk', 'hcefg', 'insfg', 'iterm', 'locdt',\n",
    "       'mcc', 'mchno', 'ovrlt', 'scity', 'stocn', 'stscd', 'loctm_hour_of_day',\n",
    "       'loctm_minute_of_hour', 'loctm_second_of_min']\n",
    "    df = pd.concat([df_train[feats], df_test[feats]], axis = 0)\n",
    "\n",
    "\n",
    "    for f in tqdm(feats):\n",
    "        try:\n",
    "            #scaler = MinMaxScaler()\n",
    "            max_ = df[f].max()\n",
    "            min_ = df[f].min()\n",
    "            df_train_normal_cano_id[f] = df_train_normal_cano_id[f].apply(lambda x: (x-min_)/(max_-min_))\n",
    "            df_[f] = df_[f].apply(lambda x: (x-min_)/(max_-min_))\n",
    "            #df_test[f] = df_test[f].apply(lambda x: (x-min_)/(max_-min_))\n",
    "        except:\n",
    "            print(f)\n",
    "    return df_train_normal_cano_id,df_\n",
    "\n",
    "def partition_(df, num_features):\n",
    "    data = []\n",
    "    for i in range(len(df)):\n",
    "        out = None\n",
    "        if i == 0:\n",
    "            out = np.concatenate(((np.zeros((2,num_features))),df.iloc[:1].values))\n",
    "        elif i== 1:\n",
    "            out = np.concatenate(((np.zeros((1,num_features))),df.iloc[:i+1].values))\n",
    "        else:\n",
    "            out = df.iloc[i+1-3:i+1].values\n",
    "        data.append(out)\n",
    "    return data\n",
    "\n",
    "def partition(df_, sequence_length = 3):\n",
    "    feats = [f for f in df_.columns if f not in {\"fraud_ind\",\"cano_help\",\"locdt_help\"}]\n",
    "    sequences = []\n",
    "    for _, df in df_.groupby(by = \"cano_help\"):\n",
    "        data = partition_(df[feats], num_features = len(feats))\n",
    "        for d in data:\n",
    "            sequences.append(d)\n",
    "    return sequences\n",
    "\n",
    "df_train = pd.read_csv(\"/data/yunrui_li/fraud/dataset/train.csv\")\n",
    "df_test = pd.read_csv(\"/data/yunrui_li/fraud/dataset/test.csv\")\n",
    "\n",
    "\n",
    "for df in [df_train, df_test]:\n",
    "    # pre-processing\n",
    "    df[\"loctm_\"] = df.loctm.astype(int).astype(str)\n",
    "    df.loctm_ = df.loctm_.apply(s_to_time_format).apply(string_to_datetime)\n",
    "    # time-related feature\n",
    "    df[\"loctm_hour_of_day\"] = df.loctm_.apply(lambda x: x.hour)\n",
    "    df[\"loctm_minute_of_hour\"] = df.loctm_.apply(lambda x: x.minute)\n",
    "    df[\"loctm_second_of_min\"] = df.loctm_.apply(lambda x: x.second)\n",
    "\n",
    "    # removed the columns no need\n",
    "    df.drop(columns = [\"loctm_\", \"loctm\",\"txkey\"], axis = 1, inplace = True)\n",
    "\n",
    "df_train[\"cano_locdt_index\"] = [\"{}_{}\".format(str(i),str(j)) for i,j in zip(df_train.cano,df_train.locdt)]\n",
    "df_test[\"cano_locdt_index\"] = [\"{}_{}\".format(str(i),str(j)) for i,j in zip(df_test.cano,df_test.locdt)]\n",
    "\n",
    "df_train[\"cano_help\"] = df_train.cano\n",
    "df_test[\"cano_help\"] = df_test.cano\n",
    "\n",
    "df_train[\"locdt_help\"] = df_train.locdt\n",
    "df_test[\"locdt_help\"] = df_test.locdt\n",
    "\n",
    "#-----------------------------\n",
    "# feature extraction\n",
    "#-----------------------------\n",
    "df = pd.concat([df_train, df_test], axis = 0)\n",
    "df.sort_values(by = [\"cano\", \"locdt\"], inplace = True)\n",
    "\n",
    "#-----------------------------\n",
    "# prepare training data\n",
    "#-----------------------------\n",
    "df_train.sort_values(by = [\"cano\", \"locdt\"], inplace = True)\n",
    "\n",
    "# df_train, df_test = value_to_count(df_train, df_test)\n",
    "# df_train, df_test = feature_normalization_auto(df_train, df_test)\n",
    "\n",
    "fraud_cano_id = df_train[df_train.fraud_ind == 1].cano.unique().tolist()\n",
    "\n",
    "df_train_normal_cano_id = df_train[~df_train.cano.isin(fraud_cano_id)]\n",
    "print (\"number of training data\",df_train_normal_cano_id.shape)\n",
    "\n",
    "df_train, df_test, df_train_normal_cano_id, df = value_to_count(df_train, df_test,df_train_normal_cano_id, df)\n",
    "df_train_normal_cano_id, df = feature_normalization_auto(df_train, df_test,df_train_normal_cano_id, df)\n",
    "\n",
    "#-----------------------------\n",
    "# post-processing\n",
    "#-----------------------------\n",
    "df.drop(columns = [\"fraud_ind\"], axis = 1, inplace = True)\n",
    "df_train_normal_cano_id.drop(columns = [\"fraud_ind\"], axis = 1, inplace = True)\n",
    "feats = ['acqic', 'bacno', 'cano', 'conam', 'contp', 'csmcu', 'ecfg', 'etymd',\n",
    "   'flbmk', 'flg_3dsmk', 'hcefg', 'insfg', 'iterm', 'locdt',\n",
    "   'mcc', 'mchno', 'ovrlt', 'scity', 'stocn', 'stscd', 'loctm_hour_of_day',\n",
    "   'loctm_minute_of_hour', 'loctm_second_of_min'] + [\"cano_locdt_index\",\"cano_help\",\"locdt_help\"]\n",
    "\n",
    "df = df[feats]\n",
    "df_train_normal_cano_id = df_train_normal_cano_id[feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8730"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fraud_cano_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1390382, 26)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_normal_cano_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acqic</th>\n",
       "      <th>bacno</th>\n",
       "      <th>cano</th>\n",
       "      <th>conam</th>\n",
       "      <th>contp</th>\n",
       "      <th>csmcu</th>\n",
       "      <th>ecfg</th>\n",
       "      <th>etymd</th>\n",
       "      <th>flbmk</th>\n",
       "      <th>flg_3dsmk</th>\n",
       "      <th>...</th>\n",
       "      <th>ovrlt</th>\n",
       "      <th>scity</th>\n",
       "      <th>stocn</th>\n",
       "      <th>stscd</th>\n",
       "      <th>loctm_hour_of_day</th>\n",
       "      <th>loctm_minute_of_hour</th>\n",
       "      <th>loctm_second_of_min</th>\n",
       "      <th>cano_locdt_index</th>\n",
       "      <th>cano_help</th>\n",
       "      <th>locdt_help</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>284164</td>\n",
       "      <td>0.245115</td>\n",
       "      <td>0.007168</td>\n",
       "      <td>0.007168</td>\n",
       "      <td>0.014266</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.322034</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0_1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1185304</td>\n",
       "      <td>0.245115</td>\n",
       "      <td>0.007168</td>\n",
       "      <td>0.007168</td>\n",
       "      <td>0.002860</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.118644</td>\n",
       "      <td>0_4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>726295</td>\n",
       "      <td>0.794559</td>\n",
       "      <td>0.007168</td>\n",
       "      <td>0.007168</td>\n",
       "      <td>0.047804</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.118140</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.898305</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>0_20</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155960</td>\n",
       "      <td>0.245115</td>\n",
       "      <td>0.007168</td>\n",
       "      <td>0.007168</td>\n",
       "      <td>0.012381</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>0.728814</td>\n",
       "      <td>0_29</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>418445</td>\n",
       "      <td>0.245115</td>\n",
       "      <td>0.007168</td>\n",
       "      <td>0.007168</td>\n",
       "      <td>0.003907</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.627119</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0_37</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840043</td>\n",
       "      <td>0.001933</td>\n",
       "      <td>0.026882</td>\n",
       "      <td>0.009857</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.028231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.612633</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.025884</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>0.949153</td>\n",
       "      <td>213329_89</td>\n",
       "      <td>213329</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1506705</td>\n",
       "      <td>0.032094</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>0.004620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010258</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.610169</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>213334_89</td>\n",
       "      <td>213334</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1509473</td>\n",
       "      <td>0.032094</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>0.002902</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010258</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>0.661017</td>\n",
       "      <td>213334_90</td>\n",
       "      <td>213334</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1509510</td>\n",
       "      <td>0.365315</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010258</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>213334_90</td>\n",
       "      <td>213334</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1514015</td>\n",
       "      <td>0.041838</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010258</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.004466</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.627119</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>213334_90</td>\n",
       "      <td>213334</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1390382 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            acqic     bacno      cano     conam  contp     csmcu  ecfg  \\\n",
       "284164   0.245115  0.007168  0.007168  0.014266    1.0  1.000000   1.0   \n",
       "1185304  0.245115  0.007168  0.007168  0.002860    1.0  1.000000   1.0   \n",
       "726295   0.794559  0.007168  0.007168  0.047804    1.0  1.000000   1.0   \n",
       "155960   0.245115  0.007168  0.007168  0.012381    1.0  1.000000   1.0   \n",
       "418445   0.245115  0.007168  0.007168  0.003907    1.0  1.000000   1.0   \n",
       "...           ...       ...       ...       ...    ...       ...   ...   \n",
       "840043   0.001933  0.026882  0.009857  0.000018    1.0  0.028231   0.0   \n",
       "1506705  0.032094  0.002688  0.002688  0.004620    1.0  0.010258   1.0   \n",
       "1509473  0.032094  0.002688  0.002688  0.002902    1.0  0.010258   1.0   \n",
       "1509510  0.365315  0.002688  0.002688  1.000000    1.0  0.010258   1.0   \n",
       "1514015  0.041838  0.002688  0.002688  0.001183    1.0  0.010258   1.0   \n",
       "\n",
       "            etymd     flbmk  flg_3dsmk  ...  ovrlt     scity     stocn  stscd  \\\n",
       "284164   0.783407  1.000000        1.0  ...    1.0  1.000000  1.000000    1.0   \n",
       "1185304  0.783407  1.000000        1.0  ...    1.0  1.000000  1.000000    1.0   \n",
       "726295   0.783407  1.000000        1.0  ...    1.0  0.118140  1.000000    1.0   \n",
       "155960   0.783407  1.000000        1.0  ...    1.0  1.000000  1.000000    1.0   \n",
       "418445   0.783407  1.000000        1.0  ...    1.0  1.000000  1.000000    1.0   \n",
       "...           ...       ...        ...  ...    ...       ...       ...    ...   \n",
       "840043   0.612633  1.000000        1.0  ...    1.0  0.000281  0.025884    1.0   \n",
       "1506705  1.000000  0.005172        0.0  ...    1.0  0.000190  1.000000    1.0   \n",
       "1509473  1.000000  0.005172        0.0  ...    1.0  0.000190  1.000000    1.0   \n",
       "1509510  1.000000  0.005172        0.0  ...    1.0  1.000000  1.000000    1.0   \n",
       "1514015  1.000000  0.005172        0.0  ...    1.0  0.004466  1.000000    1.0   \n",
       "\n",
       "         loctm_hour_of_day  loctm_minute_of_hour  loctm_second_of_min  \\\n",
       "284164            0.652174              0.322034             0.813559   \n",
       "1185304           0.652174              0.745763             0.118644   \n",
       "726295            0.608696              0.898305             0.711864   \n",
       "155960            0.652174              0.372881             0.728814   \n",
       "418445            0.608696              0.627119             0.169492   \n",
       "...                    ...                   ...                  ...   \n",
       "840043            0.826087              0.711864             0.949153   \n",
       "1506705           0.695652              0.610169             0.745763   \n",
       "1509473           0.826087              0.779661             0.661017   \n",
       "1509510           0.652174              0.779661             0.711864   \n",
       "1514015           0.565217              0.627119             0.372881   \n",
       "\n",
       "         cano_locdt_index  cano_help  locdt_help  \n",
       "284164                0_1          0           1  \n",
       "1185304               0_4          0           4  \n",
       "726295               0_20          0          20  \n",
       "155960               0_29          0          29  \n",
       "418445               0_37          0          37  \n",
       "...                   ...        ...         ...  \n",
       "840043          213329_89     213329          89  \n",
       "1506705         213334_89     213334          89  \n",
       "1509473         213334_90     213334          90  \n",
       "1509510         213334_90     213334          90  \n",
       "1514015         213334_90     213334          90  \n",
       "\n",
       "[1390382 rows x 26 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_normal_cano_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1943452"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1943452, 26)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acqic</th>\n",
       "      <th>bacno</th>\n",
       "      <th>cano</th>\n",
       "      <th>conam</th>\n",
       "      <th>contp</th>\n",
       "      <th>csmcu</th>\n",
       "      <th>ecfg</th>\n",
       "      <th>etymd</th>\n",
       "      <th>flbmk</th>\n",
       "      <th>flg_3dsmk</th>\n",
       "      <th>hcefg</th>\n",
       "      <th>insfg</th>\n",
       "      <th>iterm</th>\n",
       "      <th>locdt</th>\n",
       "      <th>mcc</th>\n",
       "      <th>mchno</th>\n",
       "      <th>ovrlt</th>\n",
       "      <th>scity</th>\n",
       "      <th>stocn</th>\n",
       "      <th>stscd</th>\n",
       "      <th>loctm_hour_of_day</th>\n",
       "      <th>loctm_minute_of_hour</th>\n",
       "      <th>loctm_second_of_min</th>\n",
       "      <th>cano_locdt_index</th>\n",
       "      <th>cano_help</th>\n",
       "      <th>locdt_help</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>284164</td>\n",
       "      <td>0.245115</td>\n",
       "      <td>0.007168</td>\n",
       "      <td>0.007168</td>\n",
       "      <td>0.014266</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.077168</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.322034</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0_1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1185304</td>\n",
       "      <td>0.245115</td>\n",
       "      <td>0.007168</td>\n",
       "      <td>0.007168</td>\n",
       "      <td>0.002860</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025210</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.077168</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.118644</td>\n",
       "      <td>0_4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>726295</td>\n",
       "      <td>0.794559</td>\n",
       "      <td>0.007168</td>\n",
       "      <td>0.007168</td>\n",
       "      <td>0.047804</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.159664</td>\n",
       "      <td>0.069285</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.118140</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.898305</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>0_20</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155960</td>\n",
       "      <td>0.245115</td>\n",
       "      <td>0.007168</td>\n",
       "      <td>0.007168</td>\n",
       "      <td>0.012381</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.077168</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>0.728814</td>\n",
       "      <td>0_29</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>418445</td>\n",
       "      <td>0.245115</td>\n",
       "      <td>0.007168</td>\n",
       "      <td>0.007168</td>\n",
       "      <td>0.003907</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.302521</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.077168</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.627119</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0_37</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104903</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053372</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.991597</td>\n",
       "      <td>0.328259</td>\n",
       "      <td>0.039895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.949153</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>213570_119</td>\n",
       "      <td>213570</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52429</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.001792</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.142137</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.324602</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.991597</td>\n",
       "      <td>0.110069</td>\n",
       "      <td>0.010656</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.196151</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>213571_119</td>\n",
       "      <td>213571</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52430</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001792</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>0.002093</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.991597</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.047051</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.026956</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>213571_119</td>\n",
       "      <td>213571</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>399288</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.053372</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.328259</td>\n",
       "      <td>0.039895</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>213572_120</td>\n",
       "      <td>213572</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>409757</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053372</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.328259</td>\n",
       "      <td>0.039895</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.389831</td>\n",
       "      <td>213575_120</td>\n",
       "      <td>213575</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1943452 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            acqic     bacno      cano     conam     contp     csmcu  ecfg  \\\n",
       "284164   0.245115  0.007168  0.007168  0.014266  1.000000  1.000000   1.0   \n",
       "1185304  0.245115  0.007168  0.007168  0.002860  1.000000  1.000000   1.0   \n",
       "726295   0.794559  0.007168  0.007168  0.047804  1.000000  1.000000   1.0   \n",
       "155960   0.245115  0.007168  0.007168  0.012381  1.000000  1.000000   1.0   \n",
       "418445   0.245115  0.007168  0.007168  0.003907  1.000000  1.000000   1.0   \n",
       "...           ...       ...       ...       ...       ...       ...   ...   \n",
       "104903   1.000000  0.000000  0.000000  0.000000  0.053372  1.000000   0.0   \n",
       "52429    0.000333  0.001792  0.000896  1.000000  1.000000  0.142137   1.0   \n",
       "52430    1.000000  0.001792  0.000896  0.002093  1.000000  1.000000   1.0   \n",
       "399288   1.000000  0.000000  0.000000  0.000006  0.053372  1.000000   0.0   \n",
       "409757   1.000000  0.000000  0.000000  0.000000  0.053372  1.000000   0.0   \n",
       "\n",
       "            etymd  flbmk  flg_3dsmk  hcefg  insfg  iterm     locdt       mcc  \\\n",
       "284164   0.783407    1.0        1.0    1.0    1.0    0.0  0.000000  1.000000   \n",
       "1185304  0.783407    1.0        1.0    1.0    1.0    0.0  0.025210  1.000000   \n",
       "726295   0.783407    1.0        1.0    1.0    1.0    0.0  0.159664  0.069285   \n",
       "155960   0.783407    1.0        1.0    1.0    1.0    0.0  0.235294  1.000000   \n",
       "418445   0.783407    1.0        1.0    1.0    1.0    0.0  0.302521  1.000000   \n",
       "...           ...    ...        ...    ...    ...    ...       ...       ...   \n",
       "104903   0.955971    1.0        1.0    1.0    1.0    0.0  0.991597  0.328259   \n",
       "52429    0.324602    1.0        1.0    1.0    1.0    0.0  0.991597  0.110069   \n",
       "52430    1.000000    1.0        1.0    1.0    1.0    0.0  0.991597  1.000000   \n",
       "399288   0.955971    1.0        1.0    1.0    1.0    0.0  1.000000  0.328259   \n",
       "409757   0.955971    1.0        1.0    1.0    1.0    0.0  1.000000  0.328259   \n",
       "\n",
       "            mchno  ovrlt     scity  stocn  stscd  loctm_hour_of_day  \\\n",
       "284164   0.077168    1.0  1.000000    1.0    1.0           0.652174   \n",
       "1185304  0.077168    1.0  1.000000    1.0    1.0           0.652174   \n",
       "726295   0.000110    1.0  0.118140    1.0    1.0           0.608696   \n",
       "155960   0.077168    1.0  1.000000    1.0    1.0           0.652174   \n",
       "418445   0.077168    1.0  1.000000    1.0    1.0           0.608696   \n",
       "...           ...    ...       ...    ...    ...                ...   \n",
       "104903   0.039895    0.0  1.000000    1.0    1.0           0.565217   \n",
       "52429    0.010656    1.0  0.196151    1.0    1.0           0.913043   \n",
       "52430    0.047051    1.0  0.026956    1.0    1.0           0.913043   \n",
       "399288   0.039895    1.0  1.000000    1.0    1.0           0.608696   \n",
       "409757   0.039895    1.0  1.000000    1.0    1.0           0.478261   \n",
       "\n",
       "         loctm_minute_of_hour  loctm_second_of_min cano_locdt_index  \\\n",
       "284164               0.322034             0.813559              0_1   \n",
       "1185304              0.745763             0.118644              0_4   \n",
       "726295               0.898305             0.711864             0_20   \n",
       "155960               0.372881             0.728814             0_29   \n",
       "418445               0.627119             0.169492             0_37   \n",
       "...                       ...                  ...              ...   \n",
       "104903               0.949153             0.050847       213570_119   \n",
       "52429                0.779661             0.050847       213571_119   \n",
       "52430                0.779661             0.372881       213571_119   \n",
       "399288               0.237288             0.135593       213572_120   \n",
       "409757               0.050847             0.389831       213575_120   \n",
       "\n",
       "         cano_help  locdt_help  \n",
       "284164           0           1  \n",
       "1185304          0           4  \n",
       "726295           0          20  \n",
       "155960           0          29  \n",
       "418445           0          37  \n",
       "...            ...         ...  \n",
       "104903      213570         119  \n",
       "52429       213571         119  \n",
       "52430       213571         119  \n",
       "399288      213572         120  \n",
       "409757      213575         120  \n",
       "\n",
       "[1943452 rows x 26 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_columns = 100\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acqic</th>\n",
       "      <th>bacno</th>\n",
       "      <th>cano</th>\n",
       "      <th>conam</th>\n",
       "      <th>contp</th>\n",
       "      <th>csmcu</th>\n",
       "      <th>ecfg</th>\n",
       "      <th>etymd</th>\n",
       "      <th>flbmk</th>\n",
       "      <th>flg_3dsmk</th>\n",
       "      <th>hcefg</th>\n",
       "      <th>insfg</th>\n",
       "      <th>iterm</th>\n",
       "      <th>locdt</th>\n",
       "      <th>mcc</th>\n",
       "      <th>mchno</th>\n",
       "      <th>ovrlt</th>\n",
       "      <th>scity</th>\n",
       "      <th>stocn</th>\n",
       "      <th>stscd</th>\n",
       "      <th>loctm_hour_of_day</th>\n",
       "      <th>loctm_minute_of_hour</th>\n",
       "      <th>loctm_second_of_min</th>\n",
       "      <th>cano_locdt_index</th>\n",
       "      <th>cano_help</th>\n",
       "      <th>locdt_help</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1420933</td>\n",
       "      <td>0.794559</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00402</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.074284</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.478992</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.006388</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.11814</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.457627</td>\n",
       "      <td>4_58</td>\n",
       "      <td>4</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            acqic  bacno  cano    conam  contp  csmcu  ecfg     etymd  flbmk  \\\n",
       "1420933  0.794559    0.0   0.0  0.00402    1.0    1.0   1.0  0.074284    0.0   \n",
       "\n",
       "         flg_3dsmk  hcefg  insfg  iterm     locdt  mcc     mchno  ovrlt  \\\n",
       "1420933        1.0    1.0    1.0    0.0  0.478992  1.0  0.006388    1.0   \n",
       "\n",
       "           scity  stocn  stscd  loctm_hour_of_day  loctm_minute_of_hour  \\\n",
       "1420933  0.11814    1.0    1.0           0.782609              0.135593   \n",
       "\n",
       "         loctm_second_of_min cano_locdt_index  cano_help  locdt_help  \n",
       "1420933             0.457627             4_58          4          58  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.cano_locdt_index == \"4_58\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acqic</th>\n",
       "      <th>bacno</th>\n",
       "      <th>cano</th>\n",
       "      <th>conam</th>\n",
       "      <th>contp</th>\n",
       "      <th>csmcu</th>\n",
       "      <th>ecfg</th>\n",
       "      <th>etymd</th>\n",
       "      <th>flbmk</th>\n",
       "      <th>flg_3dsmk</th>\n",
       "      <th>hcefg</th>\n",
       "      <th>insfg</th>\n",
       "      <th>iterm</th>\n",
       "      <th>locdt</th>\n",
       "      <th>mcc</th>\n",
       "      <th>mchno</th>\n",
       "      <th>ovrlt</th>\n",
       "      <th>scity</th>\n",
       "      <th>stocn</th>\n",
       "      <th>stscd</th>\n",
       "      <th>loctm_hour_of_day</th>\n",
       "      <th>loctm_minute_of_hour</th>\n",
       "      <th>loctm_second_of_min</th>\n",
       "      <th>cano_locdt_index</th>\n",
       "      <th>cano_help</th>\n",
       "      <th>locdt_help</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1420933</td>\n",
       "      <td>0.794559</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00402</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.074284</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.478992</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.006388</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.11814</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>0.457627</td>\n",
       "      <td>4_58</td>\n",
       "      <td>4</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            acqic  bacno  cano    conam  contp  csmcu  ecfg     etymd  flbmk  \\\n",
       "1420933  0.794559    0.0   0.0  0.00402    1.0    1.0   1.0  0.074284    0.0   \n",
       "\n",
       "         flg_3dsmk  hcefg  insfg  iterm     locdt  mcc     mchno  ovrlt  \\\n",
       "1420933        1.0    1.0    1.0    0.0  0.478992  1.0  0.006388    1.0   \n",
       "\n",
       "           scity  stocn  stscd  loctm_hour_of_day  loctm_minute_of_hour  \\\n",
       "1420933  0.11814    1.0    1.0           0.782609              0.135593   \n",
       "\n",
       "         loctm_second_of_min cano_locdt_index  cano_help  locdt_help  \n",
       "1420933             0.457627             4_58          4          58  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_normal_cano_id[df_train_normal_cano_id.cano_locdt_index == \"4_58\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_(df, num_features):\n",
    "    data = []\n",
    "    for i in range(len(df)):\n",
    "        out = None\n",
    "        if i == 0:\n",
    "            out = np.concatenate(((np.zeros((2,num_features))),df.iloc[:1].values))\n",
    "        elif i== 1:\n",
    "            out = np.concatenate(((np.zeros((1,num_features))),df.iloc[:i+1].values))\n",
    "        else:\n",
    "            out = df.iloc[i+1-3:i+1].values\n",
    "        data.append(out)\n",
    "    return data\n",
    "\n",
    "def partition(df_, sequence_length = 3):\n",
    "    feats = [f for f in df_.columns if f not in {\"fraud_ind\",\"cano_help\",\"locdt_help\"}]\n",
    "    sequences = []\n",
    "    for _, df in df_.groupby(by = \"cano_help\"):\n",
    "        data = partition_(df[feats], num_features = len(feats))\n",
    "        for d in data:\n",
    "            sequences.append(d)\n",
    "    return sequences\n",
    "\n",
    "def get_sequence_dataframe(df):\n",
    "    df_train_sequences = partition(df)\n",
    "    df_train_sequences = np.concatenate(df_train_sequences)\n",
    "    df_train_sequences = pd.DataFrame(df_train_sequences)\n",
    "    return df_train_sequences\n",
    "\n",
    "# df_train_sequences = partition(df_train_normal_cano_id)\n",
    "# X_train = np.concatenate(df_train_sequences)\n",
    "# X_train = pd.DataFrame(X_train)\n",
    "X_train = get_sequence_dataframe(df_train_normal_cano_id)\n",
    "Feature = get_sequence_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4171146, 24)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5830356, 24)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.245115</td>\n",
       "      <td>0.00716846</td>\n",
       "      <td>0.00716846</td>\n",
       "      <td>0.0142658</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0771681</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.322034</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.245115</td>\n",
       "      <td>0.00716846</td>\n",
       "      <td>0.00716846</td>\n",
       "      <td>0.0142658</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0771681</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.322034</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5830351</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5830352</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.94654e-06</td>\n",
       "      <td>0.0533723</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.328259</td>\n",
       "      <td>0.0398945</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>0.135593</td>\n",
       "      <td>213572_120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5830353</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5830354</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5830355</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0533723</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.955971</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.328259</td>\n",
       "      <td>0.0398945</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.0508475</td>\n",
       "      <td>0.389831</td>\n",
       "      <td>213575_120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5830356 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0           1           2            3          4  5  6  \\\n",
       "0               0           0           0            0          0  0  0   \n",
       "1               0           0           0            0          0  0  0   \n",
       "2        0.245115  0.00716846  0.00716846    0.0142658          1  1  1   \n",
       "3               0           0           0            0          0  0  0   \n",
       "4        0.245115  0.00716846  0.00716846    0.0142658          1  1  1   \n",
       "...           ...         ...         ...          ...        ... .. ..   \n",
       "5830351         0           0           0            0          0  0  0   \n",
       "5830352         1           0           0  5.94654e-06  0.0533723  1  0   \n",
       "5830353         0           0           0            0          0  0  0   \n",
       "5830354         0           0           0            0          0  0  0   \n",
       "5830355         1           0           0            0  0.0533723  1  0   \n",
       "\n",
       "                7  8  9 10 11 12 13        14         15 16 17 18 19  \\\n",
       "0               0  0  0  0  0  0  0         0          0  0  0  0  0   \n",
       "1               0  0  0  0  0  0  0         0          0  0  0  0  0   \n",
       "2        0.783407  1  1  1  1  0  0         1  0.0771681  1  1  1  1   \n",
       "3               0  0  0  0  0  0  0         0          0  0  0  0  0   \n",
       "4        0.783407  1  1  1  1  0  0         1  0.0771681  1  1  1  1   \n",
       "...           ... .. .. .. .. .. ..       ...        ... .. .. .. ..   \n",
       "5830351         0  0  0  0  0  0  0         0          0  0  0  0  0   \n",
       "5830352  0.955971  1  1  1  1  0  1  0.328259  0.0398945  1  1  1  1   \n",
       "5830353         0  0  0  0  0  0  0         0          0  0  0  0  0   \n",
       "5830354         0  0  0  0  0  0  0         0          0  0  0  0  0   \n",
       "5830355  0.955971  1  1  1  1  0  1  0.328259  0.0398945  1  1  1  1   \n",
       "\n",
       "               20         21        22          23  \n",
       "0               0          0         0           0  \n",
       "1               0          0         0           0  \n",
       "2        0.652174   0.322034  0.813559         0_1  \n",
       "3               0          0         0           0  \n",
       "4        0.652174   0.322034  0.813559         0_1  \n",
       "...           ...        ...       ...         ...  \n",
       "5830351         0          0         0           0  \n",
       "5830352  0.608696   0.237288  0.135593  213572_120  \n",
       "5830353         0          0         0           0  \n",
       "5830354         0          0         0           0  \n",
       "5830355  0.478261  0.0508475  0.389831  213575_120  \n",
       "\n",
       "[5830356 rows x 24 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def partition(df_, sequence_length = 3):\n",
    "#     feats = [f for f in df_.columns if f not in {\"fraud_ind\"}]\n",
    "#     sequences = []\n",
    "#     i = 0\n",
    "#     for _, df in df_[feats].groupby(by = \"cano_locdt_index\"):\n",
    "#         data = partition_(df[feats], num_features = len(feats))\n",
    "#         for d in data:\n",
    "#             sequences.append(d)\n",
    "#         i += 1\n",
    "#         if i > 10:\n",
    "#             break\n",
    "#     return sequences\n",
    "# X_train = get_sequence_dataframe(df_train_normal_cano_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.245115</td>\n",
       "      <td>0.00716846</td>\n",
       "      <td>0.00716846</td>\n",
       "      <td>0.0142658</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0771681</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.322034</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.245115</td>\n",
       "      <td>0.00716846</td>\n",
       "      <td>0.00716846</td>\n",
       "      <td>0.0142658</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.783407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0771681</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.322034</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>0_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4171141</td>\n",
       "      <td>0.0320937</td>\n",
       "      <td>0.00268817</td>\n",
       "      <td>0.00268817</td>\n",
       "      <td>0.00290191</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0102579</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00517195</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00903582</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.747899</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000141247</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000190161</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>0.661017</td>\n",
       "      <td>213334_90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4171142</td>\n",
       "      <td>0.365315</td>\n",
       "      <td>0.00268817</td>\n",
       "      <td>0.00268817</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0102579</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00517195</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00903582</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.747899</td>\n",
       "      <td>0.568704</td>\n",
       "      <td>0.000674849</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>213334_90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4171143</td>\n",
       "      <td>0.0320937</td>\n",
       "      <td>0.00268817</td>\n",
       "      <td>0.00268817</td>\n",
       "      <td>0.00290191</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0102579</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00517195</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00903582</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.747899</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000141247</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000190161</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>0.661017</td>\n",
       "      <td>213334_90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4171144</td>\n",
       "      <td>0.365315</td>\n",
       "      <td>0.00268817</td>\n",
       "      <td>0.00268817</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0102579</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00517195</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00903582</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.747899</td>\n",
       "      <td>0.568704</td>\n",
       "      <td>0.000674849</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>0.711864</td>\n",
       "      <td>213334_90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4171145</td>\n",
       "      <td>0.0418379</td>\n",
       "      <td>0.00268817</td>\n",
       "      <td>0.00268817</td>\n",
       "      <td>0.00118336</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0102579</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00517195</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00903582</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.747899</td>\n",
       "      <td>0.0573014</td>\n",
       "      <td>0.000470825</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0044661</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.627119</td>\n",
       "      <td>0.372881</td>\n",
       "      <td>213334_90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4171146 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0           1           2           3  4          5  6  \\\n",
       "0                0           0           0           0  0          0  0   \n",
       "1                0           0           0           0  0          0  0   \n",
       "2         0.245115  0.00716846  0.00716846   0.0142658  1          1  1   \n",
       "3                0           0           0           0  0          0  0   \n",
       "4         0.245115  0.00716846  0.00716846   0.0142658  1          1  1   \n",
       "...            ...         ...         ...         ... ..        ... ..   \n",
       "4171141  0.0320937  0.00268817  0.00268817  0.00290191  1  0.0102579  1   \n",
       "4171142   0.365315  0.00268817  0.00268817           1  1  0.0102579  1   \n",
       "4171143  0.0320937  0.00268817  0.00268817  0.00290191  1  0.0102579  1   \n",
       "4171144   0.365315  0.00268817  0.00268817           1  1  0.0102579  1   \n",
       "4171145  0.0418379  0.00268817  0.00268817  0.00118336  1  0.0102579  1   \n",
       "\n",
       "                7           8  9          10 11 12        13         14  \\\n",
       "0               0           0  0           0  0  0         0          0   \n",
       "1               0           0  0           0  0  0         0          0   \n",
       "2        0.783407           1  1           1  1  0         0          1   \n",
       "3               0           0  0           0  0  0         0          0   \n",
       "4        0.783407           1  1           1  1  0         0          1   \n",
       "...           ...         ... ..         ... .. ..       ...        ...   \n",
       "4171141         1  0.00517195  0  0.00903582  1  0  0.747899          1   \n",
       "4171142         1  0.00517195  0  0.00903582  1  0  0.747899   0.568704   \n",
       "4171143         1  0.00517195  0  0.00903582  1  0  0.747899          1   \n",
       "4171144         1  0.00517195  0  0.00903582  1  0  0.747899   0.568704   \n",
       "4171145         1  0.00517195  0  0.00903582  1  0  0.747899  0.0573014   \n",
       "\n",
       "                  15 16           17 18 19        20        21        22  \\\n",
       "0                  0  0            0  0  0         0         0         0   \n",
       "1                  0  0            0  0  0         0         0         0   \n",
       "2          0.0771681  1            1  1  1  0.652174  0.322034  0.813559   \n",
       "3                  0  0            0  0  0         0         0         0   \n",
       "4          0.0771681  1            1  1  1  0.652174  0.322034  0.813559   \n",
       "...              ... ..          ... .. ..       ...       ...       ...   \n",
       "4171141  0.000141247  1  0.000190161  1  1  0.826087  0.779661  0.661017   \n",
       "4171142  0.000674849  1            1  1  1  0.652174  0.779661  0.711864   \n",
       "4171143  0.000141247  1  0.000190161  1  1  0.826087  0.779661  0.661017   \n",
       "4171144  0.000674849  1            1  1  1  0.652174  0.779661  0.711864   \n",
       "4171145  0.000470825  1    0.0044661  1  1  0.565217  0.627119  0.372881   \n",
       "\n",
       "                23  \n",
       "0                0  \n",
       "1                0  \n",
       "2              0_1  \n",
       "3                0  \n",
       "4              0_1  \n",
       "...            ...  \n",
       "4171141  213334_90  \n",
       "4171142  213334_90  \n",
       "4171143  213334_90  \n",
       "4171144  213334_90  \n",
       "4171145  213334_90  \n",
       "\n",
       "[4171146 rows x 24 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"X_train.csv\", index = False)\n",
    "Feature.to_csv(\"Feature.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "from tensorflow.python.client import device_lib\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class GPUWrapper:\n",
    "    def __init__(self, gpu):\n",
    "        self.gpu = gpu\n",
    "\n",
    "    @property\n",
    "    def tf_device(self):\n",
    "        local_device_protos = device_lib.list_local_devices()\n",
    "        gpus = [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "        return tf.device(gpus[self.gpu] if gpus else '/cpu:0')\n",
    "\n",
    "    @property\n",
    "    def torch_device(self):\n",
    "        return torch.device(f'cuda:{self.gpu}' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def to_var(self, x, **kwargs):\n",
    "        \"\"\"PyTorch only: send Var to proper device.\"\"\"\n",
    "        x = x.to(self.torch_device)\n",
    "        return Variable(x, **kwargs)\n",
    "\n",
    "    def to_device(self, model):\n",
    "        \"\"\"PyTorch only: send Model to proper device.\"\"\"\n",
    "        model.to(self.torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import trange\n",
    "import sys\n",
    "sys.path.append(\"../DeepADoTS/src/algorithms/\")\n",
    "# from .algorithm_utils import Algorithm, PyTorchUtils\n",
    "# from .autoencoder import AutoEncoderModule\n",
    "#from lstm_enc_dec_axl import LSTMEDModule\n",
    "import abc\n",
    "import logging\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class Algorithm(metaclass=abc.ABCMeta):\n",
    "    def __init__(self, module_name, name, seed, details=False):\n",
    "        self.logger = logging.getLogger(module_name)\n",
    "        self.name = name\n",
    "        self.seed = seed\n",
    "        self.details = details\n",
    "        self.prediction_details = {}\n",
    "\n",
    "        if self.seed is not None:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Train the algorithm on the given dataset\n",
    "        \"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        :return anomaly score\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "class PyTorchUtils(metaclass=abc.ABCMeta):\n",
    "    def __init__(self, seed, gpu):\n",
    "        self.gpu = gpu\n",
    "        self.seed = seed\n",
    "        if self.seed is not None:\n",
    "            torch.manual_seed(self.seed)\n",
    "            torch.cuda.manual_seed(self.seed)\n",
    "        self.framework = 0\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        #return 'cuda:0'\n",
    "        return torch.device(f'cuda:{self.gpu}' if torch.cuda.is_available() and self.gpu is not None else 'cpu')\n",
    "\n",
    "    def to_var(self, t, **kwargs):\n",
    "        # ToDo: check whether cuda Variable.\n",
    "        t = t.to(self.device)\n",
    "        return Variable(t, **kwargs)\n",
    "\n",
    "    def to_device(self, model):\n",
    "        model.to(self.device)\n",
    "\n",
    "\n",
    "class TensorflowUtils(metaclass=abc.ABCMeta):\n",
    "    def __init__(self, seed, gpu):\n",
    "        self.gpu = gpu\n",
    "        self.seed = seed\n",
    "        if self.seed is not None:\n",
    "            tf.set_random_seed(seed)\n",
    "        self.framework = 1\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        local_device_protos = device_lib.list_local_devices()\n",
    "        gpus = [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "        return tf.device(gpus[self.gpu] if gpus and self.gpu is not None else '/cpu:0')\n",
    "    \n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.stats import multivariate_normal\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "class AutoEncoder(Algorithm, PyTorchUtils):\n",
    "    def __init__(self, name: str='AutoEncoder', num_epochs: int=10, batch_size: int=20, lr: float=1e-3,\n",
    "                 hidden_size: int=5, sequence_length: int=30, train_gaussian_percentage: float=0.25,\n",
    "                 seed: int=None, gpu: int=None, details=True):\n",
    "        Algorithm.__init__(self, __name__, name, seed, details=details)\n",
    "        PyTorchUtils.__init__(self, seed, gpu)\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.train_gaussian_percentage = train_gaussian_percentage\n",
    "\n",
    "        self.aed = None\n",
    "        self.mean, self.cov = None, None\n",
    "\n",
    "    def fit(self, X: pd.DataFrame):\n",
    "        X.interpolate(inplace=True)\n",
    "        X.bfill(inplace=True)\n",
    "        data = X.values\n",
    "        sequences = [data[i:i + self.sequence_length] for i in range(data.shape[0] - self.sequence_length + 1)]\n",
    "        indices = np.random.permutation(len(sequences))\n",
    "        split_point = int(self.train_gaussian_percentage * len(sequences))\n",
    "        train_loader = DataLoader(dataset=sequences, batch_size=self.batch_size, drop_last=True,\n",
    "                                  sampler=SubsetRandomSampler(indices[:-split_point]), pin_memory=True)\n",
    "        train_gaussian_loader = DataLoader(dataset=sequences, batch_size=self.batch_size, drop_last=True,\n",
    "                                           sampler=SubsetRandomSampler(indices[-split_point:]), pin_memory=True)\n",
    "\n",
    "        self.aed = AutoEncoderModule(X.shape[1], self.sequence_length, self.hidden_size, seed=self.seed, gpu=self.gpu)\n",
    "        self.to_device(self.aed)  # .double()\n",
    "        optimizer = torch.optim.Adam(self.aed.parameters(), lr=self.lr)\n",
    "\n",
    "        self.aed.train()\n",
    "        for epoch in trange(self.num_epochs):\n",
    "            logging.debug(f'Epoch {epoch+1}/{self.num_epochs}.')\n",
    "            for ts_batch in train_loader:\n",
    "                output = self.aed(self.to_var(ts_batch))\n",
    "                loss = nn.MSELoss(size_average=False)(output, self.to_var(ts_batch.float()))\n",
    "                self.aed.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        self.aed.eval()\n",
    "        error_vectors = []\n",
    "        for ts_batch in train_gaussian_loader:\n",
    "            output = self.aed(self.to_var(ts_batch))\n",
    "            error = nn.L1Loss(reduce=False)(output, self.to_var(ts_batch.float()))\n",
    "            error_vectors += list(error.view(-1, X.shape[1]).data.cpu().numpy())\n",
    "\n",
    "        self.mean = np.mean(error_vectors, axis=0)\n",
    "        self.cov = np.cov(error_vectors, rowvar=False)\n",
    "\n",
    "    def predict(self, X: pd.DataFrame) -> np.array:\n",
    "        X.interpolate(inplace=True)\n",
    "        X.bfill(inplace=True)\n",
    "        data = X.values\n",
    "        sequences = [data[i:i + self.sequence_length] for i in range(data.shape[0] - self.sequence_length + 1)]\n",
    "        data_loader = DataLoader(dataset=sequences, batch_size=self.batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "        self.aed.eval()\n",
    "        mvnormal = multivariate_normal(self.mean, self.cov, allow_singular=True)\n",
    "        scores = []\n",
    "        outputs = []\n",
    "        errors = []\n",
    "        for idx, ts in enumerate(data_loader):\n",
    "            output = self.aed(self.to_var(ts))\n",
    "            error = nn.L1Loss(reduce=False)(output, self.to_var(ts.float()))\n",
    "            score = -mvnormal.logpdf(error.view(-1, X.shape[1]).data.cpu().numpy())\n",
    "            scores.append(score.reshape(ts.size(0), self.sequence_length))\n",
    "            if self.details:\n",
    "                outputs.append(output.data.numpy())\n",
    "                errors.append(error.data.numpy())\n",
    "\n",
    "        # stores seq_len-many scores per timestamp and averages them\n",
    "        scores = np.concatenate(scores)\n",
    "        lattice = np.full((self.sequence_length, X.shape[0]), np.nan)\n",
    "        for i, score in enumerate(scores):\n",
    "            lattice[i % self.sequence_length, i:i + self.sequence_length] = score\n",
    "        scores = np.nanmean(lattice, axis=0)\n",
    "\n",
    "        if self.details:\n",
    "            outputs = np.concatenate(outputs)\n",
    "            lattice = np.full((self.sequence_length, X.shape[0], X.shape[1]), np.nan)\n",
    "            for i, output in enumerate(outputs):\n",
    "                lattice[i % self.sequence_length, i:i + self.sequence_length, :] = output\n",
    "            self.prediction_details.update({'reconstructions_mean': np.nanmean(lattice, axis=0).T})\n",
    "\n",
    "            errors = np.concatenate(errors)\n",
    "            lattice = np.full((self.sequence_length, X.shape[0], X.shape[1]), np.nan)\n",
    "            for i, error in enumerate(errors):\n",
    "                lattice[i % self.sequence_length, i:i + self.sequence_length, :] = error\n",
    "            self.prediction_details.update({'errors_mean': np.nanmean(lattice, axis=0).T})\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "class AutoEncoderModule(nn.Module, PyTorchUtils):\n",
    "    def __init__(self, n_features: int, sequence_length: int, hidden_size: int, seed: int, gpu: int):\n",
    "        # Each point is a flattened window and thus has as many features as sequence_length * features\n",
    "        super().__init__()\n",
    "        PyTorchUtils.__init__(self, seed, gpu)\n",
    "        input_length = n_features * sequence_length\n",
    "\n",
    "        # creates powers of two between eight and the next smaller power from the input_length\n",
    "        dec_steps = 2 ** np.arange(max(np.ceil(np.log2(hidden_size)), 2), np.log2(input_length))[1:]\n",
    "        dec_setup = np.concatenate([[hidden_size], dec_steps.repeat(2), [input_length]])\n",
    "        enc_setup = dec_setup[::-1]\n",
    "\n",
    "        layers = np.array([[nn.Linear(int(a), int(b)), nn.Tanh()] for a, b in enc_setup.reshape(-1, 2)]).flatten()[:-1]\n",
    "        self._encoder = nn.Sequential(*layers)\n",
    "        self.to_device(self._encoder)\n",
    "\n",
    "        layers = np.array([[nn.Linear(int(a), int(b)), nn.Tanh()] for a, b in dec_setup.reshape(-1, 2)]).flatten()[:-1]\n",
    "        self._decoder = nn.Sequential(*layers)\n",
    "        self.to_device(self._decoder)\n",
    "\n",
    "    def forward(self, ts_batch, return_latent: bool=False):\n",
    "        flattened_sequence = ts_batch.view(ts_batch.size(0), -1)\n",
    "        enc = self._encoder(flattened_sequence.float())\n",
    "        dec = self._decoder(enc)\n",
    "        reconstructed_sequence = dec.view(ts_batch.size())\n",
    "        return (reconstructed_sequence, enc) if return_latent else reconstructed_sequence\n",
    "    \n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.stats import multivariate_normal\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from tqdm import trange\n",
    "\n",
    "# from .algorithm_utils import Algorithm, PyTorchUtils\n",
    "\n",
    "\n",
    "class LSTMED(Algorithm, PyTorchUtils):\n",
    "    def __init__(self, name: str='LSTM-ED', num_epochs: int=10, batch_size: int=20, lr: float=1e-3,\n",
    "                 hidden_size: int=5, sequence_length: int=30, train_gaussian_percentage: float=0.25,\n",
    "                 n_layers: tuple=(1, 1), use_bias: tuple=(True, True), dropout: tuple=(0, 0),\n",
    "                 seed: int=None, gpu: int = None, details=True):\n",
    "        Algorithm.__init__(self, __name__, name, seed, details=details)\n",
    "        PyTorchUtils.__init__(self, seed, gpu)\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.train_gaussian_percentage = train_gaussian_percentage\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.use_bias = use_bias\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.lstmed = None\n",
    "        self.mean, self.cov = None, None\n",
    "\n",
    "    def fit(self, X: pd.DataFrame):\n",
    "        X.interpolate(inplace=True)\n",
    "        X.bfill(inplace=True)\n",
    "        data = X.values\n",
    "        sequences = [data[i:i + self.sequence_length] for i in range(data.shape[0] - self.sequence_length + 1)]\n",
    "        indices = np.random.permutation(len(sequences))\n",
    "        split_point = int(self.train_gaussian_percentage * len(sequences))\n",
    "        train_loader = DataLoader(dataset=sequences, batch_size=self.batch_size, drop_last=True,\n",
    "                                  sampler=SubsetRandomSampler(indices[:-split_point]), pin_memory=True)\n",
    "        train_gaussian_loader = DataLoader(dataset=sequences, batch_size=self.batch_size, drop_last=True,\n",
    "                                           sampler=SubsetRandomSampler(indices[-split_point:]), pin_memory=True)\n",
    "\n",
    "        self.lstmed = LSTMEDModule(X.shape[1], self.hidden_size,\n",
    "                                   self.n_layers, self.use_bias, self.dropout,\n",
    "                                   seed=self.seed, gpu=self.gpu)\n",
    "        self.to_device(self.lstmed)\n",
    "        optimizer = torch.optim.Adam(self.lstmed.parameters(), lr=self.lr)\n",
    "\n",
    "        self.lstmed.train()\n",
    "        for epoch in trange(self.num_epochs):\n",
    "            logging.debug(f'Epoch {epoch+1}/{self.num_epochs}.')\n",
    "            for ts_batch in train_loader:\n",
    "                output = self.lstmed(self.to_var(ts_batch))\n",
    "                loss = nn.MSELoss(size_average=False)(output, self.to_var(ts_batch.float()))\n",
    "                self.lstmed.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        self.lstmed.eval()\n",
    "        error_vectors = []\n",
    "        for ts_batch in train_gaussian_loader:\n",
    "            output = self.lstmed(self.to_var(ts_batch))\n",
    "            error = nn.L1Loss(reduce=False)(output, self.to_var(ts_batch.float()))\n",
    "            error_vectors += list(error.view(-1, X.shape[1]).data.cpu().numpy())\n",
    "\n",
    "        self.mean = np.mean(error_vectors, axis=0)\n",
    "        self.cov = np.cov(error_vectors, rowvar=False)\n",
    "\n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        X.interpolate(inplace=True)\n",
    "        X.bfill(inplace=True)\n",
    "        data = X.values\n",
    "        sequences = [data[i:i + self.sequence_length] for i in range(data.shape[0] - self.sequence_length + 1)]\n",
    "        data_loader = DataLoader(dataset=sequences, batch_size=self.batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "        self.lstmed.eval()\n",
    "        mvnormal = multivariate_normal(self.mean, self.cov, allow_singular=True)\n",
    "        scores = []\n",
    "        outputs = []\n",
    "        errors = []\n",
    "        for idx, ts in enumerate(data_loader):\n",
    "            output = self.lstmed(self.to_var(ts))\n",
    "            error = nn.L1Loss(reduce=False)(output, self.to_var(ts.float()))\n",
    "            score = -mvnormal.logpdf(error.view(-1, X.shape[1]).data.cpu().numpy())\n",
    "            scores.append(score.reshape(ts.size(0), self.sequence_length))\n",
    "            if self.details:\n",
    "                outputs.append(output.data.numpy())\n",
    "                errors.append(error.data.numpy())\n",
    "\n",
    "        # stores seq_len-many scores per timestamp and averages them\n",
    "        scores = np.concatenate(scores)\n",
    "        lattice = np.full((self.sequence_length, data.shape[0]), np.nan)\n",
    "        for i, score in enumerate(scores):\n",
    "            lattice[i % self.sequence_length, i:i + self.sequence_length] = score\n",
    "        scores = np.nanmean(lattice, axis=0)\n",
    "\n",
    "        if self.details:\n",
    "            outputs = np.concatenate(outputs)\n",
    "            lattice = np.full((self.sequence_length, X.shape[0], X.shape[1]), np.nan)\n",
    "            for i, output in enumerate(outputs):\n",
    "                lattice[i % self.sequence_length, i:i + self.sequence_length, :] = output\n",
    "            self.prediction_details.update({'reconstructions_mean': np.nanmean(lattice, axis=0).T})\n",
    "\n",
    "            errors = np.concatenate(errors)\n",
    "            lattice = np.full((self.sequence_length, X.shape[0], X.shape[1]), np.nan)\n",
    "            for i, error in enumerate(errors):\n",
    "                lattice[i % self.sequence_length, i:i + self.sequence_length, :] = error\n",
    "            self.prediction_details.update({'errors_mean': np.nanmean(lattice, axis=0).T})\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "class LSTMEDModule(nn.Module, PyTorchUtils):\n",
    "    def __init__(self, n_features: int, hidden_size: int,\n",
    "                 n_layers: tuple, use_bias: tuple, dropout: tuple,\n",
    "                 seed: int, gpu: int):\n",
    "        super().__init__()\n",
    "        PyTorchUtils.__init__(self, seed, gpu)\n",
    "        self.n_features = n_features\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.use_bias = use_bias\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.encoder = nn.LSTM(self.n_features, self.hidden_size, batch_first=True,\n",
    "                               num_layers=self.n_layers[0], bias=self.use_bias[0], dropout=self.dropout[0])\n",
    "        self.to_device(self.encoder)\n",
    "        self.decoder = nn.LSTM(self.n_features, self.hidden_size, batch_first=True,\n",
    "                               num_layers=self.n_layers[1], bias=self.use_bias[1], dropout=self.dropout[1])\n",
    "        self.to_device(self.decoder)\n",
    "        self.hidden2output = nn.Linear(self.hidden_size, self.n_features)\n",
    "        self.to_device(self.hidden2output)\n",
    "\n",
    "    def _init_hidden(self, batch_size):\n",
    "        return (self.to_var(torch.Tensor(self.n_layers[0], batch_size, self.hidden_size).zero_()),\n",
    "                self.to_var(torch.Tensor(self.n_layers[0], batch_size, self.hidden_size).zero_()))\n",
    "\n",
    "    def forward(self, ts_batch, return_latent: bool=False):\n",
    "        batch_size = ts_batch.shape[0]\n",
    "\n",
    "        # 1. Encode the timeseries to make use of the last hidden state.\n",
    "        enc_hidden = self._init_hidden(batch_size)  # initialization with zero\n",
    "        _, enc_hidden = self.encoder(ts_batch.float(), enc_hidden)  # .float() here or .double() for the model\n",
    "\n",
    "        # 2. Use hidden state as initialization for our Decoder-LSTM\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        # 3. Also, use this hidden state to get the first output aka the last point of the reconstructed timeseries\n",
    "        # 4. Reconstruct timeseries backwards\n",
    "        #    * Use true data for training decoder\n",
    "        #    * Use hidden2output for prediction\n",
    "        output = self.to_var(torch.Tensor(ts_batch.size()).zero_())\n",
    "        for i in reversed(range(ts_batch.shape[1])):\n",
    "            output[:, i, :] = self.hidden2output(dec_hidden[0][0, :])\n",
    "\n",
    "            if self.training:\n",
    "                _, dec_hidden = self.decoder(ts_batch[:, i].unsqueeze(1).float(), dec_hidden)\n",
    "            else:\n",
    "                _, dec_hidden = self.decoder(output[:, i].unsqueeze(1), dec_hidden)\n",
    "\n",
    "        return (output, enc_hidden[1][-1]) if return_latent else output\n",
    "\n",
    "\"\"\"Adapted from Daniel Stanley Tan (https://github.com/danieltan07/dagmm)\"\"\"\n",
    "import logging\n",
    "import sys\n",
    "sys.path.append(\"../DeepADoTS/src/algorithms/\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import trange\n",
    "\n",
    "# from .algorithm_utils import Algorithm, PyTorchUtils\n",
    "# from .autoencoder import AutoEncoderModule\n",
    "#from lstm_enc_dec_axl import LSTMEDModule\n",
    "\n",
    "\n",
    "class DAGMM(Algorithm, PyTorchUtils):\n",
    "    class AutoEncoder:\n",
    "        NN = AutoEncoderModule\n",
    "        LSTM = LSTMEDModule\n",
    "\n",
    "    def __init__(self, num_epochs=10, lambda_energy=0.1, lambda_cov_diag=0.005, lr=1e-3, batch_size=50, gmm_k=3,\n",
    "                 normal_percentile=80, sequence_length=30, autoencoder_type=AutoEncoderModule, autoencoder_args=None,\n",
    "                 hidden_size: int=5, seed: int=None, gpu: int=None, details=True):\n",
    "        _name = 'LSTM-DAGMM' if autoencoder_type == LSTMEDModule else 'DAGMM'\n",
    "        Algorithm.__init__(self, __name__, _name, seed, details=details)\n",
    "        PyTorchUtils.__init__(self, seed, gpu)\n",
    "        self.num_epochs = num_epochs\n",
    "        self.lambda_energy = lambda_energy\n",
    "        self.lambda_cov_diag = lambda_cov_diag\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.gmm_k = gmm_k  # Number of Gaussian mixtures\n",
    "        self.normal_percentile = normal_percentile  # Up to which percentile data should be considered normal\n",
    "        self.autoencoder_type = autoencoder_type\n",
    "        if autoencoder_type == AutoEncoderModule:\n",
    "            self.autoencoder_args = ({'sequence_length': self.sequence_length})\n",
    "        elif autoencoder_type == LSTMEDModule:\n",
    "            self.autoencoder_args = ({'n_layers': (1, 1), 'use_bias': (True, True), 'dropout': (0.0, 0.0)})\n",
    "        self.autoencoder_args.update({'seed': seed, 'gpu': gpu})\n",
    "        if autoencoder_args is not None:\n",
    "            self.autoencoder_args.update(autoencoder_args)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.dagmm, self.optimizer, self.train_energy, self._threshold = None, None, None, None\n",
    "\n",
    "    def reset_grad(self):\n",
    "        self.dagmm.zero_grad()\n",
    "\n",
    "    def dagmm_step(self, input_data):\n",
    "        print (\"input_data\",input_data)\n",
    "        self.dagmm.train()\n",
    "        enc, dec, z, gamma = self.dagmm(input_data)\n",
    "        #print (enc, dec, z, gamma)\n",
    "        total_loss, sample_energy, recon_error, cov_diag = self.dagmm.loss_function(input_data, dec, z, gamma,\n",
    "                                                                                    self.lambda_energy,\n",
    "                                                                                    self.lambda_cov_diag)\n",
    "        print (\"total_loss\",total_loss)\n",
    "        self.reset_grad()\n",
    "        total_loss = torch.clamp(total_loss, max=1e7)  # Extremely high loss can cause NaN gradients\n",
    "        #total_loss = total_loss.float() \n",
    "        #total_loss = total_loss.cuda()\n",
    "        #total_loss = total_loss.cpu()\n",
    "        \n",
    "        #total_loss=total_loss.type(torch.FloatTensor)\n",
    "        #total_loss = total_loss.to(\"cuda:0\")\n",
    "        print (\"total_loss\", total_loss)\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.dagmm.parameters(), 5)\n",
    "        # if np.array([np.isnan(p.grad.detach().numpy()).any() for p in self.dagmm.parameters()]).any():\n",
    "        #     import IPython; IPython.embed()\n",
    "        self.optimizer.step()\n",
    "        return total_loss, sample_energy, recon_error, cov_diag\n",
    "\n",
    "    def fit(self, X: pd.DataFrame):\n",
    "        \"\"\"Learn the mixture probability, mean and covariance for each component k.\n",
    "        Store the computed energy based on the training data and the aforementioned parameters.\"\"\"\n",
    "        #X.interpolate(inplace=True)\n",
    "        X.bfill(inplace=True)\n",
    "        data = X.values\n",
    "        sequences = [data[i:i + self.sequence_length] for i in range(X.shape[0] - self.sequence_length + 1)]\n",
    "        data_loader = DataLoader(dataset=sequences, batch_size=self.batch_size, shuffle=True, drop_last=True)\n",
    "        self.hidden_size = 5 + int(X.shape[1] / 20)\n",
    "        autoencoder = self.autoencoder_type(X.shape[1], hidden_size=self.hidden_size, **self.autoencoder_args)\n",
    "        self.dagmm = DAGMMModule(autoencoder, n_gmm=self.gmm_k, latent_dim=self.hidden_size + 2,\n",
    "                                 seed=self.seed, gpu=self.gpu)\n",
    "        self.to_device(self.dagmm)\n",
    "        self.optimizer = torch.optim.Adam(self.dagmm.parameters(), lr=self.lr)\n",
    "\n",
    "        for _ in trange(self.num_epochs):\n",
    "            for input_data in data_loader:\n",
    "                input_data = self.to_var(input_data)\n",
    "                self.dagmm_step(input_data.float())\n",
    "\n",
    "        self.dagmm.eval()\n",
    "        n = 0\n",
    "        mu_sum = 0\n",
    "        cov_sum = 0\n",
    "        gamma_sum = 0\n",
    "        for input_data in data_loader:\n",
    "            input_data = self.to_var(input_data)\n",
    "            _, _, z, gamma = self.dagmm(input_data.float())\n",
    "            phi, mu, cov = self.dagmm.compute_gmm_params(z, gamma)\n",
    "\n",
    "            batch_gamma_sum = torch.sum(gamma, dim=0)\n",
    "\n",
    "            gamma_sum += batch_gamma_sum\n",
    "            mu_sum += mu * batch_gamma_sum.unsqueeze(-1)  # keep sums of the numerator only\n",
    "            cov_sum += cov * batch_gamma_sum.unsqueeze(-1).unsqueeze(-1)  # keep sums of the numerator only\n",
    "\n",
    "            n += input_data.size(0)\n",
    "\n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        \"\"\"Using the learned mixture probability, mean and covariance for each component k, compute the energy on the\n",
    "        given data.\"\"\"\n",
    "        self.dagmm.eval()\n",
    "        #X.interpolate(inplace=True)\n",
    "        X.bfill(inplace=True)\n",
    "        data = X.values\n",
    "        sequences = [data[i:i + self.sequence_length] for i in range(len(data) - self.sequence_length + 1)]\n",
    "        data_loader = DataLoader(dataset=sequences, batch_size=1, shuffle=False)\n",
    "        test_energy = np.full((self.sequence_length, X.shape[0]), np.nan)\n",
    "\n",
    "        encodings = np.full((self.sequence_length, X.shape[0], self.hidden_size), np.nan)\n",
    "        decodings = np.full((self.sequence_length, X.shape[0], X.shape[1]), np.nan)\n",
    "        euc_errors = np.full((self.sequence_length, X.shape[0]), np.nan)\n",
    "        csn_errors = np.full((self.sequence_length, X.shape[0]), np.nan)\n",
    "\n",
    "        for i, sequence in enumerate(data_loader):\n",
    "            #print (\"shape of sequence\",self.to_var(sequence).float().shape)\n",
    "            enc, dec, z, gamma = self.dagmm(self.to_var(sequence).float())\n",
    "            sample_energy, _ = self.dagmm.compute_energy(z, size_average=False)\n",
    "            idx = (i % self.sequence_length, np.arange(i, i + self.sequence_length))\n",
    "            test_energy[idx] = sample_energy.data.numpy()\n",
    "\n",
    "            if self.details:\n",
    "                encodings[idx] = enc.data.cpu().numpy()\n",
    "                decodings[idx] = dec.data.cpu().numpy()\n",
    "                euc_errors[idx] = z[:, 1].data.cpu().numpy()\n",
    "                csn_errors[idx] = z[:, 2].data.cpu().numpy()\n",
    "\n",
    "        test_energy = np.nanmean(test_energy, axis=0)\n",
    "\n",
    "        if self.details:\n",
    "            self.prediction_details.update({'latent_representations': np.nanmean(encodings, axis=0).T})\n",
    "            self.prediction_details.update({'reconstructions_mean': np.nanmean(decodings, axis=0).T})\n",
    "            self.prediction_details.update({'euclidean_errors_mean': np.nanmean(euc_errors, axis=0)})\n",
    "            self.prediction_details.update({'cosine_errors_mean': np.nanmean(csn_errors, axis=0)})\n",
    "\n",
    "        return test_energy\n",
    "\n",
    "\n",
    "# class DAGMMModule(nn.Module, PyTorchUtils):\n",
    "#     \"\"\"Residual Block.\"\"\"\n",
    "\n",
    "#     def __init__(self, autoencoder, n_gmm, latent_dim, seed: int, gpu: int):\n",
    "#         super(DAGMMModule, self).__init__()\n",
    "#         PyTorchUtils.__init__(self, seed, gpu)\n",
    "\n",
    "#         self.add_module('autoencoder', autoencoder)\n",
    "\n",
    "#         layers = [\n",
    "#             nn.Linear(latent_dim, 10),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Dropout(p=0.5),\n",
    "#             nn.Linear(10, n_gmm),\n",
    "#             nn.Softmax(dim=1)\n",
    "#         ]\n",
    "#         self.estimation = nn.Sequential(*layers)\n",
    "#         self.to_device(self.estimation)\n",
    "\n",
    "#         self.register_buffer('phi', self.to_var(torch.zeros(n_gmm)))\n",
    "#         self.register_buffer('mu', self.to_var(torch.zeros(n_gmm, latent_dim)))\n",
    "#         self.register_buffer('cov', self.to_var(torch.zeros(n_gmm, latent_dim, latent_dim)))\n",
    "\n",
    "#     def relative_euclidean_distance(self, a, b, dim=1):\n",
    "#         return (a - b).norm(2, dim=dim) / torch.clamp(a.norm(2, dim=dim), min=1e-10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         dec, enc = self.autoencoder(x, return_latent=True)\n",
    "\n",
    "#         rec_cosine = F.cosine_similarity(x.view(x.shape[0], -1), dec.view(dec.shape[0], -1), dim=1)\n",
    "#         rec_euclidean = self.relative_euclidean_distance(x.view(x.shape[0], -1), dec.view(dec.shape[0], -1), dim=1)\n",
    "\n",
    "#         # Concatenate latent representation, cosine similarity and relative Euclidean distance between x and dec(enc(x))\n",
    "#         z = torch.cat([enc, rec_euclidean.unsqueeze(-1), rec_cosine.unsqueeze(-1)], dim=1)\n",
    "#         gamma = self.estimation(z)\n",
    "\n",
    "#         return enc, dec, z, gamma\n",
    "\n",
    "#     def compute_gmm_params(self, z, gamma):\n",
    "#         N = gamma.size(0)\n",
    "#         # K\n",
    "#         sum_gamma = torch.sum(gamma, dim=0)\n",
    "\n",
    "#         # K\n",
    "#         phi = (sum_gamma / N)\n",
    "\n",
    "#         self.phi = phi.data\n",
    "\n",
    "#         # K x D\n",
    "#         mu = torch.sum(gamma.unsqueeze(-1) * z.unsqueeze(1), dim=0) / sum_gamma.unsqueeze(-1)\n",
    "#         self.mu = mu.data\n",
    "#         # z = N x D\n",
    "#         # mu = K x D\n",
    "#         # gamma N x K\n",
    "\n",
    "#         # z_mu = N x K x D\n",
    "#         z_mu = (z.unsqueeze(1) - mu.unsqueeze(0))\n",
    "\n",
    "#         # z_mu_outer = N x K x D x D\n",
    "#         z_mu_outer = z_mu.unsqueeze(-1) * z_mu.unsqueeze(-2)\n",
    "\n",
    "#         # K x D x D\n",
    "#         cov = torch.sum(gamma.unsqueeze(-1).unsqueeze(-1) * z_mu_outer, dim=0) / sum_gamma.unsqueeze(-1).unsqueeze(-1)\n",
    "#         self.cov = cov.data\n",
    "\n",
    "#         return phi, mu, cov\n",
    "\n",
    "#     def compute_energy(self, z, phi=None, mu=None, cov=None, size_average=True):\n",
    "#         if phi is None:\n",
    "#             phi = Variable(self.phi)\n",
    "#         if mu is None:\n",
    "#             mu = Variable(self.mu)\n",
    "#         if cov is None:\n",
    "#             cov = Variable(self.cov)\n",
    "\n",
    "#         k, d, _ = cov.size()\n",
    "\n",
    "#         z_mu = (z.unsqueeze(1) - mu.unsqueeze(0))\n",
    "\n",
    "#         cov_inverse = []\n",
    "#         det_cov = []\n",
    "#         cov_diag = 0\n",
    "#         eps = 1e-12\n",
    "#         for i in range(k):\n",
    "#             # K x D x D\n",
    "#             cov_k = cov[i] + self.to_var(torch.eye(d) * eps)\n",
    "#             pinv = np.linalg.pinv(cov_k.data.cpu().numpy())\n",
    "#             cov_inverse.append(Variable(torch.from_numpy(pinv)).unsqueeze(0))\n",
    "\n",
    "#             eigvals = np.linalg.eigvals(cov_k.data.cpu().numpy() * (2 * np.pi))\n",
    "#             if np.min(eigvals) < 0:\n",
    "#                 pass\n",
    "#                 #logging.warning(f'Determinant was negative! Clipping Eigenvalues to 0+epsilon from {np.min(eigvals)}')\n",
    "#             determinant = np.prod(np.clip(eigvals, a_min=sys.float_info.epsilon, a_max=None))\n",
    "#             det_cov.append(determinant)\n",
    "\n",
    "#             cov_diag = cov_diag + torch.sum(1 / cov_k.diag())\n",
    "\n",
    "#         # K x D x D\n",
    "#         cov_inverse = torch.cat(cov_inverse, dim=0)\n",
    "#         # K\n",
    "#         det_cov = Variable(torch.from_numpy(np.float32(np.array(det_cov))))\n",
    "# #         print (\"sum-0\",cov_inverse.unsqueeze(0))\n",
    "# #         print (\"sum-1\",z_mu.ufnsqueeze(-1).cpu())\n",
    "# #         print (\"sum\", torch.sum(z_mu.unsqueeze(-1).cpu() * cov_inverse.unsqueeze(0), dim=-2))\n",
    "#         # N x K\n",
    "#         exp_term_tmp = -0.5 * torch.sum(torch.sum(z_mu.unsqueeze(-1).cpu() * cov_inverse.unsqueeze(0), dim=-2) * z_mu.cpu(), dim=-1)\n",
    "#         # for stability (logsumexp)\n",
    "#         max_val = torch.max((exp_term_tmp).clamp(min=0), dim=1, keepdim=True)[0]\n",
    "\n",
    "#         exp_term = torch.exp(exp_term_tmp - max_val)\n",
    "# #         print (\"sample_energy\", self.to_var(phi.unsqueeze(0)).cpu())\n",
    "# #         print (\"sample_energy-exp_term\", exp_term)\n",
    "#         sample_energy = -max_val.squeeze() - torch.log(\n",
    "#             torch.sum(self.to_var(phi.unsqueeze(0)).cpu() * exp_term / (torch.sqrt(self.to_var(det_cov).cpu()) + eps).unsqueeze(0),\n",
    "#                       dim=1) + eps)\n",
    "\n",
    "#         if size_average:\n",
    "#             sample_energy = torch.mean(sample_energy)\n",
    "\n",
    "#         return sample_energy, cov_diag\n",
    "\n",
    "#     def loss_function(self, x, x_hat, z, gamma, lambda_energy, lambda_cov_diag):\n",
    "#         recon_error = torch.mean((x.view(*x_hat.shape) - x_hat) ** 2)\n",
    "#         #print (z, gamma)\n",
    "#         phi, mu, cov = self.compute_gmm_params(z, gamma)\n",
    "        \n",
    "#         #print (z, phi, mu, cov)\n",
    "#         sample_energy, cov_diag = self.compute_energy(z, phi, mu, cov)\n",
    "# #         print (\"recon_error\",recon_error)\n",
    "# #         print (\"lambda_energy\",lambda_energy)\n",
    "# #         print (\"lambda_cov_diag\",lambda_cov_diag)\n",
    "# #         cov_diag = cov_diag.float()\n",
    "# #         print (\"cov_diag\",cov_diag)\n",
    "#         loss = recon_error + lambda_energy * sample_energy + lambda_cov_diag * cov_diag\n",
    "        \n",
    "#         return loss, sample_energy, recon_error, cov_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Algorithm(metaclass=abc.ABCMeta):\n",
    "    def __init__(self, module_name, name, framework):\n",
    "        self.logger = logging.getLogger(module_name)\n",
    "        self.name = name\n",
    "        self.framework = framework\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.name\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the algorithm on the given dataset\n",
    "        :param X:\n",
    "        :param y:\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        :return score\n",
    "        \"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def binarize(self, score, threshold=None):\n",
    "        \"\"\"\n",
    "        :param threshold:\n",
    "        :param score\n",
    "        :return binary_labels\n",
    "        \"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def threshold(self, score):\n",
    "        \"\"\"\n",
    "        :param score\n",
    "        :return threshold:\n",
    "        \"\"\"\n",
    "\n",
    "    class Frameworks:\n",
    "        PyTorch, Tensorflow = range(2)\n",
    "        \n",
    "class NNAutoEncoder(AutoEncoder, GPUWrapper):\n",
    "    def __init__(self, n_features=118, sequence_length=1, hidden_size=1, gpu=0):\n",
    "        AutoEncoder.__init__(self)\n",
    "        GPUWrapper.__init__(self, gpu)\n",
    "        n_features = n_features * sequence_length\n",
    "\n",
    "        layers = []\n",
    "        layers += [nn.Linear(n_features, 60)]\n",
    "        layers += [nn.Tanh()]\n",
    "        layers += [nn.Linear(60, 30)]\n",
    "        layers += [nn.Tanh()]\n",
    "        layers += [nn.Linear(30, 10)]\n",
    "        layers += [nn.Tanh()]\n",
    "        layers += [nn.Linear(10, hidden_size)]\n",
    "\n",
    "        self._encoder = nn.Sequential(*layers)\n",
    "        self.to_device(self._encoder)\n",
    "\n",
    "        layers = []\n",
    "        layers += [nn.Linear(hidden_size, 10)]\n",
    "        layers += [nn.Tanh()]\n",
    "        layers += [nn.Linear(10, 30)]\n",
    "        layers += [nn.Tanh()]\n",
    "        layers += [nn.Linear(30, 60)]\n",
    "        layers += [nn.Tanh()]\n",
    "        layers += [nn.Linear(60, n_features)]\n",
    "\n",
    "        self._decoder = nn.Sequential(*layers)\n",
    "        self.to_device(self._decoder)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        enc = self._encoder(x)\n",
    "        dec = self._decoder(enc)\n",
    "\n",
    "        return dec, enc\n",
    "\n",
    "\n",
    "class LSTMAutoEncoder(AutoEncoder, GPUWrapper):\n",
    "    \"\"\"Autoencoder with Recurrent module. Inspired by LSTM-EncDec\"\"\"\n",
    "\n",
    "    def __init__(self, n_features: int, sequence_length: int, hidden_size: int = 1, n_layers: tuple = (3, 3),\n",
    "                 use_bias: tuple = (True, True), dropout: tuple = (0.3, 0.3), gpu: int = 0):\n",
    "        AutoEncoder.__init__(self)\n",
    "        GPUWrapper.__init__(self, gpu)\n",
    "\n",
    "        self.n_features = n_features\n",
    "        self.sequence_length = sequence_length\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.use_bias = use_bias\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.encoder = nn.LSTM(self.n_features, self.hidden_size, batch_first=True,\n",
    "                               num_layers=self.n_layers[0], bias=self.use_bias[0], dropout=self.dropout[0])\n",
    "        self.to_device(self.encoder)\n",
    "        self.decoder = nn.LSTM(self.n_features, self.hidden_size, batch_first=True,\n",
    "                               num_layers=self.n_layers[1], bias=self.use_bias[1], dropout=self.dropout[1])\n",
    "        self.to_device(self.decoder)\n",
    "        self.hidden2output = nn.Linear(self.hidden_size, self.n_features)\n",
    "        self.to_device(self.hidden2output)\n",
    "\n",
    "    def _init_hidden(self, batch_size):\n",
    "        return (self.to_var(torch.zeros(self.n_layers[0], batch_size, self.hidden_size)),\n",
    "                self.to_var(torch.zeros(self.n_layers[0], batch_size, self.hidden_size)))\n",
    "\n",
    "    def forward(self, ts_batch):\n",
    "        batch_size = ts_batch.shape[0]\n",
    "\n",
    "        # 1. Encode the timeseries to make use of the last hidden state.\n",
    "        enc_hidden = self._init_hidden(ts_batch.shape[0])  # initialization with zero\n",
    "        _, enc_hidden = self.encoder(ts_batch.float(), enc_hidden)  # .float() here or .double() for the model\n",
    "\n",
    "        # 2. Use hidden state as initialization for our Decoder-LSTM\n",
    "        dec_hidden = (enc_hidden[0], self.to_var(torch.zeros(self.n_layers[1], batch_size, self.hidden_size)))\n",
    "\n",
    "        # 3. Also, use this hidden state to get the first output aka the last point of the reconstructed timeseries\n",
    "        # 4. Reconstruct timeseries backwards\n",
    "        #    * Use true data for training decoder\n",
    "        #    * Use hidden2output for prediction\n",
    "        output = self.to_var(torch.zeros(ts_batch.size()))\n",
    "        for i in reversed(range(ts_batch.shape[1])):\n",
    "            output[:, i, :] = self.hidden2output(dec_hidden[0][0, :])\n",
    "\n",
    "            if self.training:\n",
    "                _, dec_hidden = self.decoder(ts_batch[:, i].unsqueeze(1).float(), dec_hidden)\n",
    "            else:\n",
    "                _, dec_hidden = self.decoder(output[:, i].unsqueeze(1), dec_hidden)\n",
    "\n",
    "        return output.squeeze(2), enc_hidden[0][-1].view(batch_size, self.hidden_size)\n",
    "class DAGMMModule(nn.Module, GPUWrapper):\n",
    "    \"\"\"Residual Block.\"\"\"\n",
    "\n",
    "    def __init__(self, autoencoder, n_gmm=2, latent_dim=3, gpu=0):\n",
    "        super(DAGMMModule, self).__init__()\n",
    "        GPUWrapper.__init__(self, gpu)\n",
    "\n",
    "        self.add_module('autoencoder', autoencoder)\n",
    "\n",
    "        layers = [\n",
    "            nn.Linear(latent_dim, 10),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(10, n_gmm),\n",
    "            nn.Softmax(dim=1)\n",
    "        ]\n",
    "        self.estimation = nn.Sequential(*layers)\n",
    "        self.to_device(self.estimation)\n",
    "\n",
    "        self.register_buffer('phi', self.to_var(torch.zeros(n_gmm)))\n",
    "        self.register_buffer('mu', self.to_var(torch.zeros(n_gmm, latent_dim)))\n",
    "        self.register_buffer('cov', self.to_var(torch.zeros(n_gmm, latent_dim, latent_dim)))\n",
    "\n",
    "    def relative_euclidean_distance(self, a, b, dim=1):\n",
    "        return (a - b).norm(2, dim=dim) / torch.clamp(a.norm(2, dim=dim), min=1e-10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        dec, enc = self.autoencoder(x)\n",
    "\n",
    "        rec_cosine = F.cosine_similarity(x.view(x.shape[0], -1), dec.view(dec.shape[0], -1), dim=1)\n",
    "        rec_euclidean = self.relative_euclidean_distance(x.view(x.shape[0], -1), dec.view(dec.shape[0], -1), dim=1)\n",
    "\n",
    "        # Concatenate latent representation, cosine similarity and relative Euclidean distance between x and dec(enc(x))\n",
    "        z = torch.cat([enc, rec_euclidean.unsqueeze(-1), rec_cosine.unsqueeze(-1)], dim=1)\n",
    "        gamma = self.estimation(z)\n",
    "\n",
    "        return enc, dec, z, gamma\n",
    "\n",
    "    def compute_gmm_params(self, z, gamma):\n",
    "        N = gamma.size(0)\n",
    "        # K\n",
    "        sum_gamma = torch.sum(gamma, dim=0)\n",
    "\n",
    "        # K\n",
    "        phi = (sum_gamma / N)\n",
    "\n",
    "        self.phi = phi.data\n",
    "\n",
    "        # K x D\n",
    "        mu = torch.sum(gamma.unsqueeze(-1) * z.unsqueeze(1), dim=0) / sum_gamma.unsqueeze(-1)\n",
    "        self.mu = mu.data\n",
    "        # z = N x D\n",
    "        # mu = K x D\n",
    "        # gamma N x K\n",
    "\n",
    "        # z_mu = N x K x D\n",
    "        z_mu = (z.unsqueeze(1) - mu.unsqueeze(0))\n",
    "\n",
    "        # z_mu_outer = N x K x D x D\n",
    "        z_mu_outer = z_mu.unsqueeze(-1) * z_mu.unsqueeze(-2)\n",
    "\n",
    "        # K x D x D\n",
    "        cov = torch.sum(gamma.unsqueeze(-1).unsqueeze(-1) * z_mu_outer, dim=0) / sum_gamma.unsqueeze(-1).unsqueeze(-1)\n",
    "        self.cov = cov.data\n",
    "\n",
    "        return phi, mu, cov\n",
    "\n",
    "    def compute_energy(self, z, phi=None, mu=None, cov=None, size_average=True):\n",
    "        if phi is None:\n",
    "            phi = Variable(self.phi)\n",
    "        if mu is None:\n",
    "            mu = Variable(self.mu)\n",
    "        if cov is None:\n",
    "            cov = Variable(self.cov)\n",
    "\n",
    "        k, d, _ = cov.size()\n",
    "\n",
    "        z_mu = (z.unsqueeze(1) - mu.unsqueeze(0))\n",
    "\n",
    "        cov_inverse = []\n",
    "        det_cov = []\n",
    "        cov_diag = 0\n",
    "        eps = 1e-12\n",
    "        for i in range(k):\n",
    "            # K x D x D\n",
    "            cov_k = cov[i] + self.to_var(torch.eye(d) * eps)\n",
    "            cov_inverse.append(torch.inverse(cov_k).unsqueeze(0))\n",
    "\n",
    "            eigvals = np.linalg.eigvals(cov_k.data.cpu().numpy() * (2 * np.pi))\n",
    "            if np.min(eigvals) < 0:\n",
    "                logging.warning(f'Determinant was negative! Clipping Eigenvalues to 0+epsilon from {np.min(eigvals)}')\n",
    "            determinant = np.prod(np.clip(eigvals, a_min=sys.float_info.epsilon, a_max=None))\n",
    "            det_cov.append(determinant)\n",
    "\n",
    "            cov_diag = cov_diag + torch.sum(1 / cov_k.diag())\n",
    "\n",
    "        # K x D x D\n",
    "        cov_inverse = torch.cat(cov_inverse, dim=0)\n",
    "        # K\n",
    "        det_cov = Variable(torch.from_numpy(np.float32(np.array(det_cov))))\n",
    "\n",
    "        # N x K\n",
    "        exp_term_tmp = -0.5 * torch.sum(torch.sum(z_mu.unsqueeze(-1) * cov_inverse.unsqueeze(0), dim=-2) * z_mu, dim=-1)\n",
    "        # for stability (logsumexp)\n",
    "        max_val = torch.max((exp_term_tmp).clamp(min=0), dim=1, keepdim=True)[0]\n",
    "\n",
    "        exp_term = torch.exp(exp_term_tmp - max_val)\n",
    "\n",
    "        sample_energy = -max_val.squeeze() - torch.log(\n",
    "            torch.sum(self.to_var(phi.unsqueeze(0)) * exp_term / (torch.sqrt(self.to_var(det_cov))).unsqueeze(0),\n",
    "                      dim=1) + eps)\n",
    "\n",
    "        if size_average:\n",
    "            sample_energy = torch.mean(sample_energy)\n",
    "\n",
    "        return sample_energy, cov_diag\n",
    "\n",
    "    def loss_function(self, x, x_hat, z, gamma, lambda_energy, lambda_cov_diag):\n",
    "        recon_error = torch.mean((x.view(*x_hat.shape) - x_hat) ** 2)\n",
    "        phi, mu, cov = self.compute_gmm_params(z, gamma)\n",
    "        sample_energy, cov_diag = self.compute_energy(z, phi, mu, cov)\n",
    "        loss = recon_error + lambda_energy * sample_energy + lambda_cov_diag * cov_diag\n",
    "        return loss, sample_energy, recon_error, cov_diag\n",
    "\n",
    "\n",
    "class DAGMM(Algorithm, GPUWrapper):\n",
    "    def __init__(self, num_epochs=10, lambda_energy=0.1, lambda_cov_diag=0.005, lr=1e-2, batch_size=50, gmm_k=3,\n",
    "                 normal_percentile=80, sequence_length=15, autoencoder_type=NNAutoEncoder, autoencoder_args=None,\n",
    "                 framework=Algorithm.Frameworks.PyTorch, gpu: int=0):\n",
    "        window_name = 'withWindow' if sequence_length > 1 else 'withoutWindow'\n",
    "        Algorithm.__init__(self, __name__, f'DAGMM_{autoencoder_type.__name__}_{window_name}', framework)\n",
    "        GPUWrapper.__init__(self, gpu)\n",
    "        self.num_epochs = num_epochs\n",
    "        self.lambda_energy = lambda_energy\n",
    "        self.lambda_cov_diag = lambda_cov_diag\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.gmm_k = gmm_k  # Number of Gaussian mixtures\n",
    "        self.normal_percentile = normal_percentile  # Up to which percentile data should be considered normal\n",
    "        self.autoencoder_type = autoencoder_type\n",
    "        self.autoencoder_args = autoencoder_args or {'gpu': gpu}\n",
    "\n",
    "        self.dagmm, self.optimizer, self.train_energy, self._threshold = None, None, None, None\n",
    "\n",
    "    def reset_grad(self):\n",
    "        self.dagmm.zero_grad()\n",
    "\n",
    "    def dagmm_step(self, input_data):\n",
    "        self.dagmm.train()\n",
    "        enc, dec, z, gamma = self.dagmm(input_data)\n",
    "        total_loss, sample_energy, recon_error, cov_diag = self.dagmm.loss_function(input_data, dec, z, gamma,\n",
    "                                                                                    self.lambda_energy,\n",
    "                                                                                    self.lambda_cov_diag)\n",
    "        self.reset_grad()\n",
    "        total_loss = torch.clamp(total_loss, max=1e8)  # Extremely high loss can cause NaN gradients\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.dagmm.parameters(), 5)\n",
    "        self.optimizer.step()\n",
    "        return total_loss, sample_energy, recon_error, cov_diag\n",
    "\n",
    "    def fit(self, X: pd.DataFrame):\n",
    "        \"\"\"Learn the mixture probability, mean and covariance for each component k.\n",
    "        Store the computed energy based on the training data and the aforementioned parameters.\"\"\"\n",
    "        X = X.dropna()\n",
    "        data = X.values\n",
    "        # Each point is a flattened window and thus has as many features as sequence_length * features\n",
    "        multi_points = [data[i:i + self.sequence_length] for i in range(len(data) - self.sequence_length + 1)]\n",
    "        data_loader = DataLoader(dataset=multi_points, batch_size=self.batch_size, shuffle=True, drop_last=True)\n",
    "        hidden_size = max(1, X.shape[1] // 20)\n",
    "        autoencoder = self.autoencoder_type(n_features=X.shape[1], sequence_length=self.sequence_length,\n",
    "                                            hidden_size=hidden_size, **self.autoencoder_args)\n",
    "        self.dagmm = DAGMMModule(autoencoder, n_gmm=self.gmm_k, latent_dim=hidden_size + 2, gpu=self.gpu)\n",
    "        self.to_device(self.dagmm)\n",
    "        self.optimizer = torch.optim.Adam(self.dagmm.parameters(), lr=self.lr)\n",
    "\n",
    "        for _ in range(self.num_epochs):\n",
    "            for input_data in data_loader:\n",
    "                input_data = self.to_var(input_data)\n",
    "                self.dagmm_step(input_data.float())\n",
    "\n",
    "        self.dagmm.eval()\n",
    "        n = 0\n",
    "        mu_sum = 0\n",
    "        cov_sum = 0\n",
    "        gamma_sum = 0\n",
    "        for input_data in data_loader:\n",
    "            input_data = self.to_var(input_data)\n",
    "            _, _, z, gamma = self.dagmm(input_data.float())\n",
    "            phi, mu, cov = self.dagmm.compute_gmm_params(z, gamma)\n",
    "\n",
    "            batch_gamma_sum = torch.sum(gamma, dim=0)\n",
    "\n",
    "            gamma_sum += batch_gamma_sum\n",
    "            mu_sum += mu * batch_gamma_sum.unsqueeze(-1)  # keep sums of the numerator only\n",
    "            cov_sum += cov * batch_gamma_sum.unsqueeze(-1).unsqueeze(-1)  # keep sums of the numerator only\n",
    "\n",
    "            n += input_data.size(0)\n",
    "\n",
    "        train_phi = gamma_sum / n\n",
    "        train_mu = mu_sum / gamma_sum.unsqueeze(-1)\n",
    "        train_cov = cov_sum / gamma_sum.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        train_length = len(data_loader) * self.batch_size + self.sequence_length - 1\n",
    "        train_energy = np.full((self.sequence_length, train_length), np.nan)\n",
    "        for i1, ts_batch in enumerate(data_loader):\n",
    "            ts_batch = self.to_var(ts_batch)\n",
    "            _, _, z, _ = self.dagmm(ts_batch.float())\n",
    "            sample_energies, _ = self.dagmm.compute_energy(z, phi=train_phi, mu=train_mu, cov=train_cov,\n",
    "                                                           size_average=False)\n",
    "\n",
    "            for i2, sample_energy in enumerate(sample_energies):\n",
    "                index = i1 * self.batch_size + i2\n",
    "                window_elements = list(range(index, index + self.sequence_length, 1))\n",
    "                train_energy[index % self.sequence_length, window_elements] = sample_energy.data.cpu().numpy()\n",
    "        self.train_energy = np.nanmedian(train_energy, axis=0)\n",
    "\n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        \"\"\"Using the learned mixture probability, mean and covariance for each component k, compute the energy on the\n",
    "        given data.\"\"\"\n",
    "        self.dagmm.eval()\n",
    "        X = X.dropna()\n",
    "        data = X.values\n",
    "        multi_points = [data[i:i + self.sequence_length] for i in range(len(data) - self.sequence_length + 1)]\n",
    "        data_loader = DataLoader(dataset=multi_points, batch_size=1, shuffle=False)\n",
    "        test_energy = np.full((self.sequence_length, len(data)), np.nan)\n",
    "\n",
    "        for idx, multi_point in enumerate(data_loader):\n",
    "            _, _, z, _ = self.dagmm(self.to_var(multi_point).float())\n",
    "            sample_energy, _ = self.dagmm.compute_energy(z, size_average=False)\n",
    "            window_elements = np.arange(idx, idx + self.sequence_length, 1)\n",
    "            test_energy[idx % self.sequence_length, window_elements] = sample_energy.data.cpu().numpy()\n",
    "\n",
    "        test_energy = np.nanmedian(test_energy, axis=0)\n",
    "        combined_energy = np.concatenate([self.train_energy, test_energy], axis=0)\n",
    "\n",
    "        self._threshold = np.nanpercentile(combined_energy, self.normal_percentile)\n",
    "        return test_energy\n",
    "\n",
    "    def threshold(self, score):\n",
    "        return self._threshold\n",
    "\n",
    "    def binarize(self, y, threshold=None):\n",
    "        if threshold is None:\n",
    "            if self._threshold is not None:\n",
    "                threshold = self._threshold\n",
    "            else:\n",
    "                return np.zeros_like(y)\n",
    "        return np.where(y > threshold, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"  \n",
    "detectors = DAGMM(num_epochs=100, sequence_length=3, gpu = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DAGMM in module __main__ object:\n",
      "\n",
      "class DAGMM(Algorithm, GPUWrapper)\n",
      " |  Method resolution order:\n",
      " |      DAGMM\n",
      " |      Algorithm\n",
      " |      GPUWrapper\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, num_epochs=10, lambda_energy=0.1, lambda_cov_diag=0.005, lr=0.01, batch_size=50, gmm_k=3, normal_percentile=80, sequence_length=15, autoencoder_type=<class '__main__.NNAutoEncoder'>, autoencoder_args=None, framework=0, gpu:int=0)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  binarize(self, y, threshold=None)\n",
      " |      :param threshold:\n",
      " |      :param score\n",
      " |      :return binary_labels\n",
      " |  \n",
      " |  dagmm_step(self, input_data)\n",
      " |  \n",
      " |  fit(self, X:pandas.core.frame.DataFrame)\n",
      " |      Learn the mixture probability, mean and covariance for each component k.\n",
      " |      Store the computed energy based on the training data and the aforementioned parameters.\n",
      " |  \n",
      " |  predict(self, X:pandas.core.frame.DataFrame)\n",
      " |      Using the learned mixture probability, mean and covariance for each component k, compute the energy on the\n",
      " |      given data.\n",
      " |  \n",
      " |  reset_grad(self)\n",
      " |  \n",
      " |  threshold(self, score)\n",
      " |      :param score\n",
      " |      :return threshold:\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Algorithm:\n",
      " |  \n",
      " |  __str__(self) -> str\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from Algorithm:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from Algorithm:\n",
      " |  \n",
      " |  Frameworks = <class '__main__.Algorithm.Frameworks'>\n",
      " |  \n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from GPUWrapper:\n",
      " |  \n",
      " |  to_device(self, model)\n",
      " |      PyTorch only: send Model to proper device.\n",
      " |  \n",
      " |  to_var(self, x, **kwargs)\n",
      " |      PyTorch only: send Var to proper device.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from GPUWrapper:\n",
      " |  \n",
      " |  tf_device\n",
      " |  \n",
      " |  torch_device\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(detectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'details'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-1711db5689d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdetectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m52\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-130-4ff1fcc4baa2>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         autoencoder = self.autoencoder_type(n_features=X.shape[1], sequence_length=self.sequence_length,\n\u001b[0;32m--> 306\u001b[0;31m                                             hidden_size=hidden_size, **self.autoencoder_args)\n\u001b[0m\u001b[1;32m    307\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdagmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDAGMMModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_gmm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgmm_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdagmm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-130-4ff1fcc4baa2>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_features, sequence_length, hidden_size, gpu)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mNNAutoEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAutoEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGPUWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m118\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mAutoEncoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mGPUWrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-118-c70b79fce219>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, num_epochs, batch_size, lr, hidden_size, sequence_length, train_gaussian_percentage, seed, gpu, details)\u001b[0m\n\u001b[1;32m    104\u001b[0m                  \u001b[0mhidden_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_gaussian_percentage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                  seed: int=None, gpu: int=None, details=True):\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mAlgorithm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdetails\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mPyTorchUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'details'"
     ]
    }
   ],
   "source": [
    "detectors.fit(X_train.iloc[:52,:-1].copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4171146, 24)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# output features and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = detectors.predict(Feature.iloc[:,:-1].copy())\n",
    "output = pd.DataFrame({\"cano_locdt_index\":Feature.iloc[:,-1]})\n",
    "output[\"score\"] = score\n",
    "print (output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output[\"cosine_errors_mean\"] = detectors.prediction_details[\"cosine_errors_mean\"]\n",
    "output[\"euclidean_errors_mean\"]  = detectors.prediction_details[\"euclidean_errors_mean\"]\n",
    "data = detectors.prediction_details[\"reconstructions_mean\"]\n",
    "reconstructions_mean = pd.DataFrame(data.T,\n",
    "             columns = [\"reconstructions_mean_latent_features_{}\".format(i) for i in range(data.shape[0])]\n",
    "            )\n",
    "print (reconstructions_mean.shape)\n",
    "data = detectors.prediction_details[\"latent_representations\"]\n",
    "latent_representations = pd.DataFrame(data.T,\n",
    "             columns = [\"latent_representations_latent_features_{}\".format(i) for i in range(data.shape[0])]\n",
    "            )\n",
    "print (latent_representations.shape)\n",
    "output = pd.concat([output,reconstructions_mean,latent_representations], axis = 1)\n",
    "print (output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = []\n",
    "for i in range(len(output)):\n",
    "    if i%3 == 2:\n",
    "        feature.append(output.iloc[i:i+1])\n",
    "feature = pd.concat(feature,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature.to_csv(\"/data/yunrui_li/fraud/fraud_detection/features/DAGMM_features.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = 0\n",
    "torch.device(f'cuda:{gpu}' if torch.cuda.is_available() and gpu is not None else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"  \n",
    "\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"  \n",
    "local_device_protos = device_lib.list_local_devices()\n",
    "gpus = [x.name for x in local_device_protos if x.device_type == 'GPU']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_device_protos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
